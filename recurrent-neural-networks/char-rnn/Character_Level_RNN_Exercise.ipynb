{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Character-Level LSTM in PyTorch\n",
    "\n",
    "In this notebook, I'll construct a character-level LSTM with PyTorch. The network will train character by character on some text, then generate new text character by character. As an example, I will train on Anna Karenina. **This model will be able to generate new text based on the text from the book!**\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First let's load in our required resources for data loading and model creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T01:27:49.659250Z",
     "start_time": "2019-02-06T01:27:48.335213Z"
    },
    "hidden": true,
    "tags": [
     "#imports"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.cpp_extension import load\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "from time import sleep\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Custom C++ & CUDA modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\0107w\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\cpp_extension.py:92: UserWarning: Error checking compiler version: Command '['c++']' returned non-zero exit status 1.\n",
      "  warnings.warn('Error checking compiler version: {}'.format(error))\n",
      "c:\\users\\0107w\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\cpp_extension.py:118: UserWarning: \n",
      "\n",
      "                               !! WARNING !!\n",
      "\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Your compiler (c++) may be ABI-incompatible with PyTorch!\n",
      "Please use a compiler that is ABI-compatible with GCC 4.9 and above.\n",
      "See https://gcc.gnu.org/onlinedocs/libstdc++/manual/abi.html.\n",
      "\n",
      "See https://gist.github.com/goldsborough/d466f43e8ffc948ff92de7486c5216d6\n",
      "for instructions on how to install GCC 4.9 or higher.\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "\n",
      "                              !! WARNING !!\n",
      "\n",
      "  warnings.warn(ABI_INCOMPATIBILITY_WARNING.format(compiler))\n"
     ]
    }
   ],
   "source": [
    "import peephole_lstm_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dropout = 0.\n",
    "class PeepholeNetworkCPP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm0 = peephole_lstm_classes.PeepholeLSTMCell(129, 300)\n",
    "        self.lstm1 = peephole_lstm_classes.PeepholeLSTMCell(300, 300)\n",
    "        self.lstm2 = peephole_lstm_classes.PeepholeLSTMCell(300, 300)\n",
    "        self.fc = nn.Linear(300, 129)\n",
    "    def forward(self, x, states):\n",
    "        output = x.new_empty(x.size(0), x.size(1), 129)\n",
    "        hc1, hc2, hc3 = ((states[0][i], states[1][i]) for i in range(states[0].size(0)))\n",
    "        for i, seq_batch in enumerate(x.transpose(0, 1)):\n",
    "            hc1 = self.lstm0(seq_batch, hc1)\n",
    "            #states = self.lstm0(seq_batch, states)\n",
    "            hc2 = self.lstm1(F.dropout(hc1[0], p=0, training=self.training), hc2)\n",
    "            hc3 = self.lstm2(F.dropout(hc2[0], p=0, training=self.training), hc3)\n",
    "            output[:, i, :] = self.fc(F.dropout(hc3[0], p=0, training=self.training))\n",
    "            #output[:, i, :] = self.fc(F.dropout(states[0], p=0, training=self.training))\n",
    "        new_states = (torch.cat((hc1[0], hc2[0], hc3[0])), torch.cat((hc1[1], hc2[1], hc3[1])))\n",
    "        return output, new_states\n",
    "        #return output, states\n",
    "class PeepholeNetworkTorch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm0 = peephole_lstm_classes.PeepholeLSTMCellTorch(129, 300)\n",
    "        self.lstm1 = peephole_lstm_classes.PeepholeLSTMCellTorch(300, 300)\n",
    "        self.lstm2 = peephole_lstm_classes.PeepholeLSTMCellTorch(300, 300)\n",
    "        self.fc = nn.Linear(300, 129)\n",
    "    def forward(self, x, states):\n",
    "        output = x.new_empty(x.size(0), x.size(1), 129)\n",
    "        hc1, hc2, hc3 = ((states[0][i], states[1][i]) for i in range(states[0].size(0)))\n",
    "        for i, seq_batch in enumerate(x.transpose(0, 1)):\n",
    "            hc1 = self.lstm0(seq_batch, hc1)\n",
    "            #states = self.lstm0(seq_batch, states)\n",
    "            hc2 = self.lstm1(F.dropout(hc1[0], p=0, training=self.training), hc2)\n",
    "            hc3 = self.lstm2(F.dropout(hc2[0], p=0, training=self.training), hc3)\n",
    "            output[:, i, :] = self.fc(F.dropout(hc3[0], p=0, training=self.training))\n",
    "            #output[:, i, :] = self.fc(F.dropout(states[0], p=0, training=self.training))\n",
    "        new_states = (torch.cat((hc1[0], hc2[0], hc3[0])), torch.cat((hc1[1], hc2[1], hc3[1])))\n",
    "        return output, new_states\n",
    "        #return output, states\n",
    "class PeepholeLoopTorch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm0 = peephole_lstm_classes.PeepholeLSTMTorch(129, 300, 0)\n",
    "        self.lstm1 = peephole_lstm_classes.PeepholeLSTMTorch(300, 300, 0)\n",
    "        self.lstm2 = peephole_lstm_classes.PeepholeLSTMTorch(300, 300, 0)\n",
    "        self.fc = nn.Linear(300, 129)\n",
    "    def forward(self, x, states):\n",
    "        hc1, hc2, hc3 = ((states[0][i], states[1][i]) for i in range(states[0].size(0)))\n",
    "        x, hc1 = self.lstm0(x, hc1)\n",
    "        x, hc2 = self.lstm1(x, hc2)\n",
    "        x, hc3 = self.lstm2(x, hc3)\n",
    "        x = self.fc(x)\n",
    "        new_states = (torch.cat((hc1[0], hc2[0], hc3[0])), torch.cat((hc1[1], hc2[1], hc3[1])))\n",
    "        return x, new_states\n",
    "class PeepholeLoop(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm0 = peephole_lstm_classes.PeepholeLSTM(129, 300, 0)\n",
    "        self.lstm1 = peephole_lstm_classes.PeepholeLSTM(300, 300, 0)\n",
    "        self.lstm2 = peephole_lstm_classes.PeepholeLSTM(300, 300, 0)\n",
    "        self.fc = nn.Linear(300, 129)\n",
    "    def forward(self, x, states):\n",
    "        hc1, hc2, hc3 = ((states[0][i], states[1][i]) for i in range(states[0].size(0)))\n",
    "        x, hc1 = self.lstm0(x, hc1)\n",
    "        #x, states = self.lstm0(x, states)\n",
    "        x, hc2 = self.lstm1(x, hc2)\n",
    "        x, hc3 = self.lstm2(x, hc3)\n",
    "        x = self.fc(x)\n",
    "        new_states = (torch.cat((hc1[0], hc2[0], hc3[0])), torch.cat((hc1[1], hc2[1], hc3[1])))\n",
    "        return x, new_states\n",
    "        #return x, states\n",
    "class PeepholeNormTorch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm0 = peephole_lstm_classes.PeepholeNormLSTMTorch(129, 300, 0)\n",
    "        self.lstm1 = peephole_lstm_classes.PeepholeNormLSTMTorch(300, 300, 0)\n",
    "        self.lstm2 = peephole_lstm_classes.PeepholeNormLSTMTorch(300, 300, 0)\n",
    "        self.fc = nn.Linear(300, 129)\n",
    "    def forward(self, x, states):\n",
    "        hc1, hc2, hc3 = ((states[0][i], states[1][i]) for i in range(states[0].size(0)))\n",
    "        x, hc1 = self.lstm0(x, hc1)\n",
    "        x, hc2 = self.lstm1(x, hc2)\n",
    "        x, hc3 = self.lstm2(x, hc3)\n",
    "        x = self.fc(x)\n",
    "        new_states = (torch.cat((hc1[0], hc2[0], hc3[0])), torch.cat((hc1[1], hc2[1], hc3[1])))\n",
    "        return x, new_states\n",
    "\n",
    "\n",
    "device = ['cpu', 'cuda:0'][1]\n",
    "\n",
    "# model_cpp = peephole_lstm_cell_classes.PeepholeLSTMCell(129, 300)\n",
    "# model_torch = peephole_lstm_cell_classes.PeepholeLSTMCellTorch(129, 300)\n",
    "model_cpp = PeepholeNetworkCPP()\n",
    "model_torch = PeepholeNetworkTorch()\n",
    "model_torch_loop = PeepholeLoopTorch()\n",
    "model_loop = PeepholeLoop()\n",
    "\n",
    "model_norm_torch = PeepholeNormTorch()\n",
    "\n",
    "for module_name in ('lstm0', 'lstm1', 'lstm2', 'fc'):\n",
    "#for module_name in ('lstm0', 'fc'):\n",
    "    copyfrom = getattr(model_cpp, module_name)\n",
    "    if module_name != 'fc':\n",
    "        mod = getattr(model_torch, module_name)\n",
    "        mod.weight_ih.data = copyfrom.weight_ih.data\n",
    "        mod.weight_hh.data = copyfrom.weight_hh.data\n",
    "        mod.weight_ch.data = copyfrom.weight_ch.data\n",
    "        mod.bias.data = copyfrom.bias.data\n",
    "        mod = getattr(model_loop, module_name)\n",
    "        mod.weight_ih.data = copyfrom.weight_ih.data\n",
    "        mod.weight_hh.data = copyfrom.weight_hh.data\n",
    "        mod.weight_ch.data = copyfrom.weight_ch.data\n",
    "        mod.bias.data = copyfrom.bias.data\n",
    "        mod = getattr(model_torch_loop, module_name)\n",
    "        mod.weight_ih.data = copyfrom.weight_ih.data\n",
    "        mod.weight_hh.data = copyfrom.weight_hh.data\n",
    "        mod.weight_ch.data = copyfrom.weight_ch.data\n",
    "        mod.bias.data = copyfrom.bias.data\n",
    "    else:\n",
    "        mod = getattr(model_torch, module_name)\n",
    "        mod.weight.data = copyfrom.weight.data\n",
    "        mod.bias.data = copyfrom.bias.data\n",
    "        mod = getattr(model_loop, module_name)\n",
    "        mod.weight.data = copyfrom.weight.data\n",
    "        mod.bias.data = copyfrom.bias.data\n",
    "        mod = getattr(model_torch_loop, module_name)\n",
    "        mod.weight.data = copyfrom.weight.data\n",
    "        mod.bias.data = copyfrom.bias.data\n",
    "\n",
    "model_cpp.to(device)\n",
    "model_torch.to(device)\n",
    "model_torch_loop.to(device)\n",
    "model_loop.to(device)\n",
    "model_norm_torch.to(device)\n",
    "\n",
    "batch_size = 32\n",
    "seq_length = 1\n",
    "\n",
    "play_input = torch.randn(batch_size, seq_length, 129).to(device)\n",
    "play_states = (torch.zeros(3, batch_size, 300).to(device), torch.zeros(3, batch_size, 300).to(device))\n",
    "#play_states = torch.zeros(4, 600).to(device).chunk(dim=1, chunks=2)\n",
    "\n",
    "# print(model_cpp)\n",
    "# print(model_torch)\n",
    "# print(model_torch_loop)\n",
    "# print(model_loop)\n",
    "# print(model_norm_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NormalizedPeepholeLSTM(input_size=129, hidden_size=300, dropout=0.0, momentum=0.1, eps=1e-08)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n0 = peephole_lstm_classes.PeepholeNormLSTMTorch(129, 300, dropout=0., momentum=0.1, eps=1e-8)\n",
    "n1 = peephole_lstm_classes.NormalizedPeepholeLSTM(129, 300, dropout=0., momentum=0.1, eps=1e-8)\n",
    "\n",
    "for params in zip(n0.parameters(), n1.parameters()):\n",
    "    params[0].data = params[1].data\n",
    "\n",
    "n0.cuda()\n",
    "n1.cuda()\n",
    "\n",
    "# print(n0.running_var_tanh_cell)\n",
    "# print(n1.running_var_tanh_cell)\n",
    "# print(\"#####################################################\")\n",
    "# print(n0(play_input, (play_states[0][0], play_states[1][0])))\n",
    "# print('-----------------------------------------------------')\n",
    "# print(n1(play_input, (play_states[0][0], play_states[1][0])))\n",
    "# print(\"#####################################################\")\n",
    "# print(n0.running_var_tanh_cell)\n",
    "# print(n1.running_var_tanh_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.6410, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-2.6410, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "\n",
      "tensor([[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "        ...,\n",
      "        [ -0.9028,  -0.7898,   1.1119,  ...,  -1.9372,   3.4961,   1.6380],\n",
      "        [  6.0501,   7.9203,   2.0289,  ...,   7.1978,  -5.3366,   6.3599],\n",
      "        [ 21.1133, -19.8142,   6.5822,  ...,  10.6429,   0.4151,  -5.9493]],\n",
      "       device='cuda:0')\n",
      "tensor([[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "        ...,\n",
      "        [ -0.9028,  -0.7898,   1.1119,  ...,  -1.9372,   3.4961,   1.6379],\n",
      "        [  6.0501,   7.9203,   2.0289,  ...,   7.1978,  -5.3366,   6.3599],\n",
      "        [ 21.1133, -19.8142,   6.5822,  ...,  10.6429,   0.4151,  -5.9493]],\n",
      "       device='cuda:0')\n",
      "tensor([[[-0.2479, -0.9770, -0.5136,  ..., -0.4343,  1.7343, -0.0279]],\n",
      "\n",
      "        [[ 0.4815,  0.5167,  0.5479,  ..., -0.6478, -0.0401, -0.3096]],\n",
      "\n",
      "        [[-0.1281, -0.0473,  0.2143,  ..., -0.1022,  0.8218, -0.9438]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.3425, -0.0305, -0.9640,  ..., -0.1809,  0.8595,  0.3845]],\n",
      "\n",
      "        [[ 0.1896, -0.1417,  1.0448,  ...,  0.1464, -0.2985, -0.1564]],\n",
      "\n",
      "        [[-0.3682,  0.0681, -0.3280,  ...,  0.2884, -0.6389, -0.7788]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[-0.2479, -0.9770, -0.5136,  ..., -0.4343,  1.7343, -0.0279]],\n",
      "\n",
      "        [[ 0.4815,  0.5167,  0.5479,  ..., -0.6478, -0.0401, -0.3096]],\n",
      "\n",
      "        [[-0.1281, -0.0473,  0.2143,  ..., -0.1022,  0.8218, -0.9438]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.3425, -0.0305, -0.9640,  ..., -0.1809,  0.8595,  0.3845]],\n",
      "\n",
      "        [[ 0.1896, -0.1417,  1.0448,  ...,  0.1464, -0.2985, -0.1564]],\n",
      "\n",
      "        [[-0.3682,  0.0681, -0.3280,  ...,  0.2884, -0.6389, -0.7788]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "n0.zero_grad()\n",
    "n1.zero_grad()\n",
    "p0, p1 = play_input.clone().requires_grad_(True), play_input.clone().requires_grad_(True)\n",
    "noise = torch.randn(batch_size, 1, 300, device=device)\n",
    "\n",
    "t = n0(p0, (play_states[0][0], play_states[1][0]))[0].mul(noise).sum()\n",
    "print(t)\n",
    "t.backward()\n",
    "t = n1(p1, (play_states[0][0], play_states[1][0]))[0].mul(noise).sum()\n",
    "print(t)\n",
    "t.backward()\n",
    "print()\n",
    "\n",
    "print(n0.weight_ih.grad[::50])\n",
    "print(n1.weight_ih.grad[::50])\n",
    "print(p0.grad)\n",
    "print(p1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12 ms ± 267 µs per loop (mean ± std. dev. of 1000 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 200\n",
    "n0(play_input, (play_states[0][0], play_states[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.76 ms ± 1.06 ms per loop (mean ± std. dev. of 1000 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 200\n",
    "n1(play_input, (play_states[0][0], play_states[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.34 ms ± 971 µs per loop (mean ± std. dev. of 200 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 200 out = n0(play_input, (play_states[0][0], play_states[1][0]))[0]\n",
    "n0.zero_grad()\n",
    "out.pow(2).sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.63 ms ± 879 µs per loop (mean ± std. dev. of 200 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 200 out = n1(play_input, (play_states[0][0], play_states[1][0]))[0]\n",
    "n1.zero_grad()\n",
    "out.pow(2).sum().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Vanila PyTorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "493 ms ± 50.8 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 30\n",
    "out = model_torch(play_input, play_states)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896 ms ± 24.6 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 30 out = model_torch(play_input, play_states)[0]\n",
    "model_torch.zero_grad()\n",
    "out.pow(2).sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 0.0562,  0.0598,  0.0121,  ..., -0.0333, -0.0352, -0.0040],\n",
      "         [ 0.0576,  0.0620,  0.0103,  ..., -0.0361, -0.0360, -0.0049],\n",
      "         [ 0.0576,  0.0631,  0.0094,  ..., -0.0379, -0.0359, -0.0053],\n",
      "         ...,\n",
      "         [ 0.0582,  0.0649,  0.0088,  ..., -0.0394, -0.0347, -0.0060],\n",
      "         [ 0.0579,  0.0650,  0.0090,  ..., -0.0392, -0.0347, -0.0060],\n",
      "         [ 0.0577,  0.0653,  0.0091,  ..., -0.0389, -0.0349, -0.0060]],\n",
      "\n",
      "        [[ 0.0558,  0.0600,  0.0130,  ..., -0.0329, -0.0354, -0.0040],\n",
      "         [ 0.0575,  0.0622,  0.0111,  ..., -0.0361, -0.0365, -0.0049],\n",
      "         [ 0.0582,  0.0637,  0.0100,  ..., -0.0380, -0.0373, -0.0052],\n",
      "         ...,\n",
      "         [ 0.0575,  0.0636,  0.0074,  ..., -0.0397, -0.0349, -0.0064],\n",
      "         [ 0.0578,  0.0634,  0.0074,  ..., -0.0398, -0.0345, -0.0064],\n",
      "         [ 0.0578,  0.0635,  0.0078,  ..., -0.0394, -0.0345, -0.0066]],\n",
      "\n",
      "        [[ 0.0560,  0.0601,  0.0124,  ..., -0.0335, -0.0353, -0.0036],\n",
      "         [ 0.0578,  0.0626,  0.0106,  ..., -0.0368, -0.0363, -0.0041],\n",
      "         [ 0.0581,  0.0638,  0.0096,  ..., -0.0388, -0.0362, -0.0045],\n",
      "         ...,\n",
      "         [ 0.0581,  0.0648,  0.0097,  ..., -0.0386, -0.0355, -0.0060],\n",
      "         [ 0.0584,  0.0648,  0.0097,  ..., -0.0386, -0.0356, -0.0061],\n",
      "         [ 0.0588,  0.0646,  0.0093,  ..., -0.0389, -0.0355, -0.0063]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0562,  0.0598,  0.0120,  ..., -0.0334, -0.0352, -0.0038],\n",
      "         [ 0.0576,  0.0618,  0.0097,  ..., -0.0365, -0.0356, -0.0046],\n",
      "         [ 0.0583,  0.0630,  0.0083,  ..., -0.0376, -0.0356, -0.0052],\n",
      "         ...,\n",
      "         [ 0.0583,  0.0653,  0.0087,  ..., -0.0396, -0.0368, -0.0064],\n",
      "         [ 0.0586,  0.0657,  0.0090,  ..., -0.0400, -0.0363, -0.0064],\n",
      "         [ 0.0587,  0.0653,  0.0091,  ..., -0.0403, -0.0358, -0.0065]],\n",
      "\n",
      "        [[ 0.0559,  0.0600,  0.0123,  ..., -0.0334, -0.0346, -0.0035],\n",
      "         [ 0.0571,  0.0621,  0.0103,  ..., -0.0367, -0.0350, -0.0043],\n",
      "         [ 0.0577,  0.0632,  0.0096,  ..., -0.0385, -0.0357, -0.0047],\n",
      "         ...,\n",
      "         [ 0.0587,  0.0648,  0.0088,  ..., -0.0384, -0.0357, -0.0059],\n",
      "         [ 0.0583,  0.0646,  0.0087,  ..., -0.0389, -0.0352, -0.0060],\n",
      "         [ 0.0578,  0.0646,  0.0091,  ..., -0.0391, -0.0353, -0.0058]],\n",
      "\n",
      "        [[ 0.0562,  0.0597,  0.0120,  ..., -0.0337, -0.0347, -0.0039],\n",
      "         [ 0.0576,  0.0618,  0.0103,  ..., -0.0369, -0.0352, -0.0042],\n",
      "         [ 0.0582,  0.0631,  0.0092,  ..., -0.0385, -0.0354, -0.0043],\n",
      "         ...,\n",
      "         [ 0.0590,  0.0656,  0.0086,  ..., -0.0385, -0.0355, -0.0052],\n",
      "         [ 0.0589,  0.0651,  0.0083,  ..., -0.0385, -0.0356, -0.0052],\n",
      "         [ 0.0584,  0.0646,  0.0080,  ..., -0.0387, -0.0355, -0.0053]]],\n",
      "       device='cuda:0', grad_fn=<CopySlices>), (tensor([[ 0.0896,  0.0295,  0.0670,  ...,  0.0288,  0.0340, -0.0334],\n",
      "        [ 0.0865, -0.0257, -0.1103,  ..., -0.0421,  0.0467,  0.0578],\n",
      "        [-0.0170,  0.1020,  0.0433,  ...,  0.0383,  0.0466, -0.0079],\n",
      "        ...,\n",
      "        [-0.0140, -0.0079, -0.0111,  ..., -0.0038,  0.0007, -0.0025],\n",
      "        [-0.0150, -0.0087, -0.0110,  ..., -0.0010,  0.0004, -0.0019],\n",
      "        [-0.0177, -0.0078, -0.0106,  ..., -0.0037,  0.0015, -0.0039]],\n",
      "       device='cuda:0', grad_fn=<CatBackward>), tensor([[ 0.1589,  0.0538,  0.1500,  ...,  0.0568,  0.0660, -0.0603],\n",
      "        [ 0.1673, -0.0582, -0.2065,  ..., -0.0805,  0.0851,  0.0947],\n",
      "        [-0.0365,  0.2087,  0.0801,  ...,  0.1028,  0.1008, -0.0149],\n",
      "        ...,\n",
      "        [-0.0280, -0.0160, -0.0225,  ..., -0.0076,  0.0014, -0.0049],\n",
      "        [-0.0301, -0.0177, -0.0223,  ..., -0.0020,  0.0007, -0.0038],\n",
      "        [-0.0354, -0.0159, -0.0216,  ..., -0.0074,  0.0031, -0.0077]],\n",
      "       device='cuda:0', grad_fn=<CatBackward>)))\n",
      "tensor(-0.0486, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model_torch.eval()\n",
    "model_torch.zero_grad()\n",
    "out = model_torch(play_input, play_states)\n",
    "print(out)\n",
    "out[0].pow(2).mean().backward()\n",
    "print(model_torch.lstm0.weight_ih.grad.data.sum().add(model_torch.lstm0.weight_hh.grad.data.sum()).add(model_torch.lstm0.weight_ch.grad.data.sum()).mul(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sleep(3)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Loop within each module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404 ms ± 19.2 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 30\n",
    "out = model_torch_loop(play_input, play_states)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896 ms ± 34.6 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 30 out = model_torch_loop(play_input, play_states)[0]\n",
    "model_torch_loop.zero_grad()\n",
    "out.pow(2).sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 0.0562,  0.0598,  0.0121,  ..., -0.0333, -0.0352, -0.0040],\n",
      "         [ 0.0576,  0.0620,  0.0103,  ..., -0.0361, -0.0360, -0.0049],\n",
      "         [ 0.0576,  0.0631,  0.0094,  ..., -0.0379, -0.0359, -0.0053],\n",
      "         ...,\n",
      "         [ 0.0582,  0.0649,  0.0088,  ..., -0.0394, -0.0347, -0.0060],\n",
      "         [ 0.0579,  0.0650,  0.0090,  ..., -0.0392, -0.0347, -0.0060],\n",
      "         [ 0.0577,  0.0653,  0.0091,  ..., -0.0389, -0.0349, -0.0060]],\n",
      "\n",
      "        [[ 0.0558,  0.0600,  0.0130,  ..., -0.0329, -0.0354, -0.0040],\n",
      "         [ 0.0575,  0.0622,  0.0111,  ..., -0.0361, -0.0365, -0.0049],\n",
      "         [ 0.0582,  0.0637,  0.0100,  ..., -0.0380, -0.0373, -0.0052],\n",
      "         ...,\n",
      "         [ 0.0575,  0.0636,  0.0074,  ..., -0.0397, -0.0349, -0.0064],\n",
      "         [ 0.0578,  0.0634,  0.0074,  ..., -0.0398, -0.0345, -0.0064],\n",
      "         [ 0.0578,  0.0635,  0.0078,  ..., -0.0394, -0.0345, -0.0066]],\n",
      "\n",
      "        [[ 0.0560,  0.0601,  0.0124,  ..., -0.0335, -0.0353, -0.0036],\n",
      "         [ 0.0578,  0.0626,  0.0106,  ..., -0.0368, -0.0363, -0.0041],\n",
      "         [ 0.0581,  0.0638,  0.0096,  ..., -0.0388, -0.0362, -0.0045],\n",
      "         ...,\n",
      "         [ 0.0581,  0.0648,  0.0097,  ..., -0.0386, -0.0355, -0.0060],\n",
      "         [ 0.0584,  0.0648,  0.0097,  ..., -0.0386, -0.0356, -0.0061],\n",
      "         [ 0.0588,  0.0646,  0.0093,  ..., -0.0389, -0.0355, -0.0063]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0562,  0.0598,  0.0120,  ..., -0.0334, -0.0352, -0.0038],\n",
      "         [ 0.0576,  0.0618,  0.0097,  ..., -0.0365, -0.0356, -0.0046],\n",
      "         [ 0.0583,  0.0630,  0.0083,  ..., -0.0376, -0.0356, -0.0052],\n",
      "         ...,\n",
      "         [ 0.0583,  0.0653,  0.0087,  ..., -0.0396, -0.0368, -0.0064],\n",
      "         [ 0.0586,  0.0657,  0.0090,  ..., -0.0400, -0.0363, -0.0064],\n",
      "         [ 0.0587,  0.0653,  0.0091,  ..., -0.0403, -0.0358, -0.0065]],\n",
      "\n",
      "        [[ 0.0559,  0.0600,  0.0123,  ..., -0.0334, -0.0346, -0.0035],\n",
      "         [ 0.0571,  0.0621,  0.0103,  ..., -0.0367, -0.0350, -0.0043],\n",
      "         [ 0.0577,  0.0632,  0.0096,  ..., -0.0385, -0.0357, -0.0047],\n",
      "         ...,\n",
      "         [ 0.0587,  0.0648,  0.0088,  ..., -0.0384, -0.0357, -0.0059],\n",
      "         [ 0.0583,  0.0646,  0.0087,  ..., -0.0389, -0.0352, -0.0060],\n",
      "         [ 0.0578,  0.0646,  0.0091,  ..., -0.0391, -0.0353, -0.0058]],\n",
      "\n",
      "        [[ 0.0562,  0.0597,  0.0120,  ..., -0.0337, -0.0347, -0.0039],\n",
      "         [ 0.0576,  0.0618,  0.0103,  ..., -0.0369, -0.0352, -0.0042],\n",
      "         [ 0.0582,  0.0631,  0.0092,  ..., -0.0385, -0.0354, -0.0043],\n",
      "         ...,\n",
      "         [ 0.0590,  0.0656,  0.0086,  ..., -0.0385, -0.0355, -0.0052],\n",
      "         [ 0.0589,  0.0651,  0.0083,  ..., -0.0385, -0.0356, -0.0052],\n",
      "         [ 0.0584,  0.0646,  0.0080,  ..., -0.0387, -0.0355, -0.0053]]],\n",
      "       device='cuda:0', grad_fn=<ThAddBackward>), (tensor([[ 0.0896,  0.0295,  0.0670,  ...,  0.0288,  0.0340, -0.0334],\n",
      "        [ 0.0865, -0.0257, -0.1103,  ..., -0.0421,  0.0467,  0.0578],\n",
      "        [-0.0170,  0.1020,  0.0433,  ...,  0.0383,  0.0466, -0.0079],\n",
      "        ...,\n",
      "        [-0.0140, -0.0079, -0.0111,  ..., -0.0038,  0.0007, -0.0025],\n",
      "        [-0.0150, -0.0087, -0.0110,  ..., -0.0010,  0.0004, -0.0019],\n",
      "        [-0.0177, -0.0078, -0.0106,  ..., -0.0037,  0.0015, -0.0039]],\n",
      "       device='cuda:0', grad_fn=<CatBackward>), tensor([[ 0.1589,  0.0538,  0.1500,  ...,  0.0568,  0.0660, -0.0603],\n",
      "        [ 0.1673, -0.0582, -0.2065,  ..., -0.0805,  0.0851,  0.0947],\n",
      "        [-0.0365,  0.2087,  0.0801,  ...,  0.1028,  0.1008, -0.0149],\n",
      "        ...,\n",
      "        [-0.0280, -0.0160, -0.0225,  ..., -0.0076,  0.0014, -0.0049],\n",
      "        [-0.0301, -0.0177, -0.0223,  ..., -0.0020,  0.0007, -0.0038],\n",
      "        [-0.0354, -0.0159, -0.0216,  ..., -0.0074,  0.0031, -0.0077]],\n",
      "       device='cuda:0', grad_fn=<CatBackward>)))\n",
      "tensor(-0.0486, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model_torch_loop.eval()\n",
    "model_torch_loop.zero_grad()\n",
    "out = model_torch_loop(play_input, play_states)\n",
    "print(out)\n",
    "out[0].pow(2).mean().backward()\n",
    "print(model_torch_loop.lstm0.weight_ih.grad.data.sum().add(model_torch_loop.lstm0.weight_hh.grad.data.sum()).add(model_torch_loop.lstm0.weight_ch.grad.data.sum()).mul(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sleep(3)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Single Peephole LSTM Cell C++ (Loop implemented with Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418 ms ± 29.8 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 30\n",
    "out = model_cpp(play_input, play_states)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "805 ms ± 29.9 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 30 out = model_cpp(play_input, play_states)[0]\n",
    "model_cpp.zero_grad()\n",
    "out.pow(2).sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 0.0562,  0.0598,  0.0121,  ..., -0.0333, -0.0352, -0.0040],\n",
      "         [ 0.0576,  0.0620,  0.0103,  ..., -0.0361, -0.0360, -0.0049],\n",
      "         [ 0.0576,  0.0631,  0.0094,  ..., -0.0379, -0.0359, -0.0053],\n",
      "         ...,\n",
      "         [ 0.0582,  0.0649,  0.0088,  ..., -0.0394, -0.0347, -0.0060],\n",
      "         [ 0.0579,  0.0650,  0.0090,  ..., -0.0392, -0.0347, -0.0060],\n",
      "         [ 0.0577,  0.0653,  0.0091,  ..., -0.0389, -0.0349, -0.0060]],\n",
      "\n",
      "        [[ 0.0558,  0.0600,  0.0130,  ..., -0.0329, -0.0354, -0.0040],\n",
      "         [ 0.0575,  0.0622,  0.0111,  ..., -0.0361, -0.0365, -0.0049],\n",
      "         [ 0.0582,  0.0637,  0.0100,  ..., -0.0380, -0.0373, -0.0052],\n",
      "         ...,\n",
      "         [ 0.0575,  0.0636,  0.0074,  ..., -0.0397, -0.0349, -0.0064],\n",
      "         [ 0.0578,  0.0634,  0.0074,  ..., -0.0398, -0.0345, -0.0064],\n",
      "         [ 0.0578,  0.0635,  0.0078,  ..., -0.0394, -0.0345, -0.0066]],\n",
      "\n",
      "        [[ 0.0560,  0.0601,  0.0124,  ..., -0.0335, -0.0353, -0.0036],\n",
      "         [ 0.0578,  0.0626,  0.0106,  ..., -0.0368, -0.0363, -0.0041],\n",
      "         [ 0.0581,  0.0638,  0.0096,  ..., -0.0388, -0.0362, -0.0045],\n",
      "         ...,\n",
      "         [ 0.0581,  0.0648,  0.0097,  ..., -0.0386, -0.0355, -0.0060],\n",
      "         [ 0.0584,  0.0648,  0.0097,  ..., -0.0386, -0.0356, -0.0061],\n",
      "         [ 0.0588,  0.0646,  0.0093,  ..., -0.0389, -0.0355, -0.0063]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0562,  0.0598,  0.0120,  ..., -0.0334, -0.0352, -0.0038],\n",
      "         [ 0.0576,  0.0618,  0.0097,  ..., -0.0365, -0.0356, -0.0046],\n",
      "         [ 0.0583,  0.0630,  0.0083,  ..., -0.0376, -0.0356, -0.0052],\n",
      "         ...,\n",
      "         [ 0.0583,  0.0653,  0.0087,  ..., -0.0396, -0.0368, -0.0064],\n",
      "         [ 0.0586,  0.0657,  0.0090,  ..., -0.0400, -0.0363, -0.0064],\n",
      "         [ 0.0587,  0.0653,  0.0091,  ..., -0.0403, -0.0358, -0.0065]],\n",
      "\n",
      "        [[ 0.0559,  0.0600,  0.0123,  ..., -0.0334, -0.0346, -0.0035],\n",
      "         [ 0.0571,  0.0621,  0.0103,  ..., -0.0367, -0.0350, -0.0043],\n",
      "         [ 0.0577,  0.0632,  0.0096,  ..., -0.0385, -0.0357, -0.0047],\n",
      "         ...,\n",
      "         [ 0.0587,  0.0648,  0.0088,  ..., -0.0384, -0.0357, -0.0059],\n",
      "         [ 0.0583,  0.0646,  0.0087,  ..., -0.0389, -0.0352, -0.0060],\n",
      "         [ 0.0578,  0.0646,  0.0091,  ..., -0.0391, -0.0353, -0.0058]],\n",
      "\n",
      "        [[ 0.0562,  0.0597,  0.0120,  ..., -0.0337, -0.0347, -0.0039],\n",
      "         [ 0.0576,  0.0618,  0.0103,  ..., -0.0369, -0.0352, -0.0042],\n",
      "         [ 0.0582,  0.0631,  0.0092,  ..., -0.0385, -0.0354, -0.0043],\n",
      "         ...,\n",
      "         [ 0.0590,  0.0656,  0.0086,  ..., -0.0385, -0.0355, -0.0052],\n",
      "         [ 0.0589,  0.0651,  0.0083,  ..., -0.0385, -0.0356, -0.0052],\n",
      "         [ 0.0584,  0.0646,  0.0080,  ..., -0.0387, -0.0355, -0.0053]]],\n",
      "       device='cuda:0', grad_fn=<CopySlices>), (tensor([[ 0.0896,  0.0295,  0.0670,  ...,  0.0288,  0.0340, -0.0334],\n",
      "        [ 0.0865, -0.0257, -0.1103,  ..., -0.0421,  0.0467,  0.0578],\n",
      "        [-0.0170,  0.1020,  0.0433,  ...,  0.0383,  0.0466, -0.0079],\n",
      "        ...,\n",
      "        [-0.0140, -0.0079, -0.0111,  ..., -0.0038,  0.0007, -0.0025],\n",
      "        [-0.0150, -0.0087, -0.0110,  ..., -0.0010,  0.0004, -0.0019],\n",
      "        [-0.0177, -0.0078, -0.0106,  ..., -0.0037,  0.0015, -0.0039]],\n",
      "       device='cuda:0', grad_fn=<CatBackward>), tensor([[ 0.1589,  0.0538,  0.1500,  ...,  0.0568,  0.0660, -0.0603],\n",
      "        [ 0.1673, -0.0582, -0.2065,  ..., -0.0805,  0.0851,  0.0947],\n",
      "        [-0.0365,  0.2087,  0.0801,  ...,  0.1028,  0.1008, -0.0149],\n",
      "        ...,\n",
      "        [-0.0280, -0.0160, -0.0225,  ..., -0.0076,  0.0014, -0.0049],\n",
      "        [-0.0301, -0.0177, -0.0223,  ..., -0.0020,  0.0007, -0.0038],\n",
      "        [-0.0354, -0.0159, -0.0216,  ..., -0.0074,  0.0031, -0.0077]],\n",
      "       device='cuda:0', grad_fn=<CatBackward>)))\n",
      "tensor(-0.0486, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model_cpp.eval()\n",
    "model_cpp.zero_grad()\n",
    "out = model_cpp(play_input, play_states)\n",
    "print(out)\n",
    "out[0].pow(2).mean().backward()\n",
    "print(model_cpp.lstm0.weight_ih.grad.data.sum().add(model_cpp.lstm0.weight_hh.grad.data.sum()).add(model_cpp.lstm0.weight_ch.grad.data.sum()).mul(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sleep(3)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For-Loop implemented inside C++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Custom Kernal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.7 ms ± 7.98 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 30\n",
    "out = model_loop(play_input, play_states)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.3 ms ± 1.73 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 30 out = model_loop(play_input, play_states)[0]\n",
    "model_loop.zero_grad()\n",
    "out.pow(2).sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 0.0562,  0.0598,  0.0121,  ..., -0.0333, -0.0352, -0.0040],\n",
      "         [ 0.0576,  0.0620,  0.0103,  ..., -0.0361, -0.0360, -0.0049],\n",
      "         [ 0.0576,  0.0631,  0.0094,  ..., -0.0379, -0.0359, -0.0053],\n",
      "         ...,\n",
      "         [ 0.0582,  0.0649,  0.0088,  ..., -0.0394, -0.0347, -0.0060],\n",
      "         [ 0.0579,  0.0650,  0.0090,  ..., -0.0392, -0.0347, -0.0060],\n",
      "         [ 0.0577,  0.0653,  0.0091,  ..., -0.0389, -0.0349, -0.0060]],\n",
      "\n",
      "        [[ 0.0558,  0.0600,  0.0130,  ..., -0.0329, -0.0354, -0.0040],\n",
      "         [ 0.0575,  0.0622,  0.0111,  ..., -0.0361, -0.0365, -0.0049],\n",
      "         [ 0.0582,  0.0637,  0.0100,  ..., -0.0380, -0.0373, -0.0052],\n",
      "         ...,\n",
      "         [ 0.0575,  0.0636,  0.0074,  ..., -0.0397, -0.0349, -0.0064],\n",
      "         [ 0.0578,  0.0634,  0.0074,  ..., -0.0398, -0.0345, -0.0064],\n",
      "         [ 0.0578,  0.0635,  0.0078,  ..., -0.0394, -0.0345, -0.0066]],\n",
      "\n",
      "        [[ 0.0560,  0.0601,  0.0124,  ..., -0.0335, -0.0353, -0.0036],\n",
      "         [ 0.0578,  0.0626,  0.0106,  ..., -0.0368, -0.0363, -0.0041],\n",
      "         [ 0.0581,  0.0638,  0.0096,  ..., -0.0388, -0.0362, -0.0045],\n",
      "         ...,\n",
      "         [ 0.0581,  0.0648,  0.0097,  ..., -0.0386, -0.0355, -0.0060],\n",
      "         [ 0.0584,  0.0648,  0.0097,  ..., -0.0386, -0.0356, -0.0061],\n",
      "         [ 0.0588,  0.0646,  0.0093,  ..., -0.0389, -0.0355, -0.0063]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0562,  0.0598,  0.0120,  ..., -0.0334, -0.0352, -0.0038],\n",
      "         [ 0.0576,  0.0618,  0.0097,  ..., -0.0365, -0.0356, -0.0046],\n",
      "         [ 0.0583,  0.0630,  0.0083,  ..., -0.0376, -0.0356, -0.0052],\n",
      "         ...,\n",
      "         [ 0.0583,  0.0653,  0.0087,  ..., -0.0396, -0.0368, -0.0064],\n",
      "         [ 0.0586,  0.0657,  0.0090,  ..., -0.0400, -0.0363, -0.0064],\n",
      "         [ 0.0587,  0.0653,  0.0091,  ..., -0.0403, -0.0358, -0.0065]],\n",
      "\n",
      "        [[ 0.0559,  0.0600,  0.0123,  ..., -0.0334, -0.0346, -0.0035],\n",
      "         [ 0.0571,  0.0621,  0.0103,  ..., -0.0367, -0.0350, -0.0043],\n",
      "         [ 0.0577,  0.0632,  0.0096,  ..., -0.0385, -0.0357, -0.0047],\n",
      "         ...,\n",
      "         [ 0.0587,  0.0648,  0.0088,  ..., -0.0384, -0.0357, -0.0059],\n",
      "         [ 0.0583,  0.0646,  0.0087,  ..., -0.0389, -0.0352, -0.0060],\n",
      "         [ 0.0578,  0.0646,  0.0091,  ..., -0.0391, -0.0353, -0.0058]],\n",
      "\n",
      "        [[ 0.0562,  0.0597,  0.0120,  ..., -0.0337, -0.0347, -0.0039],\n",
      "         [ 0.0576,  0.0618,  0.0103,  ..., -0.0369, -0.0352, -0.0042],\n",
      "         [ 0.0582,  0.0631,  0.0092,  ..., -0.0385, -0.0354, -0.0043],\n",
      "         ...,\n",
      "         [ 0.0590,  0.0656,  0.0086,  ..., -0.0385, -0.0355, -0.0052],\n",
      "         [ 0.0589,  0.0651,  0.0083,  ..., -0.0385, -0.0356, -0.0052],\n",
      "         [ 0.0584,  0.0646,  0.0080,  ..., -0.0387, -0.0355, -0.0053]]],\n",
      "       device='cuda:0', grad_fn=<ThAddBackward>), (tensor([[ 0.0896,  0.0295,  0.0670,  ...,  0.0288,  0.0340, -0.0334],\n",
      "        [ 0.0865, -0.0257, -0.1103,  ..., -0.0421,  0.0467,  0.0578],\n",
      "        [-0.0170,  0.1020,  0.0433,  ...,  0.0383,  0.0466, -0.0079],\n",
      "        ...,\n",
      "        [-0.0140, -0.0079, -0.0111,  ..., -0.0038,  0.0007, -0.0025],\n",
      "        [-0.0150, -0.0087, -0.0110,  ..., -0.0010,  0.0004, -0.0019],\n",
      "        [-0.0177, -0.0078, -0.0106,  ..., -0.0037,  0.0015, -0.0039]],\n",
      "       device='cuda:0', grad_fn=<CatBackward>), tensor([[ 0.1589,  0.0538,  0.1500,  ...,  0.0568,  0.0660, -0.0603],\n",
      "        [ 0.1673, -0.0582, -0.2065,  ..., -0.0805,  0.0851,  0.0947],\n",
      "        [-0.0365,  0.2087,  0.0801,  ...,  0.1028,  0.1008, -0.0149],\n",
      "        ...,\n",
      "        [-0.0280, -0.0160, -0.0225,  ..., -0.0076,  0.0014, -0.0049],\n",
      "        [-0.0301, -0.0177, -0.0223,  ..., -0.0020,  0.0007, -0.0038],\n",
      "        [-0.0354, -0.0159, -0.0216,  ..., -0.0074,  0.0031, -0.0077]],\n",
      "       device='cuda:0', grad_fn=<CatBackward>)))\n",
      "tensor(-0.0486, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model_loop.eval()\n",
    "model_loop.zero_grad()\n",
    "out = model_loop(play_input, play_states)\n",
    "print(out)\n",
    "out[0].pow(2).mean().backward()\n",
    "print(model_loop.lstm0.weight_ih.grad.data.sum().add(model_loop.lstm0.weight_hh.grad.data.sum()).add(model_loop.lstm0.weight_ch.grad.data.sum()).mul(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sleep(3)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Normalized - Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "487 ms ± 60.6 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 30\n",
    "out = model_norm_torch(play_input, play_states)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "918 ms ± 44.6 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 30 out = model_norm_torch(play_input, play_states)[0]\n",
    "model_norm_torch.zero_grad()\n",
    "out.pow(2).sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-0.0704, -0.0244,  0.0305,  ..., -0.0407,  0.0059, -0.0575],\n",
      "         [-0.0381,  0.0100,  0.0460,  ..., -0.0236,  0.0547, -0.0603],\n",
      "         [-0.0108,  0.0340,  0.0947,  ...,  0.0006,  0.0875, -0.0402],\n",
      "         ...,\n",
      "         [-0.2807, -0.1009,  0.0343,  ...,  0.0449, -0.1193, -0.0647],\n",
      "         [-0.2771, -0.0293, -0.0023,  ...,  0.0099, -0.1284, -0.1171],\n",
      "         [-0.3189, -0.0023, -0.0334,  ..., -0.0255, -0.0703, -0.1567]],\n",
      "\n",
      "        [[-0.0681, -0.0217,  0.0038,  ..., -0.0490,  0.0505, -0.0682],\n",
      "         [-0.1424, -0.0186, -0.0091,  ..., -0.0620,  0.0914, -0.0968],\n",
      "         [-0.1891,  0.0523, -0.0042,  ..., -0.0220,  0.0909, -0.0717],\n",
      "         ...,\n",
      "         [ 0.2524,  0.0960,  0.3093,  ...,  0.0566,  0.0381, -0.1388],\n",
      "         [ 0.2102,  0.0286,  0.2966,  ...,  0.0631,  0.0219, -0.1538],\n",
      "         [ 0.1145, -0.0168,  0.2166,  ...,  0.0050, -0.0295, -0.1838]],\n",
      "\n",
      "        [[-0.0538, -0.0228,  0.0727,  ..., -0.0062,  0.0874, -0.0723],\n",
      "         [-0.0777, -0.0269,  0.0551,  ...,  0.0322,  0.0342, -0.0616],\n",
      "         [-0.0856, -0.0225,  0.0017,  ...,  0.0504, -0.0354, -0.0803],\n",
      "         ...,\n",
      "         [ 0.0683, -0.1027,  0.3336,  ..., -0.0808, -0.1171, -0.2741],\n",
      "         [ 0.0494, -0.1561,  0.3146,  ...,  0.0002,  0.0044, -0.2854],\n",
      "         [-0.0958, -0.1578,  0.1785,  ...,  0.1258,  0.1305, -0.2821]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0306, -0.0161, -0.0313,  ..., -0.0454, -0.0427, -0.0488],\n",
      "         [-0.0123,  0.0048, -0.1046,  ...,  0.0468, -0.0672,  0.0115],\n",
      "         [ 0.0347,  0.0261, -0.1936,  ...,  0.0260, -0.0556,  0.0643],\n",
      "         ...,\n",
      "         [-0.1514,  0.2704,  0.1843,  ..., -0.1508,  0.0848, -0.1464],\n",
      "         [-0.1234,  0.1080,  0.1219,  ..., -0.1203,  0.0760, -0.0912],\n",
      "         [-0.0965, -0.0359,  0.1091,  ..., -0.0354,  0.0936, -0.0089]],\n",
      "\n",
      "        [[-0.0505, -0.0288,  0.1256,  ...,  0.0272,  0.0162, -0.0320],\n",
      "         [-0.1151, -0.0652,  0.1303,  ...,  0.0722,  0.0219, -0.0263],\n",
      "         [-0.1231, -0.1130,  0.0516,  ...,  0.0881,  0.0671,  0.0318],\n",
      "         ...,\n",
      "         [ 0.1948, -0.0779,  0.1741,  ..., -0.0253,  0.0174,  0.0158],\n",
      "         [ 0.1995, -0.0167,  0.1570,  ...,  0.0049, -0.0422,  0.0379],\n",
      "         [ 0.1648, -0.0282,  0.1804,  ...,  0.0227, -0.1694,  0.0127]],\n",
      "\n",
      "        [[-0.0438,  0.0071,  0.0068,  ..., -0.0337,  0.0160, -0.0741],\n",
      "         [ 0.0189,  0.0088,  0.0314,  ..., -0.0244,  0.0188, -0.0513],\n",
      "         [ 0.1032, -0.0339,  0.0356,  ..., -0.0625,  0.0570, -0.0354],\n",
      "         ...,\n",
      "         [ 0.4504,  0.0177,  0.0498,  ..., -0.0353,  0.1917,  0.1410],\n",
      "         [ 0.5086,  0.1060,  0.1304,  ...,  0.0204,  0.1802,  0.0187],\n",
      "         [ 0.4902,  0.1875,  0.1483,  ..., -0.0138,  0.1341, -0.0291]]],\n",
      "       device='cuda:0', grad_fn=<ThAddBackward>), (tensor([[-0.0550, -0.3711, -0.5293,  ..., -0.0014, -0.0221,  0.1068],\n",
      "        [ 0.0617, -0.0535,  0.0187,  ..., -0.0062,  0.4434, -0.0039],\n",
      "        [ 0.0449, -0.7554,  0.1010,  ..., -0.0186, -0.1459,  0.0034],\n",
      "        ...,\n",
      "        [ 0.0199, -0.0166, -0.0061,  ...,  0.0024,  0.0514,  0.1356],\n",
      "        [-0.3616, -0.2052,  0.0026,  ...,  0.0500, -0.2868, -0.2885],\n",
      "        [-0.3496,  0.1983,  0.0127,  ..., -0.0197, -0.0785,  0.1368]],\n",
      "       device='cuda:0', grad_fn=<CatBackward>), tensor([[-0.0974, -0.4103, -0.2775,  ...,  0.0409, -0.0814,  0.0812],\n",
      "        [ 0.1037, -0.0771,  0.0375,  ..., -0.0356,  0.5613,  0.0199],\n",
      "        [ 0.0743, -0.7633,  0.1208,  ..., -0.2051, -0.1950,  0.0237],\n",
      "        ...,\n",
      "        [ 0.0379, -0.0032, -0.0971,  ...,  0.0113,  0.0332,  0.3511],\n",
      "        [-1.0637, -0.3408,  0.0772,  ...,  1.5727, -0.2354, -0.8120],\n",
      "        [-0.9379,  0.4414,  0.1771,  ..., -0.2113, -0.1176,  0.7713]],\n",
      "       device='cuda:0', grad_fn=<CatBackward>)))\n",
      "tensor(-1110.4403, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model_norm_torch.eval()\n",
    "model_norm_torch.zero_grad()\n",
    "out = model_norm_torch(play_input, play_states)\n",
    "print(out)\n",
    "out[0].pow(2).mean().backward()\n",
    "print(model_norm_torch.lstm0.weight_ih.grad.data.sum().add(model_norm_torch.lstm0.weight_hh.grad.data.sum()).add(model_norm_torch.lstm0.weight_ch.grad.data.sum()).mul(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sleep(3)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Load in Data\n",
    "\n",
    "Then, we'll load the Anna Karenina text file and convert it into integers for our network to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T01:27:49.682215Z",
     "start_time": "2019-02-06T01:27:49.663216Z"
    },
    "hidden": true,
    "tags": [
     "#read-text",
     "=>imports"
    ]
   },
   "outputs": [],
   "source": [
    "# open text file and read in data as `text`\n",
    "with open('data/anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's check out the first 100 characters, make sure everything is peachy. According to the [American Book Review](http://americanbookreview.org/100bestlines.asp), this is the 6th best first line of a book ever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T01:27:49.692211Z",
     "start_time": "2019-02-06T01:27:49.684214Z"
    },
    "hidden": true,
    "tags": [
     "#print-text",
     "=>read-text"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Tokenization\n",
    "\n",
    "In the cells, below, I'm creating a couple **dictionaries** to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T01:27:50.007212Z",
     "start_time": "2019-02-06T01:27:49.698215Z"
    },
    "hidden": true,
    "tags": [
     "#dicts-and-encodes",
     "=>print-text"
    ]
   },
   "outputs": [],
   "source": [
    "# encode the text and map each character to an integer and vice versa\n",
    "\n",
    "# we create two dictionaries:\n",
    "# 1. int2char, which maps integers to characters\n",
    "# 2. char2int, which maps characters to unique integers\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "And we can see those same characters from above, encoded as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T01:27:50.015212Z",
     "start_time": "2019-02-06T01:27:50.010214Z"
    },
    "hidden": true,
    "tags": [
     "#encoded-print",
     "=>dicts-and-encodes"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35, 53, 26,  1, 45, 70, 54, 77, 14,  6,  6,  6, 25, 26,  1,  1, 48,\n",
       "       77, 79, 26, 81, 55, 67, 55, 70,  0, 77, 26, 54, 70, 77, 26, 67, 67,\n",
       "       77, 26, 67, 55, 11, 70, 15, 77, 70, 42, 70, 54, 48, 77, 19, 73, 53,\n",
       "       26,  1,  1, 48, 77, 79, 26, 81, 55, 67, 48, 77, 55,  0, 77, 19, 73,\n",
       "       53, 26,  1,  1, 48, 77, 55, 73, 77, 55, 45,  0, 77, 75, 69, 73,  6,\n",
       "       69, 26, 48,  2,  6,  6, 21, 42, 70, 54, 48, 45, 53, 55, 73])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Pre-processing the data\n",
    "\n",
    "As you can see in our char-RNN image above, our LSTM expects an input that is **one-hot encoded** meaning that each character is converted into an integer (via our created dictionary) and *then* converted into a column vector where only it's corresponding integer index will have the value of 1 and the rest of the vector will be filled with 0's. Since we're one-hot encoding the data, let's make a function to do that!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T01:27:50.026213Z",
     "start_time": "2019-02-06T01:27:50.017217Z"
    },
    "hidden": true,
    "tags": [
     "#one-hot-implementation",
     "=>encoded-print"
    ]
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T01:27:50.045218Z",
     "start_time": "2019-02-06T01:27:50.029211Z"
    },
    "hidden": true,
    "tags": [
     "#one-hot-test",
     "=>one-hot-implementation"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# check that the function works as expected\n",
    "test_seq = np.array([[3, 5, 1]])\n",
    "one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Making training mini-batches\n",
    "\n",
    "\n",
    "To train on this data, we also want to create mini-batches for training. Remember that we want our batches to be multiple sequences of some desired number of sequence steps. Considering a simple example, our batches would look like this:\n",
    "\n",
    "<img src=\"assets/sequence_batching@1x.png\" width=500px>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "In this example, we'll take the encoded characters (passed in as the `arr` parameter) and split them into multiple sequences, given by `batch_size`. Each of our sequences will be `seq_length` long.\n",
    "\n",
    "### Creating Batches\n",
    "\n",
    "**1. The first thing we need to do is discard some of the text so we only have completely full mini-batches. **\n",
    "\n",
    "Each batch contains $N \\times M$ characters, where $N$ is the batch size (the number of sequences in a batch) and $M$ is the seq_length or number of time steps in a sequence. Then, to get the total number of batches, $K$, that we can make from the array `arr`, you divide the length of `arr` by the number of characters per batch. Once you know the number of batches, you can get the total number of characters to keep from `arr`, $N * M * K$.\n",
    "\n",
    "**2. After that, we need to split `arr` into $N$ batches. ** \n",
    "\n",
    "You can do this using `arr.reshape(size)` where `size` is a tuple containing the dimensions sizes of the reshaped array. We know we want $N$ sequences in a batch, so let's make that the size of the first dimension. For the second dimension, you can use `-1` as a placeholder in the size, it'll fill up the array with the appropriate data for you. After this, you should have an array that is $N \\times (M * K)$.\n",
    "\n",
    "**3. Now that we have this array, we can iterate through it to get our mini-batches. **\n",
    "\n",
    "The idea is each batch is a $N \\times M$ window on the $N \\times (M * K)$ array. For each subsequent batch, the window moves over by `seq_length`. We also want to create both the input and target arrays. Remember that the targets are just the inputs shifted over by one character. The way I like to do this window is use `range` to take steps of size `n_steps` from $0$ to `arr.shape[1]`, the total number of tokens in each sequence. That way, the integers you get from `range` always point to the start of a batch, and each window is `seq_length` wide.\n",
    "\n",
    "> **TODO:** Write the code for creating batches in the function below. The exercises in this notebook _will not be easy_. I've provided a notebook with solutions alongside this notebook. If you get stuck, checkout the solutions. The most important thing is that you don't copy and paste the code into here, **type out the solution code yourself.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T01:27:50.053212Z",
     "start_time": "2019-02-06T01:27:50.047216Z"
    },
    "hidden": true,
    "tags": [
     "#get-batches-implementation",
     "=>one-hot-test"
    ]
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    ## TODO: Get the number of batches we can make\n",
    "#     n_batches = ((len(arr) - batch_size) // seq_length) // batch_size\n",
    "    n_batches = ((len(arr) - 1) // seq_length) // batch_size\n",
    "    \n",
    "    ## TODO: Keep only enough characters to make full batches\n",
    "#     arr = arr[:batch_size * (seq_length * n_batches + 1)]\n",
    "    arr = np.concatenate([arr[seq * seq_length * n_batches:(seq + 1) * seq_length * n_batches + 1]\n",
    "                          for seq in range(batch_size)])\n",
    "    \n",
    "    ## TODO: Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, seq_length * n_batches + 1))\n",
    "    \n",
    "    ## TODO: Iterate over the batches using a window of size seq_length\n",
    "#     for n in range(0, arr.shape[1], seq_length):\n",
    "    for n in range(0, arr.shape[1] - 1, seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n +  seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = arr[:, n + 1:n + seq_length + 1]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Test Your Implementation\n",
    "\n",
    "Now I'll make some data sets and we can check out what's going on as we batch data. Here, as an example, I'm going to use a batch size of 8 and 50 sequence steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T01:27:50.065214Z",
     "start_time": "2019-02-06T01:27:50.055213Z"
    },
    "hidden": true,
    "tags": [
     "#get-batches-test",
     "=>get-batches-implementation"
    ]
   },
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T01:27:50.072216Z",
     "start_time": "2019-02-06T01:27:50.066214Z"
    },
    "hidden": true,
    "tags": [
     "#get-batches-test-print",
     "=>get-batches-test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[35 53 26  1 45 70 54 77 14  6]\n",
      " [ 0 75 73 77 45 53 26 45 77 26]\n",
      " [70 73 23 77 75 54 77 26 77 79]\n",
      " [ 0 77 45 53 70 77 63 53 55 70]\n",
      " [77  0 26 69 77 53 70 54 77 45]\n",
      " [63 19  0  0 55 75 73 77 26 73]\n",
      " [77 62 73 73 26 77 53 26 23 77]\n",
      " [40 34 67 75 73  0 11 48  2 77]]\n",
      "\n",
      "y\n",
      " [[53 26  1 45 70 54 77 14  6  6]\n",
      " [75 73 77 45 53 26 45 77 26 45]\n",
      " [73 23 77 75 54 77 26 77 79 75]\n",
      " [77 45 53 70 77 63 53 55 70 79]\n",
      " [ 0 26 69 77 53 70 54 77 45 70]\n",
      " [19  0  0 55 75 73 77 26 73 23]\n",
      " [62 73 73 26 77 53 26 23 77  0]\n",
      " [34 67 75 73  0 11 48  2 77 36]]\n"
     ]
    }
   ],
   "source": [
    "# printing out the first 10 items in a sequence\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If you implemented `get_batches` correctly, the above output should look something like \n",
    "```\n",
    "x\n",
    " [[25  8 60 11 45 27 28 73  1  2]\n",
    " [17  7 20 73 45  8 60 45 73 60]\n",
    " [27 20 80 73  7 28 73 60 73 65]\n",
    " [17 73 45  8 27 73 66  8 46 27]\n",
    " [73 17 60 12 73  8 27 28 73 45]\n",
    " [66 64 17 17 46  7 20 73 60 20]\n",
    " [73 76 20 20 60 73  8 60 80 73]\n",
    " [47 35 43  7 20 17 24 50 37 73]]\n",
    "\n",
    "y\n",
    " [[ 8 60 11 45 27 28 73  1  2  2]\n",
    " [ 7 20 73 45  8 60 45 73 60 45]\n",
    " [20 80 73  7 28 73 60 73 65  7]\n",
    " [73 45  8 27 73 66  8 46 27 65]\n",
    " [17 60 12 73  8 27 28 73 45 27]\n",
    " [64 17 17 46  7 20 73 60 20 80]\n",
    " [76 20 20 60 73  8 60 80 73 17]\n",
    " [35 43  7 20 17 24 50 37 73 36]]\n",
    " ```\n",
    " although the exact numbers may be different. Check to make sure the data is shifted over one step for `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO IT ALL (Runs all the previous cells except for the custom modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T01:27:50.080215Z",
     "start_time": "2019-02-06T01:27:50.075221Z"
    },
    "tags": [
     "#RUN-EVERYTHING",
     "=>get-batches-test-print"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Defining the network with PyTorch\n",
    "\n",
    "Below is where you'll define the network.\n",
    "\n",
    "<img src=\"assets/charRNN.png\" width=500px>\n",
    "\n",
    "Next, you'll use PyTorch to define the architecture of the network. We start by defining the layers and operations we want. Then, define a method for the forward pass. You've also been given a method for predicting characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Structure\n",
    "\n",
    "In `__init__` the suggested structure is as follows:\n",
    "* Create and store the necessary dictionaries (this has been done for you)\n",
    "* Define an LSTM layer that takes as params: an input size (the number of characters), a hidden layer size `n_hidden`, a number of layers `n_layers`, a dropout probability `drop_prob`, and a batch_first boolean (True, since we are batching)\n",
    "* Define a dropout layer with `dropout_prob`\n",
    "* Define a fully-connected layer with params: input size `n_hidden` and output size (the number of characters)\n",
    "* Finally, initialize the weights (again, this has been given)\n",
    "\n",
    "Note that some parameters have been named and given in the `__init__` function, and we use them and store them by doing something like `self.drop_prob = drop_prob`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### LSTM Inputs/Outputs\n",
    "\n",
    "You can create a basic [LSTM layer](https://pytorch.org/docs/stable/nn.html#lstm) as follows\n",
    "\n",
    "```python\n",
    "self.lstm = nn.LSTM(input_size, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "```\n",
    "\n",
    "where `input_size` is the number of characters this cell expects to see as sequential input, and `n_hidden` is the number of units in the hidden layers in the cell. And we can add dropout by adding a dropout parameter with a specified probability; this will automatically add dropout to the inputs or outputs. Finally, in the `forward` function, we can stack up the LSTM cells into layers using `.view`. With this, you pass in a list of cells and it will send the output of one cell into the next cell.\n",
    "\n",
    "We also need to create an initial hidden state of all zeros. This is done like so\n",
    "\n",
    "```python\n",
    "self.init_hidden()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T01:27:51.114669Z",
     "start_time": "2019-02-06T01:27:51.110707Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')\n",
    "# train_on_gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## TODO: define the layers of the model\n",
    "        self.lstm = nn.LSTM(input_size=len(tokens), hidden_size=n_hidden, num_layers=n_layers,\n",
    "                            batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(in_features=n_hidden, out_features=len(tokens))\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size, cuda=True):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if cuda:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0., lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## TODO: define the layers of the model\n",
    "        in_size = len(tokens)\n",
    "        for i in range(n_layers):\n",
    "            self.add_module(f'lstm{i}', peephole_lstm_classes.PeepholeLSTM(in_size, n_hidden, drop_prob))\n",
    "            in_size = n_hidden\n",
    "        #self.lstm = peephole_lstm_classes.PeepholeLSTM(len(tokens), n_hidden, drop_prob)\n",
    "        self.fc = nn.Linear(in_features=in_size, out_features=len(tokens))\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
    "        new_hidden = (torch.zeros_like(hidden[0]), torch.zeros_like(hidden[1]))\n",
    "        for i in range(self.n_layers):\n",
    "            x, hc = self.__getattr__(f'lstm{i}')(x, (hidden[0][i], hidden[1][i]))\n",
    "            new_hidden[0][i], new_hidden[1][i] = hc[0], hc[1]\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return x, new_hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size, cuda=True):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if cuda:\n",
    "            hidden = (weight.new_zeros(self.n_layers, batch_size, self.n_hidden).cuda(),\n",
    "                      weight.new_zeros(self.n_layers, batch_size, self.n_hidden).cuda())\n",
    "        else:\n",
    "            hidden = (weight.new_zeros(self.n_layers, batch_size, self.n_hidden),\n",
    "                      weight.new_zeros(self.n_layers, batch_size, self.n_hidden))\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class MyCharRNN(nn.Module):\n",
    "    \n",
    "    class PeepholeLSTMCell(nn.Module):\n",
    "        \n",
    "        def __init__(self, input_size, hidden_size, dropout=0):\n",
    "            assert isinstance(input_size, int) and input_size >= 1\n",
    "            assert isinstance(hidden_size, int) and hidden_size >= 1\n",
    "            assert isinstance(dropout, (int, float)) and 0 <= dropout <= 1\n",
    "\n",
    "            super().__init__()\n",
    "\n",
    "            self.input_size = input_size\n",
    "            self.hidden_size = hidden_size\n",
    "            self.dropout = dropout\n",
    "            \n",
    "            self.weight_ih = nn.Parameter(torch.empty(4*hidden_size, input_size))\n",
    "            self.weight_hh = nn.Parameter(torch.empty(4*hidden_size, hidden_size))\n",
    "            self.weight_ch = nn.Parameter(torch.empty(3*hidden_size, hidden_size))\n",
    "            self.bias = nn.Parameter(torch.empty(4*hidden_size))\n",
    "            \n",
    "            self.reset_parameters()\n",
    "            \n",
    "        def reset_parameters(self):\n",
    "            stdv = 1. / np.sqrt(self.input_size + 2 * self.hidden_size)\n",
    "            self.weight_ih.data.uniform_(-stdv, stdv)\n",
    "            self.weight_hh.data.uniform_(-stdv, stdv)\n",
    "            self.weight_ch.data.uniform_(-stdv, stdv)\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "            \n",
    "        def forward(self, input, hc=None):\n",
    "            if hc is None:\n",
    "                hc = input.new_zeros(input.size(1), 2 * self.hidden_size, requires_grad=False).chunk(chunks=2, dim=1)\n",
    "\n",
    "            for i, h in enumerate((hc[0], hc[1])):\n",
    "                if input.size(0) != h.size(0):\n",
    "                    raise RuntimeError(f\"Input batch size {input.size(0)} doesn't match hidden{i} batch size {h.size(0)}\")\n",
    "                if h.size(1) != self.hidden_size:\n",
    "                    raise RuntimeError(f\"hidden{i} has inconsistent hidden_size: got {h.size(1)}, expected {self.hidden_size}\")\n",
    "            \n",
    "            gates = torch.addmm(self.bias, input, self.weight_ih.t())\n",
    "            gates += torch.mm(hc[0], self.weight_hc.t())\n",
    "            gates[:, :3*self.hidden_size] += torch.mm(hc[1], self.weight_ch.t())\n",
    "            \n",
    "            sig_gates = gates[:, :3 * self.hidden_size].sigmoid().chunk(chunks=3, dim=1)\n",
    "            tanh_gate = gates[:, 3 * self.hidden_size:].tanh()\n",
    "            \n",
    "            c = sig_gates[0] * hc[1] + sig_gates[1] * tanh_gate\n",
    "            h = sig_gates[2] * torch.tanh(c)\n",
    "            output = F.dropout(h, p=self.dropout, training=self.training, inplace=False)\n",
    "            \n",
    "            return output, (h, c)\n",
    "            \n",
    "        def __repr__(self):\n",
    "            return f\"PeepholeLSTMCell(input_size={self.input_size}, hidden_size={self.hidden_size}, dropout={self.dropout})\"\n",
    "        \n",
    "        def extra_repr(self):\n",
    "            return ''\n",
    "\n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "\n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "\n",
    "        ## TODO: define the layers of the model\n",
    "        self.peephole_lstm0 = self.PeepholeLSTMCell(input_size=len(tokens), hidden_size=n_hidden, dropout=drop_prob)\n",
    "        for n in range(1, n_layers):\n",
    "            self.add_module(f\"peephole_lstm{n}\", self.PeepholeLSTMCell(input_size=n_hidden, hidden_size=n_hidden, dropout=drop_prob))\n",
    "        self.fc = nn.Linear(in_features=n_hidden, out_features=len(tokens))\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "        if len(x.size()) != 3:\n",
    "            raise RuntimeError(f\"Invalid dimensions {x.size()} for the input tensor. \"\n",
    "                               \"A 3D tensor with dimensions [batch_size, sequence_length, n_features] is expected.\")\n",
    "        if hidden is None:\n",
    "            hidden = x.new_zeros(self.n_layers, x.size(0), 2 * self.hidden_size, requires_grad=False).chunk(chunks=2, dim=2)\n",
    "                \n",
    "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
    "        out = x.transpose(0, 1)\n",
    "        new_hidden = torch.empty_like(hidden[0])\n",
    "        new_cell = torch.empty_like(hidden[1])\n",
    "        for l in range(self.n_layers):\n",
    "            outs = []\n",
    "            h = hidden[0][l]\n",
    "            c = hidden[1][l]\n",
    "            for step_batch in out:\n",
    "                out, (h, c) = self.__getattr__(f\"peephole_lstm{l}\")(step_batch, (h, c))\n",
    "                outs.append(out)\n",
    "            out = torch.stack(outs)\n",
    "            new_hidden[l] = h\n",
    "            new_cell[l] = c\n",
    "\n",
    "        out = out.transpose(0, 1).contiguous()\n",
    "\n",
    "        out = self.fc(out)\n",
    "        hidden = (new_hidden, new_cell)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size, cuda=True):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if cuda:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T01:27:52.867519Z",
     "start_time": "2019-02-06T01:27:52.390495Z"
    },
    "code_folding": [
     0,
     27,
     43,
     105
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\0107w\\Anaconda3\\envs\\pytorch1.0\\lib\\site-packages\\torch\\utils\\cpp_extension.py:184: UserWarning: Error checking compiler version for c++: Command 'c++' returned non-zero exit status 1.\n",
      "  warnings.warn('Error checking compiler version for {}: {}'.format(compiler, error))\n"
     ]
    }
   ],
   "source": [
    "class LNPeepholeLSTMTorch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_first=False, dropout=0., dropout_on_output=True, eps=1e-05):\n",
    "        if not 0 <= dropout <= 1:\n",
    "            raise ValueError(f\"Invalid dropout value : {dropout} dropout must be in range [0, 1].\")\n",
    "        super(LNPeepholeLSTMTorch, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_first = bool(batch_first)\n",
    "        self.dropout = float(dropout)\n",
    "        self.dropout_on_output = bool(dropout_on_output)\n",
    "        self.eps = eps\n",
    "\n",
    "        self.register_parameter('weight_ih', nn.Parameter(torch.empty(4 * hidden_size, input_size)))\n",
    "        self.register_parameter('weight_hh', nn.Parameter(torch.empty(4 * hidden_size, hidden_size)))\n",
    "        self.register_parameter('weight_ch', nn.Parameter(torch.empty(3 * hidden_size)))\n",
    "        self.register_parameter('bias', nn.Parameter(torch.empty(4 * hidden_size)))\n",
    "\n",
    "        self.register_parameter('gamma_f', nn.Parameter(torch.empty(hidden_size)))\n",
    "        self.register_parameter('gamma_i', nn.Parameter(torch.empty(hidden_size)))\n",
    "        self.register_parameter('gamma_g', nn.Parameter(torch.empty(hidden_size)))\n",
    "        self.register_parameter('gamma_o', nn.Parameter(torch.empty(hidden_size)))\n",
    "        self.register_parameter('gamma_cell', nn.Parameter(torch.empty(hidden_size)))\n",
    "        self.register_parameter('beta_cell', nn.Parameter(torch.empty(hidden_size)))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.input_size + 2 * self.hidden_size)\n",
    "        self.weight_ih.data.uniform_(-stdv, +stdv)\n",
    "        self.weight_hh.data.uniform_(-stdv, +stdv)\n",
    "        self.weight_ch.data.uniform_(-stdv, +stdv)\n",
    "\n",
    "        self.bias.data.zero_()\n",
    "        self.bias.data[:self.hidden_size].fill_(1.)\n",
    "\n",
    "        self.gamma_f.data.uniform_()\n",
    "        self.gamma_i.data.uniform_()\n",
    "        self.gamma_g.data.uniform_()\n",
    "        self.gamma_o.data.uniform_()\n",
    "        self.gamma_cell.data.uniform_()\n",
    "        self.beta_cell.data.zero_()\n",
    "\n",
    "    def forward(self, input, states):\n",
    "        assert input.dim() == 3, \"expected a 3 dimensional tensor as `input`, but te given tensor has {} dimension(s)\".format(input.dim())\n",
    "        assert len(states) == 2, \"expected a (hidden, cell) pair as `states`, but the length of the given states is {}\".format(len(states))\n",
    "        if self.batch_first:\n",
    "            input = input.transpose(0, 1).contiguous()\n",
    "        assert states[0].size() == (input.size(1), self.hidden_size), \"expected a hidden state tensor with dimensionality {}, but the given tensor has dimensionality []\".format(states[0].size(), (input.size(1), self.hidden_size))\n",
    "        assert states[1].size() == (input.size(1), self.hidden_size), \"expected a cell state tensor with dimensionality {}, but the given tensor has dimensionality []\".format(states[1].size(), (input.size(1), self.hidden_size))\n",
    "\n",
    "        hidden, cell = states\n",
    "\n",
    "        hidden_size = self.hidden_size\n",
    "        hidden_size_2 = 2 * hidden_size\n",
    "        hidden_size_3 = hidden_size_2 + hidden_size\n",
    "\n",
    "        norm_shape = torch.Size((hidden_size,))\n",
    "\n",
    "        outputs = input.new_empty((input.size(0), input.size(1), hidden_size))\n",
    "\n",
    "        ih = input.matmul(self.weight_ih.t())\n",
    "\n",
    "        weight_hc_h = torch.cat((self.weight_hh.t(),\n",
    "                                 torch.cat((self.weight_ch[:hidden_size].diag(),\n",
    "                                            self.weight_ch[hidden_size:hidden_size_2].diag(),\n",
    "                                            self.weight_ch.new_zeros(hidden_size_2, hidden_size))).t()))\n",
    "        weight_co = self.weight_ch[hidden_size_2:]\n",
    "\n",
    "        gamma_fig = torch.stack((self.gamma_f, self.gamma_i, self.gamma_g))\n",
    "\n",
    "        bias_fig = torch.stack(self.bias[:hidden_size_3].chunk(3, dim=0))\n",
    "        bias_o = self.bias[hidden_size_3:]\n",
    "\n",
    "        for i in range(input.size(0)):\n",
    "            gates = torch.addmm(ih[i], torch.cat((hidden, cell), dim=1), weight_hc_h).view(-1, 4, hidden_size)\n",
    "            gates_fig = gates[:, :3]\n",
    "\n",
    "\n",
    "            gates_fig = F.layer_norm(gates_fig, norm_shape, eps=self.eps)\n",
    "            gates_fig = torch.addcmul(bias_fig, gates_fig, gamma_fig)\n",
    "            forget_input_gates = gates_fig[:, :2].sigmoid()\n",
    "            candidate_cell = F.dropout(gates_fig[:, 2].tanh(), p=self.dropout, training=self.training)\n",
    "\n",
    "\n",
    "            cell = F.layer_norm(torch.addcmul(forget_input_gates[:, 0] * cell,\n",
    "                                              forget_input_gates[:, 1], candidate_cell),\n",
    "                                norm_shape, self.gamma_cell, self.beta_cell, self.eps)\n",
    "\n",
    "            output_gate = torch.addcmul(gates[:, 3], cell, weight_co)\n",
    "\n",
    "            output_gate = F.layer_norm(output_gate, norm_shape, self.gamma_o, bias_o, self.eps).sigmoid()\n",
    "\n",
    "            hidden = output_gate * cell.tanh()\n",
    "\n",
    "            outputs[i] = hidden\n",
    "\n",
    "        if self.dropout_on_output:\n",
    "            outputs = F.dropout(outputs, p=self.dropout, training=self.training)\n",
    "        \n",
    "        if self.batch_first:\n",
    "            outputs = outputs.transpose(0, 1).contiguous()\n",
    "\n",
    "        return outputs, (hidden, cell)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"LNPeepholeLSTMTorch(input_size={self.input_size}, hidden_size={self.hidden_size}, batch_first={self.batch_first}, dropout={self.dropout}, dropout_on_output={self.dropout_on_output}, eps={self.eps})\"\n",
    "\n",
    "from ln_lstm_layer.ln_lstm_layer import LNPeepholeLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T01:27:52.890471Z",
     "start_time": "2019-02-06T01:27:52.870467Z"
    },
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "module = lambda **kwds: LNPeepholeLSTM(**kwds, batch_first=True)\n",
    "# module = lambda **kwds: LNPeepholeLSTMTorch(**kwds, batch_first=True)\n",
    "# module = lambda **kwds: nn.LSTM(**kwds, batch_first=True)\n",
    "# module = lambda **kwds: nn.GRU(**kwds, batch_first=True)\n",
    "# module = peephole_lstm_classes.PeepholeNormLSTMTorch\n",
    "# module = peephole_lstm_classes.PeepholeNormLSTMTorch\n",
    "class MyCharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "\n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "\n",
    "        ## TODO: define the layers of the model\n",
    "        self.fc0 = nn.Linear(in_features=len(tokens), out_features=len(tokens))\n",
    "        self.rnn0 = module(input_size=len(tokens), hidden_size=n_hidden, dropout=drop_prob)\n",
    "        self.rnn_type = type(self.rnn0)\n",
    "        for n in range(1, n_layers):\n",
    "            self.add_module(f\"rnn{n}\", module(input_size=n_hidden, hidden_size=n_hidden, dropout=drop_prob))\n",
    "        self.fc1 = nn.Linear(in_features=n_hidden, out_features=len(tokens))\n",
    "        self.hidden0 = nn.Parameter(torch.randn(n_layers, n_hidden) * 0.1)\n",
    "        self.cell0 = nn.Parameter(torch.randn(n_layers, n_hidden) * 0.1)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "        if len(x.size()) != 3:\n",
    "            raise RuntimeError(f\"Invalid dimensions {x.size()} for the input tensor. \"\n",
    "                               \"A 3D tensor with dimensions [batch_size, sequence_length, n_features] is expected.\")\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(x.size(0))\n",
    "                \n",
    "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
    "        if self.rnn_type in (nn.RNN, nn.GRU):\n",
    "            new_hidden = torch.empty_like(hidden)\n",
    "        else:\n",
    "            new_hidden = torch.empty_like(hidden[0])\n",
    "            new_cell = torch.empty_like(hidden[1])\n",
    "        \n",
    "        x = F.relu(self.fc0(x))\n",
    "        outputs = []\n",
    "        for l in range(self.n_layers):\n",
    "            if self.rnn_type == nn.LSTM:\n",
    "                x, (new_hidden[l], new_cell[l]) = getattr(self, \"rnn{}\".format(l))(x, (hidden[0][l].unsqueeze(0), hidden[1][l].unsqueeze(0)))\n",
    "            elif self.rnn_type in (nn.RNN, nn.GRU):\n",
    "                x, new_hidden[l] = getattr(self, \"rnn{}\".format(l))(x, hidden[l].unsqueeze(0))\n",
    "            else:\n",
    "                x, (new_hidden[l], new_cell[l]) = getattr(self, \"rnn{}\".format(l))(x, (hidden[0][l], hidden[1][l]))\n",
    "            outputs.append(x)\n",
    "    \n",
    "        x = self.fc1(sum(outputs))\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        if self.rnn_type in (nn.RNN, nn.GRU):\n",
    "            return x, new_hidden\n",
    "        return x, (new_hidden, new_cell)\n",
    "    \n",
    "    def init_hidden(self, batch_size, cuda=None):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "#         hidden = (self.hidden0.unsqueeze(1).expand(self.n_layers, batch_size, self.n_hidden).contiguous(),\n",
    "#                   self.cell0.unsqueeze(1).expand(self.n_layers, batch_size, self.n_hidden).contiguous())\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if cuda is None:\n",
    "            if self.rnn_type in (nn.RNN, nn.GRU):\n",
    "                hidden = weight.new_zeros((self.n_layers, batch_size, self.n_hidden))\n",
    "            else:\n",
    "                hidden = (weight.new_zeros((self.n_layers, batch_size, self.n_hidden)),\n",
    "                          weight.new_zeros((self.n_layers, batch_size, self.n_hidden)))\n",
    "        elif cuda:\n",
    "            if self.rnn_type in (nn.RNN, nn.GRU):\n",
    "                hidden = weight.new_zeros((self.n_layers, batch_size, self.n_hidden), device='cude')\n",
    "            else:\n",
    "                hidden = (weight.new_zeros((self.n_layers, batch_size, self.n_hidden), device='cuda'),\n",
    "                          weight.new_zeros((self.n_layers, batch_size, self.n_hidden), device='cuda'))\n",
    "        else:\n",
    "            if self.rnn_type in (nn.RNN, nn.GRU):\n",
    "                hidden = weight.new_zeros((self.n_layers, batch_size, self.n_hidden), device='cpu')\n",
    "            else:\n",
    "                hidden = (weight.new_zeros((self.n_layers, batch_size, self.n_hidden), device='cpu'),\n",
    "                          weight.new_zeros((self.n_layers, batch_size, self.n_hidden), device='cpu'))\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to train\n",
    "\n",
    "The train function gives us the ability to set the number of epochs, the learning rate, and other parameters.\n",
    "\n",
    "Below we're using an Adam optimizer and cross entropy loss since we are looking at character class scores as output. We calculate the loss and perform backpropagation, as usual!\n",
    "\n",
    "A couple of details about training: \n",
    ">* Within the batch loop, we detach the hidden state from its history; this time setting it equal to a new *tuple* variable because an LSTM has a hidden state that is a tuple of the hidden and cell states.\n",
    "* We use [`clip_grad_norm_`](https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html) to help prevent exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T01:27:53.149476Z",
     "start_time": "2019-02-06T01:27:53.138468Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, weight_decay=0., clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if (train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if train_on_gpu:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            if isinstance(h, (tuple, list)): #LSTM\n",
    "                h = tuple([each.data for each in h])\n",
    "            else: #RNN/GRU\n",
    "                h = h.data\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output.view(batch_size*seq_length, output.shape[-1]), targets.contiguous().view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "#                 # Get validation loss\n",
    "#                 val_h = net.init_hidden(batch_size, train_on_gpu)\n",
    "#                 val_losses = []\n",
    "#                 net.eval()\n",
    "#                 for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "#                     # One-hot encode our data and make them Torch tensors\n",
    "#                     x = one_hot_encode(x, n_chars)\n",
    "#                     x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "#                     # Creating new variables for the hidden state, otherwise\n",
    "#                     # we'd backprop through the entire training history\n",
    "#                     val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "#                     inputs, targets = x, y\n",
    "#                     if(train_on_gpu):\n",
    "#                         inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "#                     output, val_h = net(inputs, val_h)\n",
    "#                     val_loss = criterion(output.view(batch_size*seq_length, output.shape[-1]), targets.contiguous().view(batch_size*seq_length).long())\n",
    "                \n",
    "#                     val_losses.append(val_loss.item())\n",
    "                \n",
    "#                 net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()))\n",
    "#                       \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the model\n",
    "\n",
    "Now we can actually train the network. First we'll create the network itself, with some given hyperparameters. Then, define the mini-batches sizes, and start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T01:27:53.812310Z",
     "start_time": "2019-02-06T01:27:53.794273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCharRNN(\n",
      "  (fc0): Linear(in_features=83, out_features=83, bias=True)\n",
      "  (rnn0): LNPeepholeLSTM(input_size=83, hidden_size=240, batch_first=True, dropout=0.0, dropout_on_output=True, eps=1e-05)\n",
      "  (rnn1): LNPeepholeLSTM(input_size=240, hidden_size=240, batch_first=True, dropout=0.0, dropout_on_output=True, eps=1e-05)\n",
      "  (rnn2): LNPeepholeLSTM(input_size=240, hidden_size=240, batch_first=True, dropout=0.0, dropout_on_output=True, eps=1e-05)\n",
      "  (fc1): Linear(in_features=240, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## TODO: set you model hyperparameters\n",
    "# define and print the net\n",
    "n_hidden=240\n",
    "n_layers=3\n",
    "\n",
    "# net = CharRNN(chars, n_hidden, n_layers, drop_prob=0.5, lr=0.0001)\n",
    "net = MyCharRNN(chars, n_hidden, n_layers, drop_prob=0., lr=0.001)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set your training hyperparameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-06T01:27:54.457Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100... Step: 1... Loss: 4.4503...\n",
      "Epoch: 1/100... Step: 2... Loss: 4.4049...\n",
      "Epoch: 1/100... Step: 3... Loss: 4.3492...\n",
      "Epoch: 1/100... Step: 4... Loss: 4.2350...\n",
      "Epoch: 1/100... Step: 5... Loss: 4.1805...\n",
      "Epoch: 1/100... Step: 6... Loss: 4.0208...\n",
      "Epoch: 1/100... Step: 7... Loss: 3.8852...\n",
      "Epoch: 1/100... Step: 8... Loss: 3.7353...\n",
      "Epoch: 1/100... Step: 9... Loss: 3.6035...\n",
      "Epoch: 1/100... Step: 10... Loss: 3.5070...\n",
      "Epoch: 1/100... Step: 11... Loss: 3.4146...\n",
      "Epoch: 1/100... Step: 12... Loss: 3.3420...\n",
      "Epoch: 1/100... Step: 13... Loss: 3.3003...\n",
      "Epoch: 1/100... Step: 14... Loss: 3.2572...\n",
      "Epoch: 1/100... Step: 15... Loss: 3.2073...\n",
      "Epoch: 1/100... Step: 16... Loss: 3.1973...\n",
      "Epoch: 1/100... Step: 17... Loss: 3.1848...\n",
      "Epoch: 1/100... Step: 18... Loss: 3.1738...\n",
      "Epoch: 1/100... Step: 19... Loss: 3.1525...\n",
      "Epoch: 1/100... Step: 20... Loss: 3.1416...\n",
      "Epoch: 1/100... Step: 21... Loss: 3.1306...\n",
      "Epoch: 1/100... Step: 22... Loss: 3.1393...\n",
      "Epoch: 1/100... Step: 23... Loss: 3.1327...\n",
      "Epoch: 1/100... Step: 24... Loss: 3.1299...\n",
      "Epoch: 1/100... Step: 25... Loss: 3.1249...\n",
      "Epoch: 1/100... Step: 26... Loss: 3.1269...\n",
      "Epoch: 1/100... Step: 27... Loss: 3.1197...\n",
      "Epoch: 1/100... Step: 28... Loss: 3.1074...\n",
      "Epoch: 1/100... Step: 29... Loss: 3.1125...\n",
      "Epoch: 1/100... Step: 30... Loss: 3.0969...\n",
      "Epoch: 1/100... Step: 31... Loss: 3.1116...\n",
      "Epoch: 1/100... Step: 32... Loss: 3.0966...\n",
      "Epoch: 1/100... Step: 33... Loss: 3.1011...\n",
      "Epoch: 1/100... Step: 34... Loss: 3.1034...\n",
      "Epoch: 1/100... Step: 35... Loss: 3.1046...\n",
      "Epoch: 1/100... Step: 36... Loss: 3.1164...\n",
      "Epoch: 1/100... Step: 37... Loss: 3.1177...\n",
      "Epoch: 2/100... Step: 38... Loss: 3.1038...\n",
      "Epoch: 2/100... Step: 39... Loss: 3.1047...\n",
      "Epoch: 2/100... Step: 40... Loss: 3.1055...\n",
      "Epoch: 2/100... Step: 41... Loss: 3.1091...\n",
      "Epoch: 2/100... Step: 42... Loss: 3.0958...\n",
      "Epoch: 2/100... Step: 43... Loss: 3.1031...\n",
      "Epoch: 2/100... Step: 44... Loss: 3.1045...\n",
      "Epoch: 2/100... Step: 45... Loss: 3.1194...\n",
      "Epoch: 2/100... Step: 46... Loss: 3.0994...\n",
      "Epoch: 2/100... Step: 47... Loss: 3.0996...\n",
      "Epoch: 2/100... Step: 48... Loss: 3.0773...\n",
      "Epoch: 2/100... Step: 49... Loss: 3.0863...\n",
      "Epoch: 2/100... Step: 50... Loss: 3.1024...\n",
      "Epoch: 2/100... Step: 51... Loss: 3.1074...\n",
      "Epoch: 2/100... Step: 52... Loss: 3.0908...\n",
      "Epoch: 2/100... Step: 53... Loss: 3.1089...\n",
      "Epoch: 2/100... Step: 54... Loss: 3.1170...\n",
      "Epoch: 2/100... Step: 55... Loss: 3.1180...\n",
      "Epoch: 2/100... Step: 56... Loss: 3.1073...\n",
      "Epoch: 2/100... Step: 57... Loss: 3.1023...\n",
      "Epoch: 2/100... Step: 58... Loss: 3.0974...\n",
      "Epoch: 2/100... Step: 59... Loss: 3.1116...\n",
      "Epoch: 2/100... Step: 60... Loss: 3.1068...\n",
      "Epoch: 2/100... Step: 61... Loss: 3.1058...\n",
      "Epoch: 2/100... Step: 62... Loss: 3.1012...\n",
      "Epoch: 2/100... Step: 63... Loss: 3.1039...\n",
      "Epoch: 2/100... Step: 64... Loss: 3.0976...\n",
      "Epoch: 2/100... Step: 65... Loss: 3.0861...\n",
      "Epoch: 2/100... Step: 66... Loss: 3.0907...\n",
      "Epoch: 2/100... Step: 67... Loss: 3.0775...\n",
      "Epoch: 2/100... Step: 68... Loss: 3.0911...\n",
      "Epoch: 2/100... Step: 69... Loss: 3.0757...\n",
      "Epoch: 2/100... Step: 70... Loss: 3.0803...\n",
      "Epoch: 2/100... Step: 71... Loss: 3.0822...\n",
      "Epoch: 2/100... Step: 72... Loss: 3.0813...\n",
      "Epoch: 2/100... Step: 73... Loss: 3.0927...\n",
      "Epoch: 2/100... Step: 74... Loss: 3.0923...\n",
      "Epoch: 3/100... Step: 75... Loss: 3.0775...\n",
      "Epoch: 3/100... Step: 76... Loss: 3.0764...\n",
      "Epoch: 3/100... Step: 77... Loss: 3.0752...\n",
      "Epoch: 3/100... Step: 78... Loss: 3.0760...\n",
      "Epoch: 3/100... Step: 79... Loss: 3.0581...\n",
      "Epoch: 3/100... Step: 80... Loss: 3.0614...\n",
      "Epoch: 3/100... Step: 81... Loss: 3.0570...\n",
      "Epoch: 3/100... Step: 82... Loss: 3.0679...\n",
      "Epoch: 3/100... Step: 83... Loss: 3.0437...\n",
      "Epoch: 3/100... Step: 84... Loss: 3.0432...\n",
      "Epoch: 3/100... Step: 85... Loss: 3.0135...\n",
      "Epoch: 3/100... Step: 86... Loss: 3.0178...\n",
      "Epoch: 3/100... Step: 87... Loss: 3.0286...\n",
      "Epoch: 3/100... Step: 88... Loss: 3.0260...\n",
      "Epoch: 3/100... Step: 89... Loss: 3.0041...\n",
      "Epoch: 3/100... Step: 90... Loss: 3.0187...\n",
      "Epoch: 3/100... Step: 91... Loss: 3.0156...\n",
      "Epoch: 3/100... Step: 92... Loss: 3.0096...\n",
      "Epoch: 3/100... Step: 93... Loss: 2.9911...\n",
      "Epoch: 3/100... Step: 94... Loss: 2.9797...\n",
      "Epoch: 3/100... Step: 95... Loss: 2.9619...\n",
      "Epoch: 3/100... Step: 96... Loss: 2.9796...\n",
      "Epoch: 3/100... Step: 97... Loss: 2.9648...\n",
      "Epoch: 3/100... Step: 98... Loss: 2.9546...\n",
      "Epoch: 3/100... Step: 99... Loss: 2.9355...\n",
      "Epoch: 3/100... Step: 100... Loss: 2.9253...\n",
      "Epoch: 3/100... Step: 101... Loss: 2.9060...\n",
      "Epoch: 3/100... Step: 102... Loss: 2.8810...\n",
      "Epoch: 3/100... Step: 103... Loss: 2.8697...\n",
      "Epoch: 3/100... Step: 104... Loss: 2.8387...\n",
      "Epoch: 3/100... Step: 105... Loss: 2.8375...\n",
      "Epoch: 3/100... Step: 106... Loss: 2.7976...\n",
      "Epoch: 3/100... Step: 107... Loss: 2.7905...\n",
      "Epoch: 3/100... Step: 108... Loss: 2.7777...\n",
      "Epoch: 3/100... Step: 109... Loss: 2.7862...\n",
      "Epoch: 3/100... Step: 110... Loss: 2.8117...\n",
      "Epoch: 3/100... Step: 111... Loss: 2.8236...\n",
      "Epoch: 4/100... Step: 112... Loss: 2.8104...\n",
      "Epoch: 4/100... Step: 113... Loss: 2.7658...\n",
      "Epoch: 4/100... Step: 114... Loss: 2.7712...\n",
      "Epoch: 4/100... Step: 115... Loss: 2.7585...\n",
      "Epoch: 4/100... Step: 116... Loss: 2.7263...\n",
      "Epoch: 4/100... Step: 117... Loss: 2.7185...\n",
      "Epoch: 4/100... Step: 118... Loss: 2.7080...\n",
      "Epoch: 4/100... Step: 119... Loss: 2.7114...\n",
      "Epoch: 4/100... Step: 120... Loss: 2.6794...\n",
      "Epoch: 4/100... Step: 121... Loss: 2.6573...\n",
      "Epoch: 4/100... Step: 122... Loss: 2.6258...\n",
      "Epoch: 4/100... Step: 123... Loss: 2.6133...\n",
      "Epoch: 4/100... Step: 124... Loss: 2.6180...\n",
      "Epoch: 4/100... Step: 125... Loss: 2.6142...\n",
      "Epoch: 4/100... Step: 126... Loss: 2.5881...\n",
      "Epoch: 4/100... Step: 127... Loss: 2.6021...\n",
      "Epoch: 4/100... Step: 128... Loss: 2.5981...\n",
      "Epoch: 4/100... Step: 129... Loss: 2.5896...\n",
      "Epoch: 4/100... Step: 130... Loss: 2.5713...\n",
      "Epoch: 4/100... Step: 131... Loss: 2.5665...\n",
      "Epoch: 4/100... Step: 132... Loss: 2.5452...\n",
      "Epoch: 4/100... Step: 133... Loss: 2.5515...\n",
      "Epoch: 4/100... Step: 134... Loss: 2.5322...\n",
      "Epoch: 4/100... Step: 135... Loss: 2.5242...\n",
      "Epoch: 4/100... Step: 136... Loss: 2.5139...\n",
      "Epoch: 4/100... Step: 137... Loss: 2.4993...\n",
      "Epoch: 4/100... Step: 138... Loss: 2.5079...\n",
      "Epoch: 4/100... Step: 139... Loss: 2.5662...\n",
      "Epoch: 4/100... Step: 140... Loss: 2.5141...\n",
      "Epoch: 4/100... Step: 141... Loss: 2.4775...\n",
      "Epoch: 4/100... Step: 142... Loss: 2.5032...\n",
      "Epoch: 4/100... Step: 143... Loss: 2.4691...\n",
      "Epoch: 4/100... Step: 144... Loss: 2.4705...\n",
      "Epoch: 4/100... Step: 145... Loss: 2.4633...\n",
      "Epoch: 4/100... Step: 146... Loss: 2.4493...\n",
      "Epoch: 4/100... Step: 147... Loss: 2.4668...\n",
      "Epoch: 4/100... Step: 148... Loss: 2.4416...\n",
      "Epoch: 5/100... Step: 149... Loss: 2.4481...\n",
      "Epoch: 5/100... Step: 150... Loss: 2.4244...\n",
      "Epoch: 5/100... Step: 151... Loss: 2.4165...\n",
      "Epoch: 5/100... Step: 152... Loss: 2.4290...\n",
      "Epoch: 5/100... Step: 153... Loss: 2.3985...\n",
      "Epoch: 5/100... Step: 154... Loss: 2.4033...\n",
      "Epoch: 5/100... Step: 155... Loss: 2.3936...\n",
      "Epoch: 5/100... Step: 156... Loss: 2.4114...\n",
      "Epoch: 5/100... Step: 157... Loss: 2.3797...\n",
      "Epoch: 5/100... Step: 158... Loss: 2.3682...\n",
      "Epoch: 5/100... Step: 159... Loss: 2.3506...\n",
      "Epoch: 5/100... Step: 160... Loss: 2.3452...\n",
      "Epoch: 5/100... Step: 161... Loss: 2.3449...\n",
      "Epoch: 5/100... Step: 162... Loss: 2.3582...\n",
      "Epoch: 5/100... Step: 163... Loss: 2.3327...\n",
      "Epoch: 5/100... Step: 164... Loss: 2.3305...\n",
      "Epoch: 5/100... Step: 165... Loss: 2.3322...\n",
      "Epoch: 5/100... Step: 166... Loss: 2.3321...\n",
      "Epoch: 5/100... Step: 167... Loss: 2.3449...\n",
      "Epoch: 5/100... Step: 168... Loss: 2.3706...\n",
      "Epoch: 5/100... Step: 169... Loss: 2.3251...\n",
      "Epoch: 5/100... Step: 170... Loss: 2.3243...\n",
      "Epoch: 5/100... Step: 171... Loss: 2.3165...\n",
      "Epoch: 5/100... Step: 172... Loss: 2.2971...\n",
      "Epoch: 5/100... Step: 173... Loss: 2.2993...\n",
      "Epoch: 5/100... Step: 174... Loss: 2.2779...\n",
      "Epoch: 5/100... Step: 175... Loss: 2.2836...\n",
      "Epoch: 5/100... Step: 176... Loss: 2.2605...\n",
      "Epoch: 5/100... Step: 177... Loss: 2.2700...\n",
      "Epoch: 5/100... Step: 178... Loss: 2.2447...\n",
      "Epoch: 5/100... Step: 179... Loss: 2.2601...\n",
      "Epoch: 5/100... Step: 180... Loss: 2.2291...\n",
      "Epoch: 5/100... Step: 181... Loss: 2.2438...\n",
      "Epoch: 5/100... Step: 182... Loss: 2.2182...\n",
      "Epoch: 5/100... Step: 183... Loss: 2.2172...\n",
      "Epoch: 5/100... Step: 184... Loss: 2.2269...\n",
      "Epoch: 5/100... Step: 185... Loss: 2.2193...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/100... Step: 186... Loss: 2.2278...\n",
      "Epoch: 6/100... Step: 187... Loss: 2.2107...\n",
      "Epoch: 6/100... Step: 188... Loss: 2.2034...\n",
      "Epoch: 6/100... Step: 189... Loss: 2.1995...\n",
      "Epoch: 6/100... Step: 190... Loss: 2.1778...\n",
      "Epoch: 6/100... Step: 191... Loss: 2.1974...\n",
      "Epoch: 6/100... Step: 192... Loss: 2.1667...\n",
      "Epoch: 6/100... Step: 193... Loss: 2.2005...\n",
      "Epoch: 6/100... Step: 194... Loss: 2.1674...\n",
      "Epoch: 6/100... Step: 195... Loss: 2.1597...\n",
      "Epoch: 6/100... Step: 196... Loss: 2.1366...\n",
      "Epoch: 6/100... Step: 197... Loss: 2.1259...\n",
      "Epoch: 6/100... Step: 198... Loss: 2.1263...\n",
      "Epoch: 6/100... Step: 199... Loss: 2.1364...\n",
      "Epoch: 6/100... Step: 200... Loss: 2.1227...\n",
      "Epoch: 6/100... Step: 201... Loss: 2.1339...\n",
      "Epoch: 6/100... Step: 202... Loss: 2.1444...\n",
      "Epoch: 6/100... Step: 203... Loss: 2.1297...\n",
      "Epoch: 6/100... Step: 204... Loss: 2.1196...\n",
      "Epoch: 6/100... Step: 205... Loss: 2.1319...\n",
      "Epoch: 6/100... Step: 206... Loss: 2.0991...\n",
      "Epoch: 6/100... Step: 207... Loss: 2.1151...\n",
      "Epoch: 6/100... Step: 208... Loss: 2.0905...\n",
      "Epoch: 6/100... Step: 209... Loss: 2.0839...\n",
      "Epoch: 6/100... Step: 210... Loss: 2.0809...\n",
      "Epoch: 6/100... Step: 211... Loss: 2.0699...\n",
      "Epoch: 6/100... Step: 212... Loss: 2.0617...\n",
      "Epoch: 6/100... Step: 213... Loss: 2.0482...\n",
      "Epoch: 6/100... Step: 214... Loss: 2.0538...\n",
      "Epoch: 6/100... Step: 215... Loss: 2.0376...\n",
      "Epoch: 6/100... Step: 216... Loss: 2.0558...\n",
      "Epoch: 6/100... Step: 217... Loss: 2.0334...\n",
      "Epoch: 6/100... Step: 218... Loss: 2.0434...\n",
      "Epoch: 6/100... Step: 219... Loss: 2.0279...\n",
      "Epoch: 6/100... Step: 220... Loss: 2.0352...\n",
      "Epoch: 6/100... Step: 221... Loss: 2.0380...\n",
      "Epoch: 6/100... Step: 222... Loss: 2.0146...\n",
      "Epoch: 7/100... Step: 223... Loss: 2.0272...\n",
      "Epoch: 7/100... Step: 224... Loss: 2.0023...\n",
      "Epoch: 7/100... Step: 225... Loss: 1.9926...\n",
      "Epoch: 7/100... Step: 226... Loss: 1.9971...\n",
      "Epoch: 7/100... Step: 227... Loss: 1.9711...\n",
      "Epoch: 7/100... Step: 228... Loss: 1.9845...\n",
      "Epoch: 7/100... Step: 229... Loss: 1.9714...\n",
      "Epoch: 7/100... Step: 230... Loss: 2.0032...\n",
      "Epoch: 7/100... Step: 231... Loss: 1.9856...\n",
      "Epoch: 7/100... Step: 232... Loss: 1.9775...\n",
      "Epoch: 7/100... Step: 233... Loss: 1.9422...\n",
      "Epoch: 7/100... Step: 234... Loss: 1.9270...\n",
      "Epoch: 7/100... Step: 235... Loss: 1.9327...\n",
      "Epoch: 7/100... Step: 236... Loss: 1.9371...\n",
      "Epoch: 7/100... Step: 237... Loss: 1.9184...\n",
      "Epoch: 7/100... Step: 238... Loss: 1.9179...\n",
      "Epoch: 7/100... Step: 239... Loss: 1.9168...\n",
      "Epoch: 7/100... Step: 240... Loss: 1.9253...\n",
      "Epoch: 7/100... Step: 241... Loss: 1.9197...\n",
      "Epoch: 7/100... Step: 242... Loss: 1.9336...\n",
      "Epoch: 7/100... Step: 243... Loss: 1.9118...\n",
      "Epoch: 7/100... Step: 244... Loss: 1.9135...\n",
      "Epoch: 7/100... Step: 245... Loss: 1.8954...\n",
      "Epoch: 7/100... Step: 246... Loss: 1.9013...\n",
      "Epoch: 7/100... Step: 247... Loss: 1.8994...\n",
      "Epoch: 7/100... Step: 248... Loss: 1.8844...\n",
      "Epoch: 7/100... Step: 249... Loss: 1.8805...\n",
      "Epoch: 7/100... Step: 250... Loss: 1.8646...\n",
      "Epoch: 7/100... Step: 251... Loss: 1.8805...\n",
      "Epoch: 7/100... Step: 252... Loss: 1.8538...\n",
      "Epoch: 7/100... Step: 253... Loss: 1.8729...\n",
      "Epoch: 7/100... Step: 254... Loss: 1.8446...\n",
      "Epoch: 7/100... Step: 255... Loss: 1.8578...\n",
      "Epoch: 7/100... Step: 256... Loss: 1.8279...\n",
      "Epoch: 7/100... Step: 257... Loss: 1.8531...\n",
      "Epoch: 7/100... Step: 258... Loss: 1.8465...\n",
      "Epoch: 7/100... Step: 259... Loss: 1.8453...\n",
      "Epoch: 8/100... Step: 260... Loss: 1.8428...\n",
      "Epoch: 8/100... Step: 261... Loss: 1.8269...\n",
      "Epoch: 8/100... Step: 262... Loss: 1.8153...\n",
      "Epoch: 8/100... Step: 263... Loss: 1.8183...\n",
      "Epoch: 8/100... Step: 264... Loss: 1.7965...\n",
      "Epoch: 8/100... Step: 265... Loss: 1.8230...\n",
      "Epoch: 8/100... Step: 266... Loss: 1.7970...\n",
      "Epoch: 8/100... Step: 267... Loss: 1.8242...\n",
      "Epoch: 8/100... Step: 268... Loss: 1.8024...\n",
      "Epoch: 8/100... Step: 269... Loss: 1.7904...\n",
      "Epoch: 8/100... Step: 270... Loss: 1.7689...\n",
      "Epoch: 8/100... Step: 271... Loss: 1.7489...\n",
      "Epoch: 8/100... Step: 272... Loss: 1.7483...\n",
      "Epoch: 8/100... Step: 273... Loss: 1.7700...\n",
      "Epoch: 8/100... Step: 274... Loss: 1.7728...\n",
      "Epoch: 8/100... Step: 275... Loss: 1.7928...\n",
      "Epoch: 8/100... Step: 276... Loss: 1.7620...\n",
      "Epoch: 8/100... Step: 277... Loss: 1.7755...\n",
      "Epoch: 8/100... Step: 278... Loss: 1.7569...\n",
      "Epoch: 8/100... Step: 279... Loss: 1.7721...\n",
      "Epoch: 8/100... Step: 280... Loss: 1.7472...\n",
      "Epoch: 8/100... Step: 281... Loss: 1.7534...\n",
      "Epoch: 8/100... Step: 282... Loss: 1.7367...\n",
      "Epoch: 8/100... Step: 283... Loss: 1.7280...\n",
      "Epoch: 8/100... Step: 284... Loss: 1.7321...\n",
      "Epoch: 8/100... Step: 285... Loss: 1.7135...\n",
      "Epoch: 8/100... Step: 286... Loss: 1.7152...\n",
      "Epoch: 8/100... Step: 287... Loss: 1.7006...\n",
      "Epoch: 8/100... Step: 288... Loss: 1.7155...\n",
      "Epoch: 8/100... Step: 289... Loss: 1.7113...\n",
      "Epoch: 8/100... Step: 290... Loss: 1.7275...\n",
      "Epoch: 8/100... Step: 291... Loss: 1.6908...\n",
      "Epoch: 8/100... Step: 292... Loss: 1.7203...\n",
      "Epoch: 8/100... Step: 293... Loss: 1.6782...\n",
      "Epoch: 8/100... Step: 294... Loss: 1.6965...\n",
      "Epoch: 8/100... Step: 295... Loss: 1.6803...\n",
      "Epoch: 8/100... Step: 296... Loss: 1.6819...\n",
      "Epoch: 9/100... Step: 297... Loss: 1.6962...\n",
      "Epoch: 9/100... Step: 298... Loss: 1.6728...\n",
      "Epoch: 9/100... Step: 299... Loss: 1.6675...\n",
      "Epoch: 9/100... Step: 300... Loss: 1.6739...\n",
      "Epoch: 9/100... Step: 301... Loss: 1.6373...\n",
      "Epoch: 9/100... Step: 302... Loss: 1.6625...\n",
      "Epoch: 9/100... Step: 303... Loss: 1.6414...\n",
      "Epoch: 9/100... Step: 304... Loss: 1.6708...\n",
      "Epoch: 9/100... Step: 305... Loss: 1.6601...\n",
      "Epoch: 9/100... Step: 306... Loss: 1.6546...\n",
      "Epoch: 9/100... Step: 307... Loss: 1.6262...\n",
      "Epoch: 9/100... Step: 308... Loss: 1.6024...\n",
      "Epoch: 9/100... Step: 309... Loss: 1.6162...\n",
      "Epoch: 9/100... Step: 310... Loss: 1.6351...\n",
      "Epoch: 9/100... Step: 311... Loss: 1.6282...\n",
      "Epoch: 9/100... Step: 312... Loss: 1.6155...\n",
      "Epoch: 9/100... Step: 313... Loss: 1.6098...\n",
      "Epoch: 9/100... Step: 314... Loss: 1.6202...\n",
      "Epoch: 9/100... Step: 315... Loss: 1.6154...\n",
      "Epoch: 9/100... Step: 316... Loss: 1.6294...\n",
      "Epoch: 9/100... Step: 317... Loss: 1.6062...\n",
      "Epoch: 9/100... Step: 318... Loss: 1.6117...\n",
      "Epoch: 9/100... Step: 319... Loss: 1.5933...\n",
      "Epoch: 9/100... Step: 320... Loss: 1.5882...\n",
      "Epoch: 9/100... Step: 321... Loss: 1.5924...\n",
      "Epoch: 9/100... Step: 322... Loss: 1.5745...\n",
      "Epoch: 9/100... Step: 323... Loss: 1.5799...\n",
      "Epoch: 9/100... Step: 324... Loss: 1.5689...\n",
      "Epoch: 9/100... Step: 325... Loss: 1.5789...\n",
      "Epoch: 9/100... Step: 326... Loss: 1.5689...\n",
      "Epoch: 9/100... Step: 327... Loss: 1.5827...\n",
      "Epoch: 9/100... Step: 328... Loss: 1.5542...\n",
      "Epoch: 9/100... Step: 329... Loss: 1.5803...\n",
      "Epoch: 9/100... Step: 330... Loss: 1.5465...\n",
      "Epoch: 9/100... Step: 331... Loss: 1.5760...\n",
      "Epoch: 9/100... Step: 332... Loss: 1.5726...\n",
      "Epoch: 9/100... Step: 333... Loss: 1.5750...\n",
      "Epoch: 10/100... Step: 334... Loss: 1.5953...\n",
      "Epoch: 10/100... Step: 335... Loss: 1.5592...\n",
      "Epoch: 10/100... Step: 336... Loss: 1.5730...\n",
      "Epoch: 10/100... Step: 337... Loss: 1.5721...\n",
      "Epoch: 10/100... Step: 338... Loss: 1.5399...\n",
      "Epoch: 10/100... Step: 339... Loss: 1.5594...\n",
      "Epoch: 10/100... Step: 340... Loss: 1.5455...\n",
      "Epoch: 10/100... Step: 341... Loss: 1.5638...\n",
      "Epoch: 10/100... Step: 342... Loss: 1.5571...\n",
      "Epoch: 10/100... Step: 343... Loss: 1.5477...\n",
      "Epoch: 10/100... Step: 344... Loss: 1.5207...\n",
      "Epoch: 10/100... Step: 345... Loss: 1.5005...\n",
      "Epoch: 10/100... Step: 346... Loss: 1.5045...\n",
      "Epoch: 10/100... Step: 347... Loss: 1.5143...\n",
      "Epoch: 10/100... Step: 348... Loss: 1.5168...\n",
      "Epoch: 10/100... Step: 349... Loss: 1.5012...\n",
      "Epoch: 10/100... Step: 350... Loss: 1.4930...\n",
      "Epoch: 10/100... Step: 351... Loss: 1.5106...\n",
      "Epoch: 10/100... Step: 352... Loss: 1.5039...\n",
      "Epoch: 10/100... Step: 353... Loss: 1.5146...\n",
      "Epoch: 10/100... Step: 354... Loss: 1.4955...\n",
      "Epoch: 10/100... Step: 355... Loss: 1.5020...\n",
      "Epoch: 10/100... Step: 356... Loss: 1.4905...\n",
      "Epoch: 10/100... Step: 357... Loss: 1.4842...\n",
      "Epoch: 10/100... Step: 358... Loss: 1.4926...\n",
      "Epoch: 10/100... Step: 359... Loss: 1.4714...\n",
      "Epoch: 10/100... Step: 360... Loss: 1.4811...\n",
      "Epoch: 10/100... Step: 361... Loss: 1.4626...\n",
      "Epoch: 10/100... Step: 362... Loss: 1.4839...\n",
      "Epoch: 10/100... Step: 363... Loss: 1.4765...\n",
      "Epoch: 10/100... Step: 364... Loss: 1.4921...\n",
      "Epoch: 10/100... Step: 365... Loss: 1.4695...\n",
      "Epoch: 10/100... Step: 366... Loss: 1.4743...\n",
      "Epoch: 10/100... Step: 367... Loss: 1.4448...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/100... Step: 368... Loss: 1.4781...\n",
      "Epoch: 10/100... Step: 369... Loss: 1.4690...\n",
      "Epoch: 10/100... Step: 370... Loss: 1.4658...\n",
      "Epoch: 11/100... Step: 371... Loss: 1.4889...\n",
      "Epoch: 11/100... Step: 372... Loss: 1.4767...\n",
      "Epoch: 11/100... Step: 373... Loss: 1.4654...\n",
      "Epoch: 11/100... Step: 374... Loss: 1.4691...\n",
      "Epoch: 11/100... Step: 375... Loss: 1.4443...\n",
      "Epoch: 11/100... Step: 376... Loss: 1.4611...\n",
      "Epoch: 11/100... Step: 377... Loss: 1.4477...\n",
      "Epoch: 11/100... Step: 378... Loss: 1.4661...\n",
      "Epoch: 11/100... Step: 379... Loss: 1.4637...\n",
      "Epoch: 11/100... Step: 380... Loss: 1.4550...\n",
      "Epoch: 11/100... Step: 381... Loss: 1.4293...\n",
      "Epoch: 11/100... Step: 382... Loss: 1.4125...\n",
      "Epoch: 11/100... Step: 383... Loss: 1.4133...\n",
      "Epoch: 11/100... Step: 384... Loss: 1.4257...\n",
      "Epoch: 11/100... Step: 385... Loss: 1.4255...\n",
      "Epoch: 11/100... Step: 386... Loss: 1.4116...\n",
      "Epoch: 11/100... Step: 387... Loss: 1.4068...\n",
      "Epoch: 11/100... Step: 388... Loss: 1.4232...\n",
      "Epoch: 11/100... Step: 389... Loss: 1.4150...\n",
      "Epoch: 11/100... Step: 390... Loss: 1.4303...\n",
      "Epoch: 11/100... Step: 391... Loss: 1.4114...\n",
      "Epoch: 11/100... Step: 392... Loss: 1.4190...\n",
      "Epoch: 11/100... Step: 393... Loss: 1.4036...\n",
      "Epoch: 11/100... Step: 394... Loss: 1.4062...\n",
      "Epoch: 11/100... Step: 395... Loss: 1.4148...\n",
      "Epoch: 11/100... Step: 396... Loss: 1.3916...\n",
      "Epoch: 11/100... Step: 397... Loss: 1.3993...\n",
      "Epoch: 11/100... Step: 398... Loss: 1.3860...\n",
      "Epoch: 11/100... Step: 399... Loss: 1.4090...\n",
      "Epoch: 11/100... Step: 400... Loss: 1.4037...\n",
      "Epoch: 11/100... Step: 401... Loss: 1.4133...\n",
      "Epoch: 11/100... Step: 402... Loss: 1.3945...\n",
      "Epoch: 11/100... Step: 403... Loss: 1.4162...\n",
      "Epoch: 11/100... Step: 404... Loss: 1.3763...\n",
      "Epoch: 11/100... Step: 405... Loss: 1.4029...\n",
      "Epoch: 11/100... Step: 406... Loss: 1.3891...\n",
      "Epoch: 11/100... Step: 407... Loss: 1.3832...\n",
      "Epoch: 12/100... Step: 408... Loss: 1.4011...\n",
      "Epoch: 12/100... Step: 409... Loss: 1.3797...\n",
      "Epoch: 12/100... Step: 410... Loss: 1.3809...\n",
      "Epoch: 12/100... Step: 411... Loss: 1.3820...\n",
      "Epoch: 12/100... Step: 412... Loss: 1.3645...\n",
      "Epoch: 12/100... Step: 413... Loss: 1.3837...\n",
      "Epoch: 12/100... Step: 414... Loss: 1.3787...\n",
      "Epoch: 12/100... Step: 415... Loss: 1.4035...\n",
      "Epoch: 12/100... Step: 416... Loss: 1.3946...\n",
      "Epoch: 12/100... Step: 417... Loss: 1.3867...\n",
      "Epoch: 12/100... Step: 418... Loss: 1.3651...\n",
      "Epoch: 12/100... Step: 419... Loss: 1.3480...\n",
      "Epoch: 12/100... Step: 420... Loss: 1.3497...\n",
      "Epoch: 12/100... Step: 421... Loss: 1.3679...\n",
      "Epoch: 12/100... Step: 422... Loss: 1.3686...\n",
      "Epoch: 12/100... Step: 423... Loss: 1.3551...\n",
      "Epoch: 12/100... Step: 424... Loss: 1.3449...\n",
      "Epoch: 12/100... Step: 425... Loss: 1.3641...\n",
      "Epoch: 12/100... Step: 426... Loss: 1.3536...\n",
      "Epoch: 12/100... Step: 427... Loss: 1.3697...\n",
      "Epoch: 12/100... Step: 428... Loss: 1.3517...\n",
      "Epoch: 12/100... Step: 429... Loss: 1.3585...\n",
      "Epoch: 12/100... Step: 430... Loss: 1.3453...\n",
      "Epoch: 12/100... Step: 431... Loss: 1.3457...\n",
      "Epoch: 12/100... Step: 432... Loss: 1.3514...\n",
      "Epoch: 12/100... Step: 433... Loss: 1.3310...\n",
      "Epoch: 12/100... Step: 434... Loss: 1.3398...\n",
      "Epoch: 12/100... Step: 435... Loss: 1.3271...\n",
      "Epoch: 12/100... Step: 436... Loss: 1.3527...\n",
      "Epoch: 12/100... Step: 437... Loss: 1.3412...\n",
      "Epoch: 12/100... Step: 438... Loss: 1.3536...\n",
      "Epoch: 12/100... Step: 439... Loss: 1.3291...\n",
      "Epoch: 12/100... Step: 440... Loss: 1.3473...\n",
      "Epoch: 12/100... Step: 441... Loss: 1.3096...\n",
      "Epoch: 12/100... Step: 442... Loss: 1.3434...\n",
      "Epoch: 12/100... Step: 443... Loss: 1.3348...\n",
      "Epoch: 12/100... Step: 444... Loss: 1.3463...\n",
      "Epoch: 13/100... Step: 445... Loss: 1.3581...\n",
      "Epoch: 13/100... Step: 446... Loss: 1.3322...\n",
      "Epoch: 13/100... Step: 447... Loss: 1.3297...\n",
      "Epoch: 13/100... Step: 448... Loss: 1.3442...\n",
      "Epoch: 13/100... Step: 449... Loss: 1.3168...\n",
      "Epoch: 13/100... Step: 450... Loss: 1.3314...\n",
      "Epoch: 13/100... Step: 451... Loss: 1.3206...\n",
      "Epoch: 13/100... Step: 452... Loss: 1.3396...\n",
      "Epoch: 13/100... Step: 453... Loss: 1.3424...\n",
      "Epoch: 13/100... Step: 454... Loss: 1.3340...\n",
      "Epoch: 13/100... Step: 455... Loss: 1.3125...\n",
      "Epoch: 13/100... Step: 456... Loss: 1.2897...\n",
      "Epoch: 13/100... Step: 457... Loss: 1.2950...\n",
      "Epoch: 13/100... Step: 458... Loss: 1.3056...\n",
      "Epoch: 13/100... Step: 459... Loss: 1.3134...\n",
      "Epoch: 13/100... Step: 460... Loss: 1.2940...\n",
      "Epoch: 13/100... Step: 461... Loss: 1.2904...\n",
      "Epoch: 13/100... Step: 462... Loss: 1.3132...\n",
      "Epoch: 13/100... Step: 463... Loss: 1.3017...\n",
      "Epoch: 13/100... Step: 464... Loss: 1.3188...\n",
      "Epoch: 13/100... Step: 465... Loss: 1.3034...\n",
      "Epoch: 13/100... Step: 466... Loss: 1.3113...\n",
      "Epoch: 13/100... Step: 467... Loss: 1.2953...\n",
      "Epoch: 13/100... Step: 468... Loss: 1.2925...\n",
      "Epoch: 13/100... Step: 469... Loss: 1.3065...\n",
      "Epoch: 13/100... Step: 470... Loss: 1.2834...\n",
      "Epoch: 13/100... Step: 471... Loss: 1.2946...\n",
      "Epoch: 13/100... Step: 472... Loss: 1.2822...\n",
      "Epoch: 13/100... Step: 473... Loss: 1.3091...\n",
      "Epoch: 13/100... Step: 474... Loss: 1.3010...\n",
      "Epoch: 13/100... Step: 475... Loss: 1.3069...\n",
      "Epoch: 13/100... Step: 476... Loss: 1.2903...\n",
      "Epoch: 13/100... Step: 477... Loss: 1.3007...\n",
      "Epoch: 13/100... Step: 478... Loss: 1.2664...\n",
      "Epoch: 13/100... Step: 479... Loss: 1.3051...\n",
      "Epoch: 13/100... Step: 480... Loss: 1.2859...\n",
      "Epoch: 13/100... Step: 481... Loss: 1.2810...\n",
      "Epoch: 14/100... Step: 482... Loss: 1.3016...\n",
      "Epoch: 14/100... Step: 483... Loss: 1.2852...\n",
      "Epoch: 14/100... Step: 484... Loss: 1.2903...\n",
      "Epoch: 14/100... Step: 485... Loss: 1.2877...\n",
      "Epoch: 14/100... Step: 486... Loss: 1.2743...\n",
      "Epoch: 14/100... Step: 487... Loss: 1.2837...\n",
      "Epoch: 14/100... Step: 488... Loss: 1.2754...\n",
      "Epoch: 14/100... Step: 489... Loss: 1.2923...\n",
      "Epoch: 14/100... Step: 490... Loss: 1.2973...\n",
      "Epoch: 14/100... Step: 491... Loss: 1.2878...\n",
      "Epoch: 14/100... Step: 492... Loss: 1.2648...\n",
      "Epoch: 14/100... Step: 493... Loss: 1.2470...\n",
      "Epoch: 14/100... Step: 494... Loss: 1.2486...\n",
      "Epoch: 14/100... Step: 495... Loss: 1.2641...\n",
      "Epoch: 14/100... Step: 496... Loss: 1.2717...\n",
      "Epoch: 14/100... Step: 497... Loss: 1.2526...\n",
      "Epoch: 14/100... Step: 498... Loss: 1.2508...\n",
      "Epoch: 14/100... Step: 499... Loss: 1.2700...\n",
      "Epoch: 14/100... Step: 500... Loss: 1.2608...\n",
      "Epoch: 14/100... Step: 501... Loss: 1.2781...\n",
      "Epoch: 14/100... Step: 502... Loss: 1.2627...\n",
      "Epoch: 14/100... Step: 503... Loss: 1.2688...\n",
      "Epoch: 14/100... Step: 504... Loss: 1.2510...\n",
      "Epoch: 14/100... Step: 505... Loss: 1.2547...\n",
      "Epoch: 14/100... Step: 506... Loss: 1.2700...\n",
      "Epoch: 14/100... Step: 507... Loss: 1.2457...\n",
      "Epoch: 14/100... Step: 508... Loss: 1.2546...\n",
      "Epoch: 14/100... Step: 509... Loss: 1.2436...\n",
      "Epoch: 14/100... Step: 510... Loss: 1.2723...\n",
      "Epoch: 14/100... Step: 511... Loss: 1.2620...\n",
      "Epoch: 14/100... Step: 512... Loss: 1.2689...\n",
      "Epoch: 14/100... Step: 513... Loss: 1.2523...\n",
      "Epoch: 14/100... Step: 514... Loss: 1.2624...\n",
      "Epoch: 14/100... Step: 515... Loss: 1.2287...\n",
      "Epoch: 14/100... Step: 516... Loss: 1.2683...\n",
      "Epoch: 14/100... Step: 517... Loss: 1.2478...\n",
      "Epoch: 14/100... Step: 518... Loss: 1.2501...\n",
      "Epoch: 15/100... Step: 519... Loss: 1.2782...\n",
      "Epoch: 15/100... Step: 520... Loss: 1.2471...\n",
      "Epoch: 15/100... Step: 521... Loss: 1.2523...\n",
      "Epoch: 15/100... Step: 522... Loss: 1.2552...\n",
      "Epoch: 15/100... Step: 523... Loss: 1.2405...\n",
      "Epoch: 15/100... Step: 524... Loss: 1.2560...\n",
      "Epoch: 15/100... Step: 525... Loss: 1.2448...\n",
      "Epoch: 15/100... Step: 526... Loss: 1.2622...\n",
      "Epoch: 15/100... Step: 527... Loss: 1.2639...\n",
      "Epoch: 15/100... Step: 528... Loss: 1.2594...\n",
      "Epoch: 15/100... Step: 529... Loss: 1.2344...\n",
      "Epoch: 15/100... Step: 530... Loss: 1.2187...\n",
      "Epoch: 15/100... Step: 531... Loss: 1.2229...\n",
      "Epoch: 15/100... Step: 532... Loss: 1.2339...\n",
      "Epoch: 15/100... Step: 533... Loss: 1.2431...\n",
      "Epoch: 15/100... Step: 534... Loss: 1.2255...\n",
      "Epoch: 15/100... Step: 535... Loss: 1.2153...\n",
      "Epoch: 15/100... Step: 536... Loss: 1.2444...\n",
      "Epoch: 15/100... Step: 537... Loss: 1.2274...\n",
      "Epoch: 15/100... Step: 538... Loss: 1.2482...\n",
      "Epoch: 15/100... Step: 539... Loss: 1.2327...\n",
      "Epoch: 15/100... Step: 540... Loss: 1.2360...\n",
      "Epoch: 15/100... Step: 541... Loss: 1.2243...\n",
      "Epoch: 15/100... Step: 542... Loss: 1.2282...\n",
      "Epoch: 15/100... Step: 543... Loss: 1.2346...\n",
      "Epoch: 15/100... Step: 544... Loss: 1.2200...\n",
      "Epoch: 15/100... Step: 545... Loss: 1.2239...\n",
      "Epoch: 15/100... Step: 546... Loss: 1.2153...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/100... Step: 547... Loss: 1.2400...\n",
      "Epoch: 15/100... Step: 548... Loss: 1.2301...\n",
      "Epoch: 15/100... Step: 549... Loss: 1.2370...\n",
      "Epoch: 15/100... Step: 550... Loss: 1.2174...\n",
      "Epoch: 15/100... Step: 551... Loss: 1.2362...\n",
      "Epoch: 15/100... Step: 552... Loss: 1.1982...\n",
      "Epoch: 15/100... Step: 553... Loss: 1.2397...\n",
      "Epoch: 15/100... Step: 554... Loss: 1.2208...\n",
      "Epoch: 15/100... Step: 555... Loss: 1.2170...\n",
      "Epoch: 16/100... Step: 556... Loss: 1.2518...\n",
      "Epoch: 16/100... Step: 557... Loss: 1.2253...\n",
      "Epoch: 16/100... Step: 558... Loss: 1.2222...\n",
      "Epoch: 16/100... Step: 559... Loss: 1.2331...\n",
      "Epoch: 16/100... Step: 560... Loss: 1.2145...\n",
      "Epoch: 16/100... Step: 561... Loss: 1.2259...\n",
      "Epoch: 16/100... Step: 562... Loss: 1.2164...\n",
      "Epoch: 16/100... Step: 563... Loss: 1.2339...\n",
      "Epoch: 16/100... Step: 564... Loss: 1.2358...\n",
      "Epoch: 16/100... Step: 565... Loss: 1.2281...\n",
      "Epoch: 16/100... Step: 566... Loss: 1.2075...\n",
      "Epoch: 16/100... Step: 567... Loss: 1.1919...\n",
      "Epoch: 16/100... Step: 568... Loss: 1.1923...\n",
      "Epoch: 16/100... Step: 569... Loss: 1.2034...\n",
      "Epoch: 16/100... Step: 570... Loss: 1.2140...\n",
      "Epoch: 16/100... Step: 571... Loss: 1.1969...\n",
      "Epoch: 16/100... Step: 572... Loss: 1.1952...\n",
      "Epoch: 16/100... Step: 573... Loss: 1.2129...\n",
      "Epoch: 16/100... Step: 574... Loss: 1.2014...\n",
      "Epoch: 16/100... Step: 575... Loss: 1.2184...\n",
      "Epoch: 16/100... Step: 576... Loss: 1.2066...\n",
      "Epoch: 16/100... Step: 577... Loss: 1.2074...\n",
      "Epoch: 16/100... Step: 578... Loss: 1.1936...\n",
      "Epoch: 16/100... Step: 579... Loss: 1.1933...\n",
      "Epoch: 16/100... Step: 580... Loss: 1.2090...\n",
      "Epoch: 16/100... Step: 581... Loss: 1.1851...\n",
      "Epoch: 16/100... Step: 582... Loss: 1.1991...\n",
      "Epoch: 16/100... Step: 583... Loss: 1.1876...\n",
      "Epoch: 16/100... Step: 584... Loss: 1.3691...\n",
      "Epoch: 16/100... Step: 585... Loss: 1.3386...\n",
      "Epoch: 16/100... Step: 586... Loss: 1.3261...\n",
      "Epoch: 16/100... Step: 587... Loss: 1.2916...\n",
      "Epoch: 16/100... Step: 588... Loss: 1.3086...\n",
      "Epoch: 16/100... Step: 589... Loss: 1.2652...\n",
      "Epoch: 16/100... Step: 590... Loss: 1.2983...\n",
      "Epoch: 16/100... Step: 591... Loss: 1.2727...\n",
      "Epoch: 16/100... Step: 592... Loss: 1.2643...\n",
      "Epoch: 17/100... Step: 593... Loss: 1.2890...\n",
      "Epoch: 17/100... Step: 594... Loss: 1.2764...\n",
      "Epoch: 17/100... Step: 595... Loss: 1.2683...\n",
      "Epoch: 17/100... Step: 596... Loss: 1.2735...\n",
      "Epoch: 17/100... Step: 597... Loss: 1.2562...\n",
      "Epoch: 17/100... Step: 598... Loss: 1.2596...\n",
      "Epoch: 17/100... Step: 599... Loss: 1.2498...\n",
      "Epoch: 17/100... Step: 600... Loss: 1.2621...\n",
      "Epoch: 17/100... Step: 601... Loss: 1.2596...\n",
      "Epoch: 17/100... Step: 602... Loss: 1.2526...\n",
      "Epoch: 17/100... Step: 603... Loss: 1.2220...\n",
      "Epoch: 17/100... Step: 604... Loss: 1.2064...\n",
      "Epoch: 17/100... Step: 605... Loss: 1.2073...\n",
      "Epoch: 17/100... Step: 606... Loss: 1.2197...\n",
      "Epoch: 17/100... Step: 607... Loss: 1.2212...\n",
      "Epoch: 17/100... Step: 608... Loss: 1.2094...\n",
      "Epoch: 17/100... Step: 609... Loss: 1.2033...\n",
      "Epoch: 17/100... Step: 610... Loss: 1.2212...\n",
      "Epoch: 17/100... Step: 611... Loss: 1.2115...\n",
      "Epoch: 17/100... Step: 612... Loss: 1.2373...\n",
      "Epoch: 17/100... Step: 613... Loss: 1.2146...\n",
      "Epoch: 17/100... Step: 614... Loss: 1.2183...\n",
      "Epoch: 17/100... Step: 615... Loss: 1.2038...\n",
      "Epoch: 17/100... Step: 616... Loss: 1.2032...\n",
      "Epoch: 17/100... Step: 617... Loss: 1.2142...\n",
      "Epoch: 17/100... Step: 618... Loss: 1.1958...\n",
      "Epoch: 17/100... Step: 619... Loss: 1.2034...\n",
      "Epoch: 17/100... Step: 620... Loss: 1.2010...\n",
      "Epoch: 17/100... Step: 621... Loss: 1.2222...\n",
      "Epoch: 17/100... Step: 622... Loss: 1.2120...\n",
      "Epoch: 17/100... Step: 623... Loss: 1.2162...\n",
      "Epoch: 17/100... Step: 624... Loss: 1.1998...\n",
      "Epoch: 17/100... Step: 625... Loss: 1.2249...\n",
      "Epoch: 17/100... Step: 626... Loss: 1.1853...\n",
      "Epoch: 17/100... Step: 627... Loss: 1.2223...\n",
      "Epoch: 17/100... Step: 628... Loss: 1.1963...\n",
      "Epoch: 17/100... Step: 629... Loss: 1.1916...\n",
      "Epoch: 18/100... Step: 630... Loss: 1.2217...\n",
      "Epoch: 18/100... Step: 631... Loss: 1.2006...\n",
      "Epoch: 18/100... Step: 632... Loss: 1.2011...\n",
      "Epoch: 18/100... Step: 633... Loss: 1.2015...\n",
      "Epoch: 18/100... Step: 634... Loss: 1.1885...\n",
      "Epoch: 18/100... Step: 635... Loss: 1.1995...\n",
      "Epoch: 18/100... Step: 636... Loss: 1.1880...\n",
      "Epoch: 18/100... Step: 637... Loss: 1.2030...\n",
      "Epoch: 18/100... Step: 638... Loss: 1.2084...\n",
      "Epoch: 18/100... Step: 639... Loss: 1.1985...\n",
      "Epoch: 18/100... Step: 640... Loss: 1.1743...\n",
      "Epoch: 18/100... Step: 641... Loss: 1.1595...\n",
      "Epoch: 18/100... Step: 642... Loss: 1.1623...\n",
      "Epoch: 18/100... Step: 643... Loss: 1.1748...\n",
      "Epoch: 18/100... Step: 644... Loss: 1.1853...\n",
      "Epoch: 18/100... Step: 645... Loss: 1.1794...\n",
      "Epoch: 18/100... Step: 646... Loss: 1.1693...\n",
      "Epoch: 18/100... Step: 647... Loss: 1.1863...\n",
      "Epoch: 18/100... Step: 648... Loss: 1.1733...\n",
      "Epoch: 18/100... Step: 649... Loss: 1.1902...\n",
      "Epoch: 18/100... Step: 650... Loss: 1.1786...\n",
      "Epoch: 18/100... Step: 651... Loss: 1.1800...\n",
      "Epoch: 18/100... Step: 652... Loss: 1.1646...\n",
      "Epoch: 18/100... Step: 653... Loss: 1.1656...\n",
      "Epoch: 18/100... Step: 654... Loss: 1.1798...\n",
      "Epoch: 18/100... Step: 655... Loss: 1.1547...\n",
      "Epoch: 18/100... Step: 656... Loss: 1.1687...\n",
      "Epoch: 18/100... Step: 657... Loss: 1.1649...\n",
      "Epoch: 18/100... Step: 658... Loss: 1.1867...\n",
      "Epoch: 18/100... Step: 659... Loss: 1.1765...\n",
      "Epoch: 18/100... Step: 660... Loss: 1.1800...\n",
      "Epoch: 18/100... Step: 661... Loss: 1.1678...\n",
      "Epoch: 18/100... Step: 662... Loss: 1.1783...\n",
      "Epoch: 18/100... Step: 663... Loss: 1.1500...\n",
      "Epoch: 18/100... Step: 664... Loss: 1.1826...\n",
      "Epoch: 18/100... Step: 665... Loss: 1.1587...\n",
      "Epoch: 18/100... Step: 666... Loss: 1.1546...\n",
      "Epoch: 19/100... Step: 667... Loss: 1.1823...\n",
      "Epoch: 19/100... Step: 668... Loss: 1.1662...\n",
      "Epoch: 19/100... Step: 669... Loss: 1.1610...\n",
      "Epoch: 19/100... Step: 670... Loss: 1.1689...\n",
      "Epoch: 19/100... Step: 671... Loss: 1.1585...\n",
      "Epoch: 19/100... Step: 672... Loss: 1.1661...\n",
      "Epoch: 19/100... Step: 673... Loss: 1.1600...\n",
      "Epoch: 19/100... Step: 674... Loss: 1.1734...\n",
      "Epoch: 19/100... Step: 675... Loss: 1.1772...\n",
      "Epoch: 19/100... Step: 676... Loss: 1.1725...\n",
      "Epoch: 19/100... Step: 677... Loss: 1.1472...\n",
      "Epoch: 19/100... Step: 678... Loss: 1.1332...\n",
      "Epoch: 19/100... Step: 679... Loss: 1.1320...\n",
      "Epoch: 19/100... Step: 680... Loss: 1.1492...\n",
      "Epoch: 19/100... Step: 681... Loss: 1.1925...\n",
      "Epoch: 19/100... Step: 682... Loss: 1.1730...\n",
      "Epoch: 19/100... Step: 683... Loss: 1.1663...\n",
      "Epoch: 19/100... Step: 684... Loss: 1.1856...\n",
      "Epoch: 19/100... Step: 685... Loss: 1.1651...\n",
      "Epoch: 19/100... Step: 686... Loss: 1.1847...\n",
      "Epoch: 19/100... Step: 687... Loss: 1.1675...\n",
      "Epoch: 19/100... Step: 688... Loss: 1.1693...\n",
      "Epoch: 19/100... Step: 689... Loss: 1.1556...\n",
      "Epoch: 19/100... Step: 690... Loss: 1.1511...\n",
      "Epoch: 19/100... Step: 691... Loss: 1.1697...\n",
      "Epoch: 19/100... Step: 692... Loss: 1.1424...\n",
      "Epoch: 19/100... Step: 693... Loss: 1.1514...\n",
      "Epoch: 19/100... Step: 694... Loss: 1.1494...\n",
      "Epoch: 19/100... Step: 695... Loss: 1.1676...\n",
      "Epoch: 19/100... Step: 696... Loss: 1.1610...\n",
      "Epoch: 19/100... Step: 697... Loss: 1.1611...\n",
      "Epoch: 19/100... Step: 698... Loss: 1.1507...\n",
      "Epoch: 19/100... Step: 699... Loss: 1.1621...\n",
      "Epoch: 19/100... Step: 700... Loss: 1.1300...\n",
      "Epoch: 19/100... Step: 701... Loss: 1.1707...\n",
      "Epoch: 19/100... Step: 702... Loss: 1.1381...\n",
      "Epoch: 19/100... Step: 703... Loss: 1.1395...\n",
      "Epoch: 20/100... Step: 704... Loss: 1.1662...\n",
      "Epoch: 20/100... Step: 705... Loss: 1.1569...\n",
      "Epoch: 20/100... Step: 706... Loss: 1.1488...\n",
      "Epoch: 20/100... Step: 707... Loss: 1.1562...\n",
      "Epoch: 20/100... Step: 708... Loss: 1.1481...\n",
      "Epoch: 20/100... Step: 709... Loss: 1.1533...\n",
      "Epoch: 20/100... Step: 710... Loss: 1.1436...\n",
      "Epoch: 20/100... Step: 711... Loss: 1.1597...\n",
      "Epoch: 20/100... Step: 712... Loss: 1.1675...\n",
      "Epoch: 20/100... Step: 713... Loss: 1.1634...\n",
      "Epoch: 20/100... Step: 714... Loss: 1.1375...\n",
      "Epoch: 20/100... Step: 715... Loss: 1.1234...\n",
      "Epoch: 20/100... Step: 716... Loss: 1.1204...\n",
      "Epoch: 20/100... Step: 717... Loss: 1.1351...\n",
      "Epoch: 20/100... Step: 718... Loss: 1.1376...\n",
      "Epoch: 20/100... Step: 719... Loss: 1.1302...\n",
      "Epoch: 20/100... Step: 720... Loss: 1.1276...\n",
      "Epoch: 20/100... Step: 721... Loss: 1.1458...\n",
      "Epoch: 20/100... Step: 722... Loss: 1.1332...\n",
      "Epoch: 20/100... Step: 723... Loss: 1.1492...\n",
      "Epoch: 20/100... Step: 724... Loss: 1.1399...\n",
      "Epoch: 20/100... Step: 725... Loss: 1.1423...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/100... Step: 726... Loss: 1.1224...\n",
      "Epoch: 20/100... Step: 727... Loss: 1.1387...\n",
      "Epoch: 20/100... Step: 728... Loss: 1.1914...\n",
      "Epoch: 20/100... Step: 729... Loss: 1.1659...\n",
      "Epoch: 20/100... Step: 730... Loss: 1.1566...\n",
      "Epoch: 20/100... Step: 731... Loss: 1.1426...\n",
      "Epoch: 20/100... Step: 732... Loss: 1.1686...\n",
      "Epoch: 20/100... Step: 733... Loss: 1.1609...\n",
      "Epoch: 20/100... Step: 734... Loss: 1.1611...\n",
      "Epoch: 20/100... Step: 735... Loss: 1.1486...\n",
      "Epoch: 20/100... Step: 736... Loss: 1.1608...\n",
      "Epoch: 20/100... Step: 737... Loss: 1.1270...\n",
      "Epoch: 20/100... Step: 738... Loss: 1.1631...\n",
      "Epoch: 20/100... Step: 739... Loss: 1.1333...\n",
      "Epoch: 20/100... Step: 740... Loss: 1.1293...\n",
      "Epoch: 21/100... Step: 741... Loss: 1.1603...\n",
      "Epoch: 21/100... Step: 742... Loss: 1.1399...\n",
      "Epoch: 21/100... Step: 743... Loss: 1.1365...\n",
      "Epoch: 21/100... Step: 744... Loss: 1.1447...\n",
      "Epoch: 21/100... Step: 745... Loss: 1.1334...\n",
      "Epoch: 21/100... Step: 746... Loss: 1.1390...\n",
      "Epoch: 21/100... Step: 747... Loss: 1.1302...\n",
      "Epoch: 21/100... Step: 748... Loss: 1.1457...\n",
      "Epoch: 21/100... Step: 749... Loss: 1.1477...\n",
      "Epoch: 21/100... Step: 750... Loss: 1.1463...\n",
      "Epoch: 21/100... Step: 751... Loss: 1.1191...\n",
      "Epoch: 21/100... Step: 752... Loss: 1.1059...\n",
      "Epoch: 21/100... Step: 753... Loss: 1.1097...\n",
      "Epoch: 21/100... Step: 754... Loss: 1.1189...\n",
      "Epoch: 21/100... Step: 755... Loss: 1.1245...\n",
      "Epoch: 21/100... Step: 756... Loss: 1.1120...\n",
      "Epoch: 21/100... Step: 757... Loss: 1.1117...\n",
      "Epoch: 21/100... Step: 758... Loss: 1.1303...\n",
      "Epoch: 21/100... Step: 759... Loss: 1.1173...\n",
      "Epoch: 21/100... Step: 760... Loss: 1.1361...\n",
      "Epoch: 21/100... Step: 761... Loss: 1.1267...\n",
      "Epoch: 21/100... Step: 762... Loss: 1.1265...\n",
      "Epoch: 21/100... Step: 763... Loss: 1.1085...\n",
      "Epoch: 21/100... Step: 764... Loss: 1.1068...\n",
      "Epoch: 21/100... Step: 765... Loss: 1.1294...\n",
      "Epoch: 21/100... Step: 766... Loss: 1.1073...\n",
      "Epoch: 21/100... Step: 767... Loss: 1.1142...\n",
      "Epoch: 21/100... Step: 768... Loss: 1.1118...\n",
      "Epoch: 21/100... Step: 769... Loss: 1.1355...\n",
      "Epoch: 21/100... Step: 770... Loss: 1.1263...\n",
      "Epoch: 21/100... Step: 771... Loss: 1.1305...\n",
      "Epoch: 21/100... Step: 772... Loss: 1.1168...\n",
      "Epoch: 21/100... Step: 773... Loss: 1.1300...\n",
      "Epoch: 21/100... Step: 774... Loss: 1.1025...\n",
      "Epoch: 21/100... Step: 775... Loss: 1.1389...\n",
      "Epoch: 21/100... Step: 776... Loss: 1.1101...\n",
      "Epoch: 21/100... Step: 777... Loss: 1.1053...\n",
      "Epoch: 22/100... Step: 778... Loss: 1.1338...\n",
      "Epoch: 22/100... Step: 779... Loss: 1.1166...\n",
      "Epoch: 22/100... Step: 780... Loss: 1.1141...\n",
      "Epoch: 22/100... Step: 781... Loss: 1.1205...\n",
      "Epoch: 22/100... Step: 782... Loss: 1.1118...\n",
      "Epoch: 22/100... Step: 783... Loss: 1.1169...\n",
      "Epoch: 22/100... Step: 784... Loss: 1.1110...\n",
      "Epoch: 22/100... Step: 785... Loss: 1.1279...\n",
      "Epoch: 22/100... Step: 786... Loss: 1.1266...\n",
      "Epoch: 22/100... Step: 787... Loss: 1.1265...\n",
      "Epoch: 22/100... Step: 788... Loss: 1.1038...\n",
      "Epoch: 22/100... Step: 789... Loss: 1.0893...\n",
      "Epoch: 22/100... Step: 790... Loss: 1.0878...\n",
      "Epoch: 22/100... Step: 791... Loss: 1.0996...\n",
      "Epoch: 22/100... Step: 792... Loss: 1.1068...\n",
      "Epoch: 22/100... Step: 793... Loss: 1.0943...\n",
      "Epoch: 22/100... Step: 794... Loss: 1.0965...\n",
      "Epoch: 22/100... Step: 795... Loss: 1.1097...\n",
      "Epoch: 22/100... Step: 796... Loss: 1.0977...\n",
      "Epoch: 22/100... Step: 797... Loss: 1.1195...\n",
      "Epoch: 22/100... Step: 798... Loss: 1.1071...\n",
      "Epoch: 22/100... Step: 799... Loss: 1.1103...\n",
      "Epoch: 22/100... Step: 800... Loss: 1.0929...\n",
      "Epoch: 22/100... Step: 801... Loss: 1.0902...\n",
      "Epoch: 22/100... Step: 802... Loss: 1.1139...\n",
      "Epoch: 22/100... Step: 803... Loss: 1.0876...\n",
      "Epoch: 22/100... Step: 804... Loss: 1.0984...\n",
      "Epoch: 22/100... Step: 805... Loss: 1.0962...\n",
      "Epoch: 22/100... Step: 806... Loss: 1.1177...\n",
      "Epoch: 22/100... Step: 807... Loss: 1.1101...\n",
      "Epoch: 22/100... Step: 808... Loss: 1.1117...\n",
      "Epoch: 22/100... Step: 809... Loss: 1.1007...\n",
      "Epoch: 22/100... Step: 810... Loss: 1.1139...\n",
      "Epoch: 22/100... Step: 811... Loss: 1.0891...\n",
      "Epoch: 22/100... Step: 812... Loss: 1.1232...\n",
      "Epoch: 22/100... Step: 813... Loss: 1.0960...\n",
      "Epoch: 22/100... Step: 814... Loss: 1.0887...\n",
      "Epoch: 23/100... Step: 815... Loss: 1.1203...\n",
      "Epoch: 23/100... Step: 816... Loss: 1.1058...\n",
      "Epoch: 23/100... Step: 817... Loss: 1.1021...\n",
      "Epoch: 23/100... Step: 818... Loss: 1.1085...\n",
      "Epoch: 23/100... Step: 819... Loss: 1.1014...\n",
      "Epoch: 23/100... Step: 820... Loss: 1.1075...\n",
      "Epoch: 23/100... Step: 821... Loss: 1.0984...\n",
      "Epoch: 23/100... Step: 822... Loss: 1.1150...\n",
      "Epoch: 23/100... Step: 823... Loss: 1.1163...\n",
      "Epoch: 23/100... Step: 824... Loss: 1.1182...\n",
      "Epoch: 23/100... Step: 825... Loss: 1.0923...\n",
      "Epoch: 23/100... Step: 826... Loss: 1.0784...\n",
      "Epoch: 23/100... Step: 827... Loss: 1.0804...\n",
      "Epoch: 23/100... Step: 828... Loss: 1.0870...\n",
      "Epoch: 23/100... Step: 829... Loss: 1.0921...\n",
      "Epoch: 23/100... Step: 830... Loss: 1.0811...\n",
      "Epoch: 23/100... Step: 831... Loss: 1.0848...\n",
      "Epoch: 23/100... Step: 832... Loss: 1.1037...\n",
      "Epoch: 23/100... Step: 833... Loss: 1.0881...\n",
      "Epoch: 23/100... Step: 834... Loss: 1.1068...\n",
      "Epoch: 23/100... Step: 835... Loss: 1.0989...\n",
      "Epoch: 23/100... Step: 836... Loss: 1.0976...\n",
      "Epoch: 23/100... Step: 837... Loss: 1.0839...\n",
      "Epoch: 23/100... Step: 838... Loss: 1.0830...\n",
      "Epoch: 23/100... Step: 839... Loss: 1.1066...\n",
      "Epoch: 23/100... Step: 840... Loss: 1.0763...\n",
      "Epoch: 23/100... Step: 841... Loss: 1.0906...\n",
      "Epoch: 23/100... Step: 842... Loss: 1.0850...\n",
      "Epoch: 23/100... Step: 843... Loss: 1.1074...\n",
      "Epoch: 23/100... Step: 844... Loss: 1.0978...\n",
      "Epoch: 23/100... Step: 845... Loss: 1.1022...\n",
      "Epoch: 23/100... Step: 846... Loss: 1.0941...\n",
      "Epoch: 23/100... Step: 847... Loss: 1.1011...\n",
      "Epoch: 23/100... Step: 848... Loss: 1.0810...\n",
      "Epoch: 23/100... Step: 849... Loss: 1.1122...\n",
      "Epoch: 23/100... Step: 850... Loss: 1.0875...\n",
      "Epoch: 23/100... Step: 851... Loss: 1.0814...\n",
      "Epoch: 24/100... Step: 852... Loss: 1.1085...\n",
      "Epoch: 24/100... Step: 853... Loss: 1.0944...\n",
      "Epoch: 24/100... Step: 854... Loss: 1.0870...\n",
      "Epoch: 24/100... Step: 855... Loss: 1.0991...\n",
      "Epoch: 24/100... Step: 856... Loss: 1.0909...\n",
      "Epoch: 24/100... Step: 857... Loss: 1.0981...\n",
      "Epoch: 24/100... Step: 858... Loss: 1.0902...\n",
      "Epoch: 24/100... Step: 859... Loss: 1.1033...\n",
      "Epoch: 24/100... Step: 860... Loss: 1.1042...\n",
      "Epoch: 24/100... Step: 861... Loss: 1.1065...\n",
      "Epoch: 24/100... Step: 862... Loss: 1.0816...\n",
      "Epoch: 24/100... Step: 863... Loss: 1.0676...\n",
      "Epoch: 24/100... Step: 864... Loss: 1.0688...\n",
      "Epoch: 24/100... Step: 865... Loss: 1.0787...\n",
      "Epoch: 24/100... Step: 866... Loss: 1.0791...\n",
      "Epoch: 24/100... Step: 867... Loss: 1.0736...\n",
      "Epoch: 24/100... Step: 868... Loss: 1.0744...\n",
      "Epoch: 24/100... Step: 869... Loss: 1.0865...\n",
      "Epoch: 24/100... Step: 870... Loss: 1.0804...\n",
      "Epoch: 24/100... Step: 871... Loss: 1.0962...\n",
      "Epoch: 24/100... Step: 872... Loss: 1.0844...\n",
      "Epoch: 24/100... Step: 873... Loss: 1.0871...\n",
      "Epoch: 24/100... Step: 874... Loss: 1.0707...\n",
      "Epoch: 24/100... Step: 875... Loss: 1.0706...\n",
      "Epoch: 24/100... Step: 876... Loss: 1.0908...\n",
      "Epoch: 24/100... Step: 877... Loss: 1.0648...\n",
      "Epoch: 24/100... Step: 878... Loss: 1.0803...\n",
      "Epoch: 24/100... Step: 879... Loss: 1.0765...\n",
      "Epoch: 24/100... Step: 880... Loss: 1.1022...\n",
      "Epoch: 24/100... Step: 881... Loss: 1.0904...\n",
      "Epoch: 24/100... Step: 882... Loss: 1.0987...\n",
      "Epoch: 24/100... Step: 883... Loss: 1.0833...\n",
      "Epoch: 24/100... Step: 884... Loss: 1.0964...\n",
      "Epoch: 24/100... Step: 885... Loss: 1.0684...\n",
      "Epoch: 24/100... Step: 886... Loss: 1.1067...\n",
      "Epoch: 24/100... Step: 887... Loss: 1.0770...\n",
      "Epoch: 24/100... Step: 888... Loss: 1.0839...\n",
      "Epoch: 25/100... Step: 889... Loss: 1.1058...\n",
      "Epoch: 25/100... Step: 890... Loss: 1.0871...\n",
      "Epoch: 25/100... Step: 891... Loss: 1.0813...\n",
      "Epoch: 25/100... Step: 892... Loss: 1.0937...\n",
      "Epoch: 25/100... Step: 893... Loss: 1.0869...\n",
      "Epoch: 25/100... Step: 894... Loss: 1.0881...\n",
      "Epoch: 25/100... Step: 895... Loss: 1.0870...\n",
      "Epoch: 25/100... Step: 896... Loss: 1.1024...\n",
      "Epoch: 25/100... Step: 897... Loss: 1.2074...\n",
      "Epoch: 25/100... Step: 898... Loss: 1.1971...\n",
      "Epoch: 25/100... Step: 899... Loss: 1.1744...\n",
      "Epoch: 25/100... Step: 900... Loss: 1.1697...\n",
      "Epoch: 25/100... Step: 901... Loss: 1.1616...\n",
      "Epoch: 25/100... Step: 902... Loss: 1.1675...\n",
      "Epoch: 25/100... Step: 903... Loss: 1.1653...\n",
      "Epoch: 25/100... Step: 904... Loss: 1.1553...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25/100... Step: 905... Loss: 1.1470...\n",
      "Epoch: 25/100... Step: 906... Loss: 1.1606...\n",
      "Epoch: 25/100... Step: 907... Loss: 1.1434...\n",
      "Epoch: 25/100... Step: 908... Loss: 1.1559...\n",
      "Epoch: 25/100... Step: 909... Loss: 1.1426...\n",
      "Epoch: 25/100... Step: 910... Loss: 1.1420...\n",
      "Epoch: 25/100... Step: 911... Loss: 1.1250...\n",
      "Epoch: 25/100... Step: 912... Loss: 1.2737...\n",
      "Epoch: 25/100... Step: 913... Loss: 1.2213...\n",
      "Epoch: 25/100... Step: 914... Loss: 1.2173...\n",
      "Epoch: 25/100... Step: 915... Loss: 1.2172...\n",
      "Epoch: 25/100... Step: 916... Loss: 1.1987...\n",
      "Epoch: 25/100... Step: 917... Loss: 1.2180...\n",
      "Epoch: 25/100... Step: 918... Loss: 1.2034...\n",
      "Epoch: 25/100... Step: 919... Loss: 1.1999...\n",
      "Epoch: 25/100... Step: 920... Loss: 1.1819...\n",
      "Epoch: 25/100... Step: 921... Loss: 1.1947...\n",
      "Epoch: 25/100... Step: 922... Loss: 1.1541...\n",
      "Epoch: 25/100... Step: 923... Loss: 1.1849...\n",
      "Epoch: 25/100... Step: 924... Loss: 1.1603...\n",
      "Epoch: 25/100... Step: 925... Loss: 1.1509...\n",
      "Epoch: 26/100... Step: 926... Loss: 1.1738...\n",
      "Epoch: 26/100... Step: 927... Loss: 1.1521...\n",
      "Epoch: 26/100... Step: 928... Loss: 1.1452...\n",
      "Epoch: 26/100... Step: 929... Loss: 1.1527...\n",
      "Epoch: 26/100... Step: 930... Loss: 1.1393...\n",
      "Epoch: 26/100... Step: 931... Loss: 1.1437...\n",
      "Epoch: 26/100... Step: 932... Loss: 1.1322...\n",
      "Epoch: 26/100... Step: 933... Loss: 1.1585...\n",
      "Epoch: 26/100... Step: 934... Loss: 1.1475...\n",
      "Epoch: 26/100... Step: 935... Loss: 1.1474...\n",
      "Epoch: 26/100... Step: 936... Loss: 1.1197...\n",
      "Epoch: 26/100... Step: 937... Loss: 1.1078...\n",
      "Epoch: 26/100... Step: 938... Loss: 1.1017...\n",
      "Epoch: 26/100... Step: 939... Loss: 1.1116...\n",
      "Epoch: 26/100... Step: 940... Loss: 1.1128...\n",
      "Epoch: 26/100... Step: 941... Loss: 1.1053...\n",
      "Epoch: 26/100... Step: 942... Loss: 1.0989...\n",
      "Epoch: 26/100... Step: 943... Loss: 1.1148...\n",
      "Epoch: 26/100... Step: 944... Loss: 1.0988...\n",
      "Epoch: 26/100... Step: 945... Loss: 1.1157...\n",
      "Epoch: 26/100... Step: 946... Loss: 1.1045...\n",
      "Epoch: 26/100... Step: 947... Loss: 1.1057...\n",
      "Epoch: 26/100... Step: 948... Loss: 1.0852...\n",
      "Epoch: 26/100... Step: 949... Loss: 1.0918...\n",
      "Epoch: 26/100... Step: 950... Loss: 1.1084...\n",
      "Epoch: 26/100... Step: 951... Loss: 1.0808...\n",
      "Epoch: 26/100... Step: 952... Loss: 1.0962...\n",
      "Epoch: 26/100... Step: 953... Loss: 1.0890...\n",
      "Epoch: 26/100... Step: 954... Loss: 1.1097...\n",
      "Epoch: 26/100... Step: 955... Loss: 1.1011...\n",
      "Epoch: 26/100... Step: 956... Loss: 1.1028...\n",
      "Epoch: 26/100... Step: 957... Loss: 1.0949...\n",
      "Epoch: 26/100... Step: 958... Loss: 1.1060...\n",
      "Epoch: 26/100... Step: 959... Loss: 1.0758...\n",
      "Epoch: 26/100... Step: 960... Loss: 1.1072...\n",
      "Epoch: 26/100... Step: 961... Loss: 1.0831...\n",
      "Epoch: 26/100... Step: 962... Loss: 1.0819...\n",
      "Epoch: 27/100... Step: 963... Loss: 1.1036...\n",
      "Epoch: 27/100... Step: 964... Loss: 1.0880...\n",
      "Epoch: 27/100... Step: 965... Loss: 1.0835...\n",
      "Epoch: 27/100... Step: 966... Loss: 1.0902...\n",
      "Epoch: 27/100... Step: 967... Loss: 1.0825...\n",
      "Epoch: 27/100... Step: 968... Loss: 1.0888...\n",
      "Epoch: 27/100... Step: 969... Loss: 1.0778...\n",
      "Epoch: 27/100... Step: 970... Loss: 1.1048...\n",
      "Epoch: 27/100... Step: 971... Loss: 1.0945...\n",
      "Epoch: 27/100... Step: 972... Loss: 1.0995...\n",
      "Epoch: 27/100... Step: 973... Loss: 1.0718...\n",
      "Epoch: 27/100... Step: 974... Loss: 1.0623...\n",
      "Epoch: 27/100... Step: 975... Loss: 1.0616...\n",
      "Epoch: 27/100... Step: 976... Loss: 1.0697...\n",
      "Epoch: 27/100... Step: 977... Loss: 1.0727...\n",
      "Epoch: 27/100... Step: 978... Loss: 1.0636...\n",
      "Epoch: 27/100... Step: 979... Loss: 1.0615...\n",
      "Epoch: 27/100... Step: 980... Loss: 1.0739...\n",
      "Epoch: 27/100... Step: 981... Loss: 1.0631...\n",
      "Epoch: 27/100... Step: 982... Loss: 1.0814...\n",
      "Epoch: 27/100... Step: 983... Loss: 1.0689...\n",
      "Epoch: 27/100... Step: 984... Loss: 1.0753...\n",
      "Epoch: 27/100... Step: 985... Loss: 1.0548...\n",
      "Epoch: 27/100... Step: 986... Loss: 1.0595...\n",
      "Epoch: 27/100... Step: 987... Loss: 1.0781...\n",
      "Epoch: 27/100... Step: 988... Loss: 1.0500...\n",
      "Epoch: 27/100... Step: 989... Loss: 1.0629...\n",
      "Epoch: 27/100... Step: 990... Loss: 1.0587...\n",
      "Epoch: 27/100... Step: 991... Loss: 1.0761...\n",
      "Epoch: 27/100... Step: 992... Loss: 1.0712...\n",
      "Epoch: 27/100... Step: 993... Loss: 1.0739...\n",
      "Epoch: 27/100... Step: 994... Loss: 1.0635...\n",
      "Epoch: 27/100... Step: 995... Loss: 1.0740...\n",
      "Epoch: 27/100... Step: 996... Loss: 1.0439...\n",
      "Epoch: 27/100... Step: 997... Loss: 1.0779...\n",
      "Epoch: 27/100... Step: 998... Loss: 1.0516...\n",
      "Epoch: 27/100... Step: 999... Loss: 1.0508...\n",
      "Epoch: 28/100... Step: 1000... Loss: 1.0752...\n",
      "Epoch: 28/100... Step: 1001... Loss: 1.0592...\n",
      "Epoch: 28/100... Step: 1002... Loss: 1.0556...\n",
      "Epoch: 28/100... Step: 1003... Loss: 1.0659...\n",
      "Epoch: 28/100... Step: 1004... Loss: 1.0574...\n",
      "Epoch: 28/100... Step: 1005... Loss: 1.0647...\n",
      "Epoch: 28/100... Step: 1006... Loss: 1.0540...\n",
      "Epoch: 28/100... Step: 1007... Loss: 1.0770...\n",
      "Epoch: 28/100... Step: 1008... Loss: 1.0695...\n",
      "Epoch: 28/100... Step: 1009... Loss: 1.0740...\n",
      "Epoch: 28/100... Step: 1010... Loss: 1.0530...\n",
      "Epoch: 28/100... Step: 1011... Loss: 1.0433...\n",
      "Epoch: 28/100... Step: 1012... Loss: 1.0342...\n",
      "Epoch: 28/100... Step: 1013... Loss: 1.0484...\n",
      "Epoch: 28/100... Step: 1014... Loss: 1.0492...\n",
      "Epoch: 28/100... Step: 1015... Loss: 1.0405...\n",
      "Epoch: 28/100... Step: 1016... Loss: 1.0418...\n",
      "Epoch: 28/100... Step: 1017... Loss: 1.0507...\n",
      "Epoch: 28/100... Step: 1018... Loss: 1.0376...\n",
      "Epoch: 28/100... Step: 1019... Loss: 1.0588...\n",
      "Epoch: 28/100... Step: 1020... Loss: 1.0431...\n",
      "Epoch: 28/100... Step: 1021... Loss: 1.0507...\n",
      "Epoch: 28/100... Step: 1022... Loss: 1.0324...\n",
      "Epoch: 28/100... Step: 1023... Loss: 1.0354...\n",
      "Epoch: 28/100... Step: 1024... Loss: 1.0540...\n",
      "Epoch: 28/100... Step: 1025... Loss: 1.0280...\n",
      "Epoch: 28/100... Step: 1026... Loss: 1.0361...\n",
      "Epoch: 28/100... Step: 1027... Loss: 1.0359...\n",
      "Epoch: 28/100... Step: 1028... Loss: 1.0565...\n",
      "Epoch: 28/100... Step: 1029... Loss: 1.0470...\n",
      "Epoch: 28/100... Step: 1030... Loss: 1.0518...\n",
      "Epoch: 28/100... Step: 1031... Loss: 1.0443...\n",
      "Epoch: 28/100... Step: 1032... Loss: 1.0538...\n",
      "Epoch: 28/100... Step: 1033... Loss: 1.0270...\n",
      "Epoch: 28/100... Step: 1034... Loss: 1.0567...\n",
      "Epoch: 28/100... Step: 1035... Loss: 1.0313...\n",
      "Epoch: 28/100... Step: 1036... Loss: 1.0296...\n",
      "Epoch: 29/100... Step: 1037... Loss: 1.0541...\n",
      "Epoch: 29/100... Step: 1038... Loss: 1.0388...\n",
      "Epoch: 29/100... Step: 1039... Loss: 1.0313...\n",
      "Epoch: 29/100... Step: 1040... Loss: 1.0423...\n",
      "Epoch: 29/100... Step: 1041... Loss: 1.0365...\n",
      "Epoch: 29/100... Step: 1042... Loss: 1.0442...\n",
      "Epoch: 29/100... Step: 1043... Loss: 1.0340...\n",
      "Epoch: 29/100... Step: 1044... Loss: 1.0556...\n",
      "Epoch: 29/100... Step: 1045... Loss: 1.0479...\n",
      "Epoch: 29/100... Step: 1046... Loss: 1.0536...\n",
      "Epoch: 29/100... Step: 1047... Loss: 1.0269...\n",
      "Epoch: 29/100... Step: 1048... Loss: 1.0189...\n",
      "Epoch: 29/100... Step: 1049... Loss: 1.0162...\n",
      "Epoch: 29/100... Step: 1050... Loss: 1.0252...\n",
      "Epoch: 29/100... Step: 1051... Loss: 1.0344...\n",
      "Epoch: 29/100... Step: 1052... Loss: 1.0288...\n",
      "Epoch: 29/100... Step: 1053... Loss: 1.0337...\n",
      "Epoch: 29/100... Step: 1054... Loss: 1.0443...\n",
      "Epoch: 29/100... Step: 1055... Loss: 1.0258...\n",
      "Epoch: 29/100... Step: 1056... Loss: 1.0563...\n",
      "Epoch: 29/100... Step: 1057... Loss: 1.0324...\n",
      "Epoch: 29/100... Step: 1058... Loss: 1.0401...\n",
      "Epoch: 29/100... Step: 1059... Loss: 1.0229...\n",
      "Epoch: 29/100... Step: 1060... Loss: 1.0257...\n",
      "Epoch: 29/100... Step: 1061... Loss: 1.0425...\n",
      "Epoch: 29/100... Step: 1062... Loss: 1.0184...\n",
      "Epoch: 29/100... Step: 1063... Loss: 1.0233...\n",
      "Epoch: 29/100... Step: 1064... Loss: 1.0236...\n",
      "Epoch: 29/100... Step: 1065... Loss: 1.0497...\n",
      "Epoch: 29/100... Step: 1066... Loss: 1.0354...\n",
      "Epoch: 29/100... Step: 1067... Loss: 1.0409...\n",
      "Epoch: 29/100... Step: 1068... Loss: 1.0346...\n",
      "Epoch: 29/100... Step: 1069... Loss: 1.0429...\n",
      "Epoch: 29/100... Step: 1070... Loss: 1.0147...\n",
      "Epoch: 29/100... Step: 1071... Loss: 1.0539...\n",
      "Epoch: 29/100... Step: 1072... Loss: 1.0258...\n",
      "Epoch: 29/100... Step: 1073... Loss: 1.0208...\n",
      "Epoch: 30/100... Step: 1074... Loss: 1.0508...\n",
      "Epoch: 30/100... Step: 1075... Loss: 1.0281...\n",
      "Epoch: 30/100... Step: 1076... Loss: 1.0200...\n",
      "Epoch: 30/100... Step: 1077... Loss: 1.0390...\n",
      "Epoch: 30/100... Step: 1078... Loss: 1.0276...\n",
      "Epoch: 30/100... Step: 1079... Loss: 1.0378...\n",
      "Epoch: 30/100... Step: 1080... Loss: 1.0290...\n",
      "Epoch: 30/100... Step: 1081... Loss: 1.0474...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30/100... Step: 1082... Loss: 1.0433...\n",
      "Epoch: 30/100... Step: 1083... Loss: 1.0449...\n",
      "Epoch: 30/100... Step: 1084... Loss: 1.0250...\n",
      "Epoch: 30/100... Step: 1085... Loss: 1.0140...\n",
      "Epoch: 30/100... Step: 1086... Loss: 1.0092...\n",
      "Epoch: 30/100... Step: 1087... Loss: 1.0211...\n",
      "Epoch: 30/100... Step: 1088... Loss: 1.0219...\n",
      "Epoch: 30/100... Step: 1089... Loss: 1.0176...\n",
      "Epoch: 30/100... Step: 1090... Loss: 1.0191...\n",
      "Epoch: 30/100... Step: 1091... Loss: 1.0251...\n",
      "Epoch: 30/100... Step: 1092... Loss: 1.0155...\n",
      "Epoch: 30/100... Step: 1093... Loss: 1.0359...\n",
      "Epoch: 30/100... Step: 1094... Loss: 1.0250...\n",
      "Epoch: 30/100... Step: 1095... Loss: 1.0268...\n",
      "Epoch: 30/100... Step: 1096... Loss: 1.0179...\n",
      "Epoch: 30/100... Step: 1097... Loss: 1.0130...\n",
      "Epoch: 30/100... Step: 1098... Loss: 1.0332...\n",
      "Epoch: 30/100... Step: 1099... Loss: 1.0066...\n",
      "Epoch: 30/100... Step: 1100... Loss: 1.0163...\n",
      "Epoch: 30/100... Step: 1101... Loss: 1.0166...\n",
      "Epoch: 30/100... Step: 1102... Loss: 1.0366...\n",
      "Epoch: 30/100... Step: 1103... Loss: 1.0250...\n",
      "Epoch: 30/100... Step: 1104... Loss: 1.0322...\n",
      "Epoch: 30/100... Step: 1105... Loss: 1.0204...\n",
      "Epoch: 30/100... Step: 1106... Loss: 1.0334...\n",
      "Epoch: 30/100... Step: 1107... Loss: 1.0053...\n",
      "Epoch: 30/100... Step: 1108... Loss: 1.0356...\n",
      "Epoch: 30/100... Step: 1109... Loss: 1.0104...\n",
      "Epoch: 30/100... Step: 1110... Loss: 1.0071...\n",
      "Epoch: 31/100... Step: 1111... Loss: 1.0329...\n",
      "Epoch: 31/100... Step: 1112... Loss: 1.0161...\n",
      "Epoch: 31/100... Step: 1113... Loss: 1.0187...\n",
      "Epoch: 31/100... Step: 1114... Loss: 1.0173...\n",
      "Epoch: 31/100... Step: 1115... Loss: 1.0157...\n",
      "Epoch: 31/100... Step: 1116... Loss: 1.0262...\n",
      "Epoch: 31/100... Step: 1117... Loss: 1.0152...\n",
      "Epoch: 31/100... Step: 1118... Loss: 1.0386...\n",
      "Epoch: 31/100... Step: 1119... Loss: 1.0309...\n",
      "Epoch: 31/100... Step: 1120... Loss: 1.0309...\n",
      "Epoch: 31/100... Step: 1121... Loss: 1.0128...\n",
      "Epoch: 31/100... Step: 1122... Loss: 0.9993...\n",
      "Epoch: 31/100... Step: 1123... Loss: 0.9971...\n",
      "Epoch: 31/100... Step: 1124... Loss: 1.0034...\n",
      "Epoch: 31/100... Step: 1125... Loss: 1.0104...\n",
      "Epoch: 31/100... Step: 1126... Loss: 1.0035...\n",
      "Epoch: 31/100... Step: 1127... Loss: 1.0060...\n",
      "Epoch: 31/100... Step: 1128... Loss: 1.0164...\n",
      "Epoch: 31/100... Step: 1129... Loss: 1.0003...\n",
      "Epoch: 31/100... Step: 1130... Loss: 1.0232...\n",
      "Epoch: 31/100... Step: 1131... Loss: 1.0119...\n",
      "Epoch: 31/100... Step: 1132... Loss: 1.0168...\n",
      "Epoch: 31/100... Step: 1133... Loss: 1.0042...\n",
      "Epoch: 31/100... Step: 1134... Loss: 1.0078...\n",
      "Epoch: 31/100... Step: 1135... Loss: 1.0155...\n",
      "Epoch: 31/100... Step: 1136... Loss: 1.0014...\n",
      "Epoch: 31/100... Step: 1137... Loss: 1.0027...\n",
      "Epoch: 31/100... Step: 1138... Loss: 1.0020...\n",
      "Epoch: 31/100... Step: 1139... Loss: 1.0258...\n",
      "Epoch: 31/100... Step: 1140... Loss: 1.0199...\n",
      "Epoch: 31/100... Step: 1141... Loss: 1.0191...\n",
      "Epoch: 31/100... Step: 1142... Loss: 1.0093...\n",
      "Epoch: 31/100... Step: 1143... Loss: 1.0206...\n",
      "Epoch: 31/100... Step: 1144... Loss: 0.9960...\n",
      "Epoch: 31/100... Step: 1145... Loss: 1.0252...\n",
      "Epoch: 31/100... Step: 1146... Loss: 0.9997...\n",
      "Epoch: 31/100... Step: 1147... Loss: 0.9975...\n",
      "Epoch: 32/100... Step: 1148... Loss: 1.0241...\n",
      "Epoch: 32/100... Step: 1149... Loss: 1.0090...\n",
      "Epoch: 32/100... Step: 1150... Loss: 0.9969...\n",
      "Epoch: 32/100... Step: 1151... Loss: 1.0083...\n",
      "Epoch: 32/100... Step: 1152... Loss: 1.0077...\n",
      "Epoch: 32/100... Step: 1153... Loss: 1.0113...\n",
      "Epoch: 32/100... Step: 1154... Loss: 1.0049...\n",
      "Epoch: 32/100... Step: 1155... Loss: 1.0202...\n",
      "Epoch: 32/100... Step: 1156... Loss: 1.0198...\n",
      "Epoch: 32/100... Step: 1157... Loss: 1.0221...\n",
      "Epoch: 32/100... Step: 1158... Loss: 0.9951...\n",
      "Epoch: 32/100... Step: 1159... Loss: 0.9864...\n",
      "Epoch: 32/100... Step: 1160... Loss: 0.9827...\n",
      "Epoch: 32/100... Step: 1161... Loss: 0.9945...\n",
      "Epoch: 32/100... Step: 1162... Loss: 0.9957...\n",
      "Epoch: 32/100... Step: 1163... Loss: 0.9884...\n",
      "Epoch: 32/100... Step: 1164... Loss: 0.9906...\n",
      "Epoch: 32/100... Step: 1165... Loss: 1.0011...\n",
      "Epoch: 32/100... Step: 1166... Loss: 0.9889...\n",
      "Epoch: 32/100... Step: 1167... Loss: 1.0083...\n",
      "Epoch: 32/100... Step: 1168... Loss: 1.0009...\n",
      "Epoch: 32/100... Step: 1169... Loss: 0.9997...\n",
      "Epoch: 32/100... Step: 1170... Loss: 0.9900...\n",
      "Epoch: 32/100... Step: 1171... Loss: 0.9899...\n",
      "Epoch: 32/100... Step: 1172... Loss: 1.0076...\n",
      "Epoch: 32/100... Step: 1173... Loss: 0.9800...\n",
      "Epoch: 32/100... Step: 1174... Loss: 0.9938...\n",
      "Epoch: 32/100... Step: 1175... Loss: 0.9855...\n",
      "Epoch: 32/100... Step: 1176... Loss: 1.0124...\n",
      "Epoch: 32/100... Step: 1177... Loss: 1.0065...\n",
      "Epoch: 32/100... Step: 1178... Loss: 1.0081...\n",
      "Epoch: 32/100... Step: 1179... Loss: 0.9986...\n",
      "Epoch: 32/100... Step: 1180... Loss: 1.0097...\n",
      "Epoch: 32/100... Step: 1181... Loss: 0.9808...\n",
      "Epoch: 32/100... Step: 1182... Loss: 1.0159...\n",
      "Epoch: 32/100... Step: 1183... Loss: 0.9911...\n",
      "Epoch: 32/100... Step: 1184... Loss: 0.9895...\n",
      "Epoch: 33/100... Step: 1185... Loss: 1.0124...\n",
      "Epoch: 33/100... Step: 1186... Loss: 0.9962...\n",
      "Epoch: 33/100... Step: 1187... Loss: 0.9914...\n",
      "Epoch: 33/100... Step: 1188... Loss: 0.9984...\n",
      "Epoch: 33/100... Step: 1189... Loss: 0.9943...\n",
      "Epoch: 33/100... Step: 1190... Loss: 1.0021...\n",
      "Epoch: 33/100... Step: 1191... Loss: 0.9934...\n",
      "Epoch: 33/100... Step: 1192... Loss: 1.0134...\n",
      "Epoch: 33/100... Step: 1193... Loss: 1.0082...\n",
      "Epoch: 33/100... Step: 1194... Loss: 1.0081...\n",
      "Epoch: 33/100... Step: 1195... Loss: 0.9860...\n",
      "Epoch: 33/100... Step: 1196... Loss: 0.9765...\n",
      "Epoch: 33/100... Step: 1197... Loss: 0.9735...\n",
      "Epoch: 33/100... Step: 1198... Loss: 0.9846...\n",
      "Epoch: 33/100... Step: 1199... Loss: 0.9856...\n",
      "Epoch: 33/100... Step: 1200... Loss: 0.9775...\n",
      "Epoch: 33/100... Step: 1201... Loss: 0.9785...\n",
      "Epoch: 33/100... Step: 1202... Loss: 0.9893...\n",
      "Epoch: 33/100... Step: 1203... Loss: 0.9776...\n",
      "Epoch: 33/100... Step: 1204... Loss: 0.9960...\n",
      "Epoch: 33/100... Step: 1205... Loss: 0.9857...\n",
      "Epoch: 33/100... Step: 1206... Loss: 0.9890...\n",
      "Epoch: 33/100... Step: 1207... Loss: 0.9815...\n",
      "Epoch: 33/100... Step: 1208... Loss: 0.9785...\n",
      "Epoch: 33/100... Step: 1209... Loss: 0.9972...\n",
      "Epoch: 33/100... Step: 1210... Loss: 0.9709...\n",
      "Epoch: 33/100... Step: 1211... Loss: 0.9795...\n",
      "Epoch: 33/100... Step: 1212... Loss: 0.9797...\n",
      "Epoch: 33/100... Step: 1213... Loss: 1.0000...\n",
      "Epoch: 33/100... Step: 1214... Loss: 0.9961...\n",
      "Epoch: 33/100... Step: 1215... Loss: 0.9973...\n",
      "Epoch: 33/100... Step: 1216... Loss: 0.9933...\n",
      "Epoch: 33/100... Step: 1217... Loss: 0.9992...\n",
      "Epoch: 33/100... Step: 1218... Loss: 0.9744...\n",
      "Epoch: 33/100... Step: 1219... Loss: 1.0053...\n",
      "Epoch: 33/100... Step: 1220... Loss: 0.9874...\n",
      "Epoch: 33/100... Step: 1221... Loss: 0.9782...\n",
      "Epoch: 34/100... Step: 1222... Loss: 1.0052...\n",
      "Epoch: 34/100... Step: 1223... Loss: 0.9898...\n",
      "Epoch: 34/100... Step: 1224... Loss: 0.9811...\n",
      "Epoch: 34/100... Step: 1225... Loss: 0.9892...\n",
      "Epoch: 34/100... Step: 1226... Loss: 0.9892...\n",
      "Epoch: 34/100... Step: 1227... Loss: 0.9932...\n",
      "Epoch: 34/100... Step: 1228... Loss: 0.9920...\n",
      "Epoch: 34/100... Step: 1229... Loss: 1.0078...\n",
      "Epoch: 34/100... Step: 1230... Loss: 1.0047...\n",
      "Epoch: 34/100... Step: 1231... Loss: 1.0074...\n",
      "Epoch: 34/100... Step: 1232... Loss: 0.9818...\n",
      "Epoch: 34/100... Step: 1233... Loss: 0.9770...\n",
      "Epoch: 34/100... Step: 1234... Loss: 0.9704...\n",
      "Epoch: 34/100... Step: 1235... Loss: 0.9794...\n",
      "Epoch: 34/100... Step: 1236... Loss: 0.9851...\n",
      "Epoch: 34/100... Step: 1237... Loss: 0.9796...\n",
      "Epoch: 34/100... Step: 1238... Loss: 0.9764...\n",
      "Epoch: 34/100... Step: 1239... Loss: 0.9848...\n",
      "Epoch: 34/100... Step: 1240... Loss: 0.9753...\n",
      "Epoch: 34/100... Step: 1241... Loss: 0.9910...\n",
      "Epoch: 34/100... Step: 1242... Loss: 0.9853...\n",
      "Epoch: 34/100... Step: 1243... Loss: 0.9836...\n",
      "Epoch: 34/100... Step: 1244... Loss: 0.9760...\n",
      "Epoch: 34/100... Step: 1245... Loss: 0.9712...\n",
      "Epoch: 34/100... Step: 1246... Loss: 0.9948...\n",
      "Epoch: 34/100... Step: 1247... Loss: 0.9668...\n",
      "Epoch: 34/100... Step: 1248... Loss: 0.9768...\n",
      "Epoch: 34/100... Step: 1249... Loss: 0.9725...\n",
      "Epoch: 34/100... Step: 1250... Loss: 0.9965...\n",
      "Epoch: 34/100... Step: 1251... Loss: 0.9914...\n",
      "Epoch: 34/100... Step: 1252... Loss: 0.9984...\n",
      "Epoch: 34/100... Step: 1253... Loss: 0.9918...\n",
      "Epoch: 34/100... Step: 1254... Loss: 1.0038...\n",
      "Epoch: 34/100... Step: 1255... Loss: 0.9767...\n",
      "Epoch: 34/100... Step: 1256... Loss: 1.0074...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34/100... Step: 1257... Loss: 0.9845...\n",
      "Epoch: 34/100... Step: 1258... Loss: 0.9836...\n",
      "Epoch: 35/100... Step: 1259... Loss: 1.0096...\n",
      "Epoch: 35/100... Step: 1260... Loss: 0.9912...\n",
      "Epoch: 35/100... Step: 1261... Loss: 0.9854...\n",
      "Epoch: 35/100... Step: 1262... Loss: 0.9889...\n",
      "Epoch: 35/100... Step: 1263... Loss: 0.9934...\n",
      "Epoch: 35/100... Step: 1264... Loss: 0.9912...\n",
      "Epoch: 35/100... Step: 1265... Loss: 0.9887...\n",
      "Epoch: 35/100... Step: 1266... Loss: 1.0056...\n",
      "Epoch: 35/100... Step: 1267... Loss: 1.0042...\n",
      "Epoch: 35/100... Step: 1268... Loss: 1.0041...\n",
      "Epoch: 35/100... Step: 1269... Loss: 0.9801...\n",
      "Epoch: 35/100... Step: 1270... Loss: 0.9727...\n",
      "Epoch: 35/100... Step: 1271... Loss: 0.9707...\n",
      "Epoch: 35/100... Step: 1272... Loss: 0.9798...\n",
      "Epoch: 35/100... Step: 1273... Loss: 0.9796...\n",
      "Epoch: 35/100... Step: 1274... Loss: 0.9736...\n",
      "Epoch: 35/100... Step: 1275... Loss: 0.9770...\n",
      "Epoch: 35/100... Step: 1276... Loss: 0.9839...\n",
      "Epoch: 35/100... Step: 1277... Loss: 0.9713...\n",
      "Epoch: 35/100... Step: 1278... Loss: 0.9880...\n",
      "Epoch: 35/100... Step: 1279... Loss: 0.9789...\n",
      "Epoch: 35/100... Step: 1280... Loss: 0.9808...\n",
      "Epoch: 35/100... Step: 1281... Loss: 0.9669...\n",
      "Epoch: 35/100... Step: 1282... Loss: 0.9693...\n",
      "Epoch: 35/100... Step: 1283... Loss: 0.9832...\n",
      "Epoch: 35/100... Step: 1284... Loss: 0.9603...\n",
      "Epoch: 35/100... Step: 1285... Loss: 0.9690...\n",
      "Epoch: 35/100... Step: 1286... Loss: 0.9676...\n",
      "Epoch: 35/100... Step: 1287... Loss: 0.9916...\n",
      "Epoch: 35/100... Step: 1288... Loss: 0.9855...\n",
      "Epoch: 35/100... Step: 1289... Loss: 0.9886...\n",
      "Epoch: 35/100... Step: 1290... Loss: 0.9774...\n",
      "Epoch: 35/100... Step: 1291... Loss: 0.9921...\n",
      "Epoch: 35/100... Step: 1292... Loss: 0.9633...\n",
      "Epoch: 35/100... Step: 1293... Loss: 0.9920...\n",
      "Epoch: 35/100... Step: 1294... Loss: 0.9735...\n",
      "Epoch: 35/100... Step: 1295... Loss: 0.9691...\n",
      "Epoch: 36/100... Step: 1296... Loss: 0.9966...\n",
      "Epoch: 36/100... Step: 1297... Loss: 0.9732...\n",
      "Epoch: 36/100... Step: 1298... Loss: 0.9690...\n",
      "Epoch: 36/100... Step: 1299... Loss: 0.9828...\n",
      "Epoch: 36/100... Step: 1300... Loss: 0.9793...\n",
      "Epoch: 36/100... Step: 1301... Loss: 0.9859...\n",
      "Epoch: 36/100... Step: 1302... Loss: 0.9757...\n",
      "Epoch: 36/100... Step: 1303... Loss: 0.9931...\n",
      "Epoch: 36/100... Step: 1304... Loss: 0.9908...\n",
      "Epoch: 36/100... Step: 1305... Loss: 0.9926...\n",
      "Epoch: 36/100... Step: 1306... Loss: 0.9688...\n",
      "Epoch: 36/100... Step: 1307... Loss: 0.9579...\n",
      "Epoch: 36/100... Step: 1308... Loss: 0.9576...\n",
      "Epoch: 36/100... Step: 1309... Loss: 0.9695...\n",
      "Epoch: 36/100... Step: 1310... Loss: 0.9712...\n",
      "Epoch: 36/100... Step: 1311... Loss: 0.9609...\n",
      "Epoch: 36/100... Step: 1312... Loss: 0.9636...\n",
      "Epoch: 36/100... Step: 1313... Loss: 0.9756...\n",
      "Epoch: 36/100... Step: 1314... Loss: 0.9620...\n",
      "Epoch: 36/100... Step: 1315... Loss: 0.9761...\n",
      "Epoch: 36/100... Step: 1316... Loss: 0.9706...\n",
      "Epoch: 36/100... Step: 1317... Loss: 0.9711...\n",
      "Epoch: 36/100... Step: 1318... Loss: 0.9610...\n",
      "Epoch: 36/100... Step: 1319... Loss: 0.9618...\n",
      "Epoch: 36/100... Step: 1320... Loss: 0.9766...\n",
      "Epoch: 36/100... Step: 1321... Loss: 0.9522...\n",
      "Epoch: 36/100... Step: 1322... Loss: 0.9620...\n",
      "Epoch: 36/100... Step: 1323... Loss: 0.9609...\n",
      "Epoch: 36/100... Step: 1324... Loss: 0.9802...\n",
      "Epoch: 36/100... Step: 1325... Loss: 0.9742...\n",
      "Epoch: 36/100... Step: 1326... Loss: 0.9838...\n",
      "Epoch: 36/100... Step: 1327... Loss: 0.9723...\n",
      "Epoch: 36/100... Step: 1328... Loss: 0.9807...\n",
      "Epoch: 36/100... Step: 1329... Loss: 0.9597...\n",
      "Epoch: 36/100... Step: 1330... Loss: 0.9899...\n",
      "Epoch: 36/100... Step: 1331... Loss: 0.9646...\n",
      "Epoch: 36/100... Step: 1332... Loss: 0.9630...\n",
      "Epoch: 37/100... Step: 1333... Loss: 0.9839...\n",
      "Epoch: 37/100... Step: 1334... Loss: 0.9717...\n",
      "Epoch: 37/100... Step: 1335... Loss: 0.9700...\n",
      "Epoch: 37/100... Step: 1336... Loss: 0.9781...\n",
      "Epoch: 37/100... Step: 1337... Loss: 0.9834...\n",
      "Epoch: 37/100... Step: 1338... Loss: 0.9830...\n",
      "Epoch: 37/100... Step: 1339... Loss: 0.9852...\n",
      "Epoch: 37/100... Step: 1340... Loss: 0.9956...\n",
      "Epoch: 37/100... Step: 1341... Loss: 0.9858...\n",
      "Epoch: 37/100... Step: 1342... Loss: 0.9905...\n",
      "Epoch: 37/100... Step: 1343... Loss: 0.9657...\n",
      "Epoch: 37/100... Step: 1344... Loss: 1.0384...\n",
      "Epoch: 37/100... Step: 1345... Loss: 1.0738...\n",
      "Epoch: 37/100... Step: 1346... Loss: 1.0721...\n",
      "Epoch: 37/100... Step: 1347... Loss: 1.0778...\n",
      "Epoch: 37/100... Step: 1348... Loss: 1.0592...\n",
      "Epoch: 37/100... Step: 1349... Loss: 1.0506...\n",
      "Epoch: 37/100... Step: 1350... Loss: 1.0644...\n",
      "Epoch: 37/100... Step: 1351... Loss: 1.0381...\n",
      "Epoch: 37/100... Step: 1352... Loss: 1.0541...\n",
      "Epoch: 37/100... Step: 1353... Loss: 1.0367...\n",
      "Epoch: 37/100... Step: 1354... Loss: 1.0392...\n",
      "Epoch: 37/100... Step: 1355... Loss: 1.0214...\n",
      "Epoch: 37/100... Step: 1356... Loss: 1.0222...\n",
      "Epoch: 37/100... Step: 1357... Loss: 1.0319...\n",
      "Epoch: 37/100... Step: 1358... Loss: 1.0059...\n",
      "Epoch: 37/100... Step: 1359... Loss: 1.0090...\n",
      "Epoch: 37/100... Step: 1360... Loss: 1.0044...\n",
      "Epoch: 37/100... Step: 1361... Loss: 1.0218...\n",
      "Epoch: 37/100... Step: 1362... Loss: 1.0091...\n",
      "Epoch: 37/100... Step: 1363... Loss: 1.0163...\n",
      "Epoch: 37/100... Step: 1364... Loss: 1.0043...\n",
      "Epoch: 37/100... Step: 1365... Loss: 1.0111...\n",
      "Epoch: 37/100... Step: 1366... Loss: 0.9774...\n",
      "Epoch: 37/100... Step: 1367... Loss: 1.0126...\n",
      "Epoch: 37/100... Step: 1368... Loss: 0.9863...\n",
      "Epoch: 37/100... Step: 1369... Loss: 0.9799...\n",
      "Epoch: 38/100... Step: 1370... Loss: 1.0052...\n",
      "Epoch: 38/100... Step: 1371... Loss: 0.9855...\n",
      "Epoch: 38/100... Step: 1372... Loss: 0.9786...\n",
      "Epoch: 38/100... Step: 1373... Loss: 0.9806...\n",
      "Epoch: 38/100... Step: 1374... Loss: 0.9801...\n",
      "Epoch: 38/100... Step: 1375... Loss: 0.9872...\n",
      "Epoch: 38/100... Step: 1376... Loss: 0.9793...\n",
      "Epoch: 38/100... Step: 1377... Loss: 0.9856...\n",
      "Epoch: 38/100... Step: 1378... Loss: 0.9869...\n",
      "Epoch: 38/100... Step: 1379... Loss: 0.9911...\n",
      "Epoch: 38/100... Step: 1380... Loss: 0.9778...\n",
      "Epoch: 38/100... Step: 1381... Loss: 0.9602...\n",
      "Epoch: 38/100... Step: 1382... Loss: 0.9596...\n",
      "Epoch: 38/100... Step: 1383... Loss: 0.9701...\n",
      "Epoch: 38/100... Step: 1384... Loss: 0.9723...\n",
      "Epoch: 38/100... Step: 1385... Loss: 0.9605...\n",
      "Epoch: 38/100... Step: 1386... Loss: 0.9616...\n",
      "Epoch: 38/100... Step: 1387... Loss: 0.9693...\n",
      "Epoch: 38/100... Step: 1388... Loss: 0.9577...\n",
      "Epoch: 38/100... Step: 1389... Loss: 0.9714...\n",
      "Epoch: 38/100... Step: 1390... Loss: 0.9605...\n",
      "Epoch: 38/100... Step: 1391... Loss: 0.9626...\n",
      "Epoch: 38/100... Step: 1392... Loss: 0.9510...\n",
      "Epoch: 38/100... Step: 1393... Loss: 0.9546...\n",
      "Epoch: 38/100... Step: 1394... Loss: 0.9693...\n",
      "Epoch: 38/100... Step: 1395... Loss: 0.9425...\n",
      "Epoch: 38/100... Step: 1396... Loss: 0.9492...\n",
      "Epoch: 38/100... Step: 1397... Loss: 0.9474...\n",
      "Epoch: 38/100... Step: 1398... Loss: 0.9681...\n",
      "Epoch: 38/100... Step: 1399... Loss: 0.9602...\n",
      "Epoch: 38/100... Step: 1400... Loss: 0.9699...\n",
      "Epoch: 38/100... Step: 1401... Loss: 0.9604...\n",
      "Epoch: 38/100... Step: 1402... Loss: 0.9645...\n",
      "Epoch: 38/100... Step: 1403... Loss: 0.9403...\n",
      "Epoch: 38/100... Step: 1404... Loss: 0.9727...\n",
      "Epoch: 38/100... Step: 1405... Loss: 0.9481...\n",
      "Epoch: 38/100... Step: 1406... Loss: 0.9443...\n",
      "Epoch: 39/100... Step: 1407... Loss: 0.9679...\n",
      "Epoch: 39/100... Step: 1408... Loss: 0.9502...\n",
      "Epoch: 39/100... Step: 1409... Loss: 0.9428...\n",
      "Epoch: 39/100... Step: 1410... Loss: 0.9514...\n",
      "Epoch: 39/100... Step: 1411... Loss: 0.9509...\n",
      "Epoch: 39/100... Step: 1412... Loss: 0.9564...\n",
      "Epoch: 39/100... Step: 1413... Loss: 0.9511...\n",
      "Epoch: 39/100... Step: 1414... Loss: 0.9610...\n",
      "Epoch: 39/100... Step: 1415... Loss: 0.9572...\n",
      "Epoch: 39/100... Step: 1416... Loss: 0.9619...\n",
      "Epoch: 39/100... Step: 1417... Loss: 0.9478...\n",
      "Epoch: 39/100... Step: 1418... Loss: 0.9354...\n",
      "Epoch: 39/100... Step: 1419... Loss: 0.9349...\n",
      "Epoch: 39/100... Step: 1420... Loss: 0.9458...\n",
      "Epoch: 39/100... Step: 1421... Loss: 0.9460...\n",
      "Epoch: 39/100... Step: 1422... Loss: 0.9408...\n",
      "Epoch: 39/100... Step: 1423... Loss: 0.9407...\n",
      "Epoch: 39/100... Step: 1424... Loss: 0.9467...\n",
      "Epoch: 39/100... Step: 1425... Loss: 0.9393...\n",
      "Epoch: 39/100... Step: 1426... Loss: 0.9514...\n",
      "Epoch: 39/100... Step: 1427... Loss: 0.9422...\n",
      "Epoch: 39/100... Step: 1428... Loss: 0.9436...\n",
      "Epoch: 39/100... Step: 1429... Loss: 0.9355...\n",
      "Epoch: 39/100... Step: 1430... Loss: 0.9374...\n",
      "Epoch: 39/100... Step: 1431... Loss: 0.9507...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39/100... Step: 1432... Loss: 0.9277...\n",
      "Epoch: 39/100... Step: 1433... Loss: 0.9344...\n",
      "Epoch: 39/100... Step: 1434... Loss: 0.9313...\n",
      "Epoch: 39/100... Step: 1435... Loss: 0.9504...\n",
      "Epoch: 39/100... Step: 1436... Loss: 0.9436...\n",
      "Epoch: 39/100... Step: 1437... Loss: 0.9567...\n",
      "Epoch: 39/100... Step: 1438... Loss: 0.9532...\n",
      "Epoch: 39/100... Step: 1439... Loss: 0.9555...\n",
      "Epoch: 39/100... Step: 1440... Loss: 0.9270...\n",
      "Epoch: 39/100... Step: 1441... Loss: 0.9604...\n",
      "Epoch: 39/100... Step: 1442... Loss: 0.9377...\n",
      "Epoch: 39/100... Step: 1443... Loss: 0.9347...\n",
      "Epoch: 40/100... Step: 1444... Loss: 0.9566...\n",
      "Epoch: 40/100... Step: 1445... Loss: 0.9463...\n",
      "Epoch: 40/100... Step: 1446... Loss: 0.9344...\n",
      "Epoch: 40/100... Step: 1447... Loss: 0.9406...\n",
      "Epoch: 40/100... Step: 1448... Loss: 0.9456...\n",
      "Epoch: 40/100... Step: 1449... Loss: 0.9580...\n",
      "Epoch: 40/100... Step: 1450... Loss: 0.9508...\n",
      "Epoch: 40/100... Step: 1451... Loss: 0.9607...\n",
      "Epoch: 40/100... Step: 1452... Loss: 0.9604...\n",
      "Epoch: 40/100... Step: 1453... Loss: 0.9643...\n",
      "Epoch: 40/100... Step: 1454... Loss: 0.9476...\n",
      "Epoch: 40/100... Step: 1455... Loss: 0.9306...\n",
      "Epoch: 40/100... Step: 1456... Loss: 0.9384...\n",
      "Epoch: 40/100... Step: 1457... Loss: 0.9495...\n",
      "Epoch: 40/100... Step: 1458... Loss: 0.9487...\n",
      "Epoch: 40/100... Step: 1459... Loss: 0.9430...\n",
      "Epoch: 40/100... Step: 1460... Loss: 0.9445...\n",
      "Epoch: 40/100... Step: 1461... Loss: 0.9509...\n",
      "Epoch: 40/100... Step: 1462... Loss: 0.9365...\n",
      "Epoch: 40/100... Step: 1463... Loss: 0.9562...\n",
      "Epoch: 40/100... Step: 1464... Loss: 0.9521...\n",
      "Epoch: 40/100... Step: 1465... Loss: 0.9529...\n",
      "Epoch: 40/100... Step: 1466... Loss: 0.9329...\n",
      "Epoch: 40/100... Step: 1467... Loss: 0.9373...\n",
      "Epoch: 40/100... Step: 1468... Loss: 0.9568...\n",
      "Epoch: 40/100... Step: 1469... Loss: 0.9336...\n",
      "Epoch: 40/100... Step: 1470... Loss: 0.9426...\n",
      "Epoch: 40/100... Step: 1471... Loss: 0.9330...\n",
      "Epoch: 40/100... Step: 1472... Loss: 0.9530...\n",
      "Epoch: 40/100... Step: 1473... Loss: 0.9498...\n",
      "Epoch: 40/100... Step: 1474... Loss: 0.9551...\n",
      "Epoch: 40/100... Step: 1475... Loss: 0.9505...\n",
      "Epoch: 40/100... Step: 1476... Loss: 0.9593...\n",
      "Epoch: 40/100... Step: 1477... Loss: 0.9337...\n",
      "Epoch: 40/100... Step: 1478... Loss: 0.9621...\n",
      "Epoch: 40/100... Step: 1479... Loss: 0.9414...\n",
      "Epoch: 40/100... Step: 1480... Loss: 0.9432...\n",
      "Epoch: 41/100... Step: 1481... Loss: 0.9603...\n",
      "Epoch: 41/100... Step: 1482... Loss: 0.9456...\n",
      "Epoch: 41/100... Step: 1483... Loss: 0.9402...\n",
      "Epoch: 41/100... Step: 1484... Loss: 0.9451...\n",
      "Epoch: 41/100... Step: 1485... Loss: 0.9507...\n",
      "Epoch: 41/100... Step: 1486... Loss: 0.9553...\n",
      "Epoch: 41/100... Step: 1487... Loss: 0.9432...\n",
      "Epoch: 41/100... Step: 1488... Loss: 0.9572...\n",
      "Epoch: 41/100... Step: 1489... Loss: 0.9605...\n",
      "Epoch: 41/100... Step: 1490... Loss: 0.9633...\n",
      "Epoch: 41/100... Step: 1491... Loss: 0.9429...\n",
      "Epoch: 41/100... Step: 1492... Loss: 0.9308...\n",
      "Epoch: 41/100... Step: 1493... Loss: 0.9314...\n",
      "Epoch: 41/100... Step: 1494... Loss: 0.9417...\n",
      "Epoch: 41/100... Step: 1495... Loss: 0.9453...\n",
      "Epoch: 41/100... Step: 1496... Loss: 0.9389...\n",
      "Epoch: 41/100... Step: 1497... Loss: 0.9407...\n",
      "Epoch: 41/100... Step: 1498... Loss: 0.9459...\n",
      "Epoch: 41/100... Step: 1499... Loss: 0.9321...\n",
      "Epoch: 41/100... Step: 1500... Loss: 0.9525...\n",
      "Epoch: 41/100... Step: 1501... Loss: 0.9424...\n",
      "Epoch: 41/100... Step: 1502... Loss: 0.9465...\n",
      "Epoch: 41/100... Step: 1503... Loss: 0.9321...\n",
      "Epoch: 41/100... Step: 1504... Loss: 0.9364...\n",
      "Epoch: 41/100... Step: 1505... Loss: 0.9519...\n",
      "Epoch: 41/100... Step: 1506... Loss: 0.9313...\n",
      "Epoch: 41/100... Step: 1507... Loss: 0.9354...\n",
      "Epoch: 41/100... Step: 1508... Loss: 0.9302...\n",
      "Epoch: 41/100... Step: 1509... Loss: 0.9524...\n",
      "Epoch: 41/100... Step: 1510... Loss: 0.9392...\n",
      "Epoch: 41/100... Step: 1511... Loss: 0.9466...\n",
      "Epoch: 41/100... Step: 1512... Loss: 0.9436...\n",
      "Epoch: 41/100... Step: 1513... Loss: 0.9560...\n",
      "Epoch: 41/100... Step: 1514... Loss: 0.9289...\n",
      "Epoch: 41/100... Step: 1515... Loss: 0.9667...\n",
      "Epoch: 41/100... Step: 1516... Loss: 0.9460...\n",
      "Epoch: 41/100... Step: 1517... Loss: 0.9469...\n",
      "Epoch: 42/100... Step: 1518... Loss: 0.9611...\n",
      "Epoch: 42/100... Step: 1519... Loss: 0.9493...\n",
      "Epoch: 42/100... Step: 1520... Loss: 0.9457...\n",
      "Epoch: 42/100... Step: 1521... Loss: 0.9483...\n",
      "Epoch: 42/100... Step: 1522... Loss: 0.9461...\n",
      "Epoch: 42/100... Step: 1523... Loss: 0.9513...\n",
      "Epoch: 42/100... Step: 1524... Loss: 0.9423...\n",
      "Epoch: 42/100... Step: 1525... Loss: 0.9589...\n",
      "Epoch: 42/100... Step: 1526... Loss: 0.9592...\n",
      "Epoch: 42/100... Step: 1527... Loss: 0.9562...\n",
      "Epoch: 42/100... Step: 1528... Loss: 0.9420...\n",
      "Epoch: 42/100... Step: 1529... Loss: 0.9262...\n",
      "Epoch: 42/100... Step: 1530... Loss: 0.9220...\n",
      "Epoch: 42/100... Step: 1531... Loss: 0.9388...\n",
      "Epoch: 42/100... Step: 1532... Loss: 0.9367...\n",
      "Epoch: 42/100... Step: 1533... Loss: 0.9310...\n",
      "Epoch: 42/100... Step: 1534... Loss: 0.9346...\n",
      "Epoch: 42/100... Step: 1535... Loss: 0.9390...\n",
      "Epoch: 42/100... Step: 1536... Loss: 0.9336...\n",
      "Epoch: 42/100... Step: 1537... Loss: 0.9453...\n",
      "Epoch: 42/100... Step: 1538... Loss: 0.9413...\n",
      "Epoch: 42/100... Step: 1539... Loss: 0.9391...\n",
      "Epoch: 42/100... Step: 1540... Loss: 0.9277...\n",
      "Epoch: 42/100... Step: 1541... Loss: 0.9327...\n",
      "Epoch: 42/100... Step: 1542... Loss: 0.9479...\n",
      "Epoch: 42/100... Step: 1543... Loss: 0.9218...\n",
      "Epoch: 42/100... Step: 1544... Loss: 0.9338...\n",
      "Epoch: 42/100... Step: 1545... Loss: 0.9260...\n",
      "Epoch: 42/100... Step: 1546... Loss: 0.9458...\n",
      "Epoch: 42/100... Step: 1547... Loss: 0.9386...\n",
      "Epoch: 42/100... Step: 1548... Loss: 0.9410...\n",
      "Epoch: 42/100... Step: 1549... Loss: 0.9372...\n",
      "Epoch: 42/100... Step: 1550... Loss: 0.9454...\n",
      "Epoch: 42/100... Step: 1551... Loss: 0.9195...\n",
      "Epoch: 42/100... Step: 1552... Loss: 0.9494...\n",
      "Epoch: 42/100... Step: 1553... Loss: 0.9273...\n",
      "Epoch: 42/100... Step: 1554... Loss: 0.9246...\n",
      "Epoch: 43/100... Step: 1555... Loss: 0.9467...\n",
      "Epoch: 43/100... Step: 1556... Loss: 0.9290...\n",
      "Epoch: 43/100... Step: 1557... Loss: 0.9253...\n",
      "Epoch: 43/100... Step: 1558... Loss: 0.9351...\n",
      "Epoch: 43/100... Step: 1559... Loss: 0.9325...\n",
      "Epoch: 43/100... Step: 1560... Loss: 0.9370...\n",
      "Epoch: 43/100... Step: 1561... Loss: 0.9296...\n",
      "Epoch: 43/100... Step: 1562... Loss: 0.9455...\n",
      "Epoch: 43/100... Step: 1563... Loss: 0.9479...\n",
      "Epoch: 43/100... Step: 1564... Loss: 0.9452...\n",
      "Epoch: 43/100... Step: 1565... Loss: 0.9276...\n",
      "Epoch: 43/100... Step: 1566... Loss: 0.9150...\n",
      "Epoch: 43/100... Step: 1567... Loss: 0.9126...\n",
      "Epoch: 43/100... Step: 1568... Loss: 0.9249...\n",
      "Epoch: 43/100... Step: 1569... Loss: 0.9293...\n",
      "Epoch: 43/100... Step: 1570... Loss: 0.9196...\n",
      "Epoch: 43/100... Step: 1571... Loss: 0.9227...\n",
      "Epoch: 43/100... Step: 1572... Loss: 0.9258...\n",
      "Epoch: 43/100... Step: 1573... Loss: 0.9170...\n",
      "Epoch: 43/100... Step: 1574... Loss: 0.9344...\n",
      "Epoch: 43/100... Step: 1575... Loss: 0.9257...\n",
      "Epoch: 43/100... Step: 1576... Loss: 0.9294...\n",
      "Epoch: 43/100... Step: 1577... Loss: 0.9212...\n",
      "Epoch: 43/100... Step: 1578... Loss: 0.9143...\n",
      "Epoch: 43/100... Step: 1579... Loss: 0.9430...\n",
      "Epoch: 43/100... Step: 1580... Loss: 0.9175...\n",
      "Epoch: 43/100... Step: 1581... Loss: 0.9268...\n",
      "Epoch: 43/100... Step: 1582... Loss: 0.9236...\n",
      "Epoch: 43/100... Step: 1583... Loss: 0.9426...\n",
      "Epoch: 43/100... Step: 1584... Loss: 0.9335...\n",
      "Epoch: 43/100... Step: 1585... Loss: 0.9386...\n",
      "Epoch: 43/100... Step: 1586... Loss: 0.9269...\n",
      "Epoch: 43/100... Step: 1587... Loss: 0.9395...\n",
      "Epoch: 43/100... Step: 1588... Loss: 0.9145...\n",
      "Epoch: 43/100... Step: 1589... Loss: 0.9453...\n",
      "Epoch: 43/100... Step: 1590... Loss: 0.9261...\n",
      "Epoch: 43/100... Step: 1591... Loss: 0.9189...\n",
      "Epoch: 44/100... Step: 1592... Loss: 0.9379...\n",
      "Epoch: 44/100... Step: 1593... Loss: 0.9243...\n",
      "Epoch: 44/100... Step: 1594... Loss: 0.9223...\n",
      "Epoch: 44/100... Step: 1595... Loss: 0.9302...\n",
      "Epoch: 44/100... Step: 1596... Loss: 0.9558...\n",
      "Epoch: 44/100... Step: 1597... Loss: 0.9861...\n",
      "Epoch: 44/100... Step: 1598... Loss: 0.9843...\n",
      "Epoch: 44/100... Step: 1599... Loss: 1.0005...\n",
      "Epoch: 44/100... Step: 1600... Loss: 1.0002...\n",
      "Epoch: 44/100... Step: 1601... Loss: 1.0045...\n",
      "Epoch: 44/100... Step: 1602... Loss: 0.9802...\n",
      "Epoch: 44/100... Step: 1603... Loss: 0.9592...\n",
      "Epoch: 44/100... Step: 1604... Loss: 0.9576...\n",
      "Epoch: 44/100... Step: 1605... Loss: 0.9639...\n",
      "Epoch: 44/100... Step: 1606... Loss: 0.9638...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44/100... Step: 1607... Loss: 0.9539...\n",
      "Epoch: 44/100... Step: 1608... Loss: 0.9532...\n",
      "Epoch: 44/100... Step: 1609... Loss: 0.9611...\n",
      "Epoch: 44/100... Step: 1610... Loss: 0.9479...\n",
      "Epoch: 44/100... Step: 1611... Loss: 0.9601...\n",
      "Epoch: 44/100... Step: 1612... Loss: 0.9476...\n",
      "Epoch: 44/100... Step: 1613... Loss: 0.9497...\n",
      "Epoch: 44/100... Step: 1614... Loss: 0.9376...\n",
      "Epoch: 44/100... Step: 1615... Loss: 0.9407...\n",
      "Epoch: 44/100... Step: 1616... Loss: 0.9538...\n",
      "Epoch: 44/100... Step: 1617... Loss: 0.9338...\n",
      "Epoch: 44/100... Step: 1618... Loss: 0.9371...\n",
      "Epoch: 44/100... Step: 1619... Loss: 0.9281...\n",
      "Epoch: 44/100... Step: 1620... Loss: 0.9513...\n",
      "Epoch: 44/100... Step: 1621... Loss: 0.9460...\n",
      "Epoch: 44/100... Step: 1622... Loss: 0.9489...\n",
      "Epoch: 44/100... Step: 1623... Loss: 0.9408...\n",
      "Epoch: 44/100... Step: 1624... Loss: 0.9436...\n",
      "Epoch: 44/100... Step: 1625... Loss: 0.9236...\n",
      "Epoch: 44/100... Step: 1626... Loss: 0.9498...\n",
      "Epoch: 44/100... Step: 1627... Loss: 0.9300...\n",
      "Epoch: 44/100... Step: 1628... Loss: 0.9256...\n",
      "Epoch: 45/100... Step: 1629... Loss: 0.9513...\n",
      "Epoch: 45/100... Step: 1630... Loss: 0.9327...\n",
      "Epoch: 45/100... Step: 1631... Loss: 0.9252...\n",
      "Epoch: 45/100... Step: 1632... Loss: 0.9306...\n",
      "Epoch: 45/100... Step: 1633... Loss: 0.9306...\n",
      "Epoch: 45/100... Step: 1634... Loss: 0.9390...\n",
      "Epoch: 45/100... Step: 1635... Loss: 0.9335...\n",
      "Epoch: 45/100... Step: 1636... Loss: 0.9450...\n",
      "Epoch: 45/100... Step: 1637... Loss: 0.9437...\n",
      "Epoch: 45/100... Step: 1638... Loss: 0.9413...\n",
      "Epoch: 45/100... Step: 1639... Loss: 0.9291...\n",
      "Epoch: 45/100... Step: 1640... Loss: 0.9215...\n",
      "Epoch: 45/100... Step: 1641... Loss: 0.9206...\n",
      "Epoch: 45/100... Step: 1642... Loss: 0.9287...\n",
      "Epoch: 45/100... Step: 1643... Loss: 0.9281...\n",
      "Epoch: 45/100... Step: 1644... Loss: 0.9191...\n",
      "Epoch: 45/100... Step: 1645... Loss: 0.9236...\n",
      "Epoch: 45/100... Step: 1646... Loss: 0.9415...\n",
      "Epoch: 45/100... Step: 1647... Loss: 0.9390...\n",
      "Epoch: 45/100... Step: 1648... Loss: 0.9578...\n",
      "Epoch: 45/100... Step: 1649... Loss: 0.9474...\n",
      "Epoch: 45/100... Step: 1650... Loss: 0.9416...\n",
      "Epoch: 45/100... Step: 1651... Loss: 0.9334...\n",
      "Epoch: 45/100... Step: 1652... Loss: 0.9390...\n",
      "Epoch: 45/100... Step: 1653... Loss: 0.9505...\n",
      "Epoch: 45/100... Step: 1654... Loss: 0.9280...\n",
      "Epoch: 45/100... Step: 1655... Loss: 0.9362...\n",
      "Epoch: 45/100... Step: 1656... Loss: 0.9316...\n",
      "Epoch: 45/100... Step: 1657... Loss: 0.9453...\n",
      "Epoch: 45/100... Step: 1658... Loss: 0.9401...\n",
      "Epoch: 45/100... Step: 1659... Loss: 0.9432...\n",
      "Epoch: 45/100... Step: 1660... Loss: 0.9355...\n",
      "Epoch: 45/100... Step: 1661... Loss: 0.9365...\n",
      "Epoch: 45/100... Step: 1662... Loss: 0.9135...\n",
      "Epoch: 45/100... Step: 1663... Loss: 0.9431...\n",
      "Epoch: 45/100... Step: 1664... Loss: 0.9229...\n",
      "Epoch: 45/100... Step: 1665... Loss: 0.9201...\n",
      "Epoch: 46/100... Step: 1666... Loss: 0.9422...\n",
      "Epoch: 46/100... Step: 1667... Loss: 0.9223...\n",
      "Epoch: 46/100... Step: 1668... Loss: 0.9218...\n",
      "Epoch: 46/100... Step: 1669... Loss: 0.9272...\n",
      "Epoch: 46/100... Step: 1670... Loss: 0.9232...\n",
      "Epoch: 46/100... Step: 1671... Loss: 0.9300...\n",
      "Epoch: 46/100... Step: 1672... Loss: 0.9231...\n",
      "Epoch: 46/100... Step: 1673... Loss: 0.9383...\n",
      "Epoch: 46/100... Step: 1674... Loss: 0.9382...\n",
      "Epoch: 46/100... Step: 1675... Loss: 0.9411...\n",
      "Epoch: 46/100... Step: 1676... Loss: 0.9199...\n",
      "Epoch: 46/100... Step: 1677... Loss: 0.9033...\n",
      "Epoch: 46/100... Step: 1678... Loss: 0.9049...\n",
      "Epoch: 46/100... Step: 1679... Loss: 0.9170...\n",
      "Epoch: 46/100... Step: 1680... Loss: 0.9166...\n",
      "Epoch: 46/100... Step: 1681... Loss: 0.9044...\n",
      "Epoch: 46/100... Step: 1682... Loss: 0.9098...\n",
      "Epoch: 46/100... Step: 1683... Loss: 0.9182...\n",
      "Epoch: 46/100... Step: 1684... Loss: 0.9057...\n",
      "Epoch: 46/100... Step: 1685... Loss: 0.9194...\n",
      "Epoch: 46/100... Step: 1686... Loss: 0.9133...\n",
      "Epoch: 46/100... Step: 1687... Loss: 0.9183...\n",
      "Epoch: 46/100... Step: 1688... Loss: 0.9014...\n",
      "Epoch: 46/100... Step: 1689... Loss: 0.9026...\n",
      "Epoch: 46/100... Step: 1690... Loss: 0.9183...\n",
      "Epoch: 46/100... Step: 1691... Loss: 0.8980...\n",
      "Epoch: 46/100... Step: 1692... Loss: 0.9039...\n",
      "Epoch: 46/100... Step: 1693... Loss: 0.8966...\n",
      "Epoch: 46/100... Step: 1694... Loss: 0.9160...\n",
      "Epoch: 46/100... Step: 1695... Loss: 0.9125...\n",
      "Epoch: 46/100... Step: 1696... Loss: 0.9195...\n",
      "Epoch: 46/100... Step: 1697... Loss: 0.9111...\n",
      "Epoch: 46/100... Step: 1698... Loss: 0.9137...\n",
      "Epoch: 46/100... Step: 1699... Loss: 0.8913...\n",
      "Epoch: 46/100... Step: 1700... Loss: 0.9177...\n",
      "Epoch: 46/100... Step: 1701... Loss: 0.8941...\n",
      "Epoch: 46/100... Step: 1702... Loss: 0.8936...\n",
      "Epoch: 47/100... Step: 1703... Loss: 0.9149...\n",
      "Epoch: 47/100... Step: 1704... Loss: 0.9066...\n",
      "Epoch: 47/100... Step: 1705... Loss: 0.9022...\n",
      "Epoch: 47/100... Step: 1706... Loss: 0.9114...\n",
      "Epoch: 47/100... Step: 1707... Loss: 0.9136...\n",
      "Epoch: 47/100... Step: 1708... Loss: 0.9189...\n",
      "Epoch: 47/100... Step: 1709... Loss: 0.9149...\n",
      "Epoch: 47/100... Step: 1710... Loss: 0.9269...\n",
      "Epoch: 47/100... Step: 1711... Loss: 0.9321...\n",
      "Epoch: 47/100... Step: 1712... Loss: 0.9285...\n",
      "Epoch: 47/100... Step: 1713... Loss: 0.9109...\n",
      "Epoch: 47/100... Step: 1714... Loss: 0.9006...\n",
      "Epoch: 47/100... Step: 1715... Loss: 0.9005...\n",
      "Epoch: 47/100... Step: 1716... Loss: 0.9148...\n",
      "Epoch: 47/100... Step: 1717... Loss: 0.9137...\n",
      "Epoch: 47/100... Step: 1718... Loss: 0.9043...\n",
      "Epoch: 47/100... Step: 1719... Loss: 0.9079...\n",
      "Epoch: 47/100... Step: 1720... Loss: 0.9142...\n",
      "Epoch: 47/100... Step: 1721... Loss: 0.9025...\n",
      "Epoch: 47/100... Step: 1722... Loss: 0.9161...\n",
      "Epoch: 47/100... Step: 1723... Loss: 0.9092...\n",
      "Epoch: 47/100... Step: 1724... Loss: 0.9117...\n",
      "Epoch: 47/100... Step: 1725... Loss: 0.8993...\n",
      "Epoch: 47/100... Step: 1726... Loss: 0.9023...\n",
      "Epoch: 47/100... Step: 1727... Loss: 0.9149...\n",
      "Epoch: 47/100... Step: 1728... Loss: 0.8919...\n",
      "Epoch: 47/100... Step: 1729... Loss: 0.9012...\n",
      "Epoch: 47/100... Step: 1730... Loss: 0.8998...\n",
      "Epoch: 47/100... Step: 1731... Loss: 0.9137...\n",
      "Epoch: 47/100... Step: 1732... Loss: 0.9083...\n",
      "Epoch: 47/100... Step: 1733... Loss: 0.9182...\n",
      "Epoch: 47/100... Step: 1734... Loss: 0.9132...\n",
      "Epoch: 47/100... Step: 1735... Loss: 0.9545...\n",
      "Epoch: 47/100... Step: 1736... Loss: 0.9494...\n",
      "Epoch: 47/100... Step: 1737... Loss: 0.9848...\n",
      "Epoch: 47/100... Step: 1738... Loss: 0.9780...\n",
      "Epoch: 47/100... Step: 1739... Loss: 0.9707...\n",
      "Epoch: 48/100... Step: 1740... Loss: 1.0006...\n",
      "Epoch: 48/100... Step: 1741... Loss: 0.9795...\n",
      "Epoch: 48/100... Step: 1742... Loss: 0.9724...\n",
      "Epoch: 48/100... Step: 1743... Loss: 0.9802...\n",
      "Epoch: 48/100... Step: 1744... Loss: 0.9815...\n",
      "Epoch: 48/100... Step: 1745... Loss: 0.9831...\n",
      "Epoch: 48/100... Step: 1746... Loss: 0.9699...\n",
      "Epoch: 48/100... Step: 1747... Loss: 0.9831...\n",
      "Epoch: 48/100... Step: 1748... Loss: 0.9792...\n",
      "Epoch: 48/100... Step: 1749... Loss: 0.9783...\n",
      "Epoch: 48/100... Step: 1750... Loss: 0.9557...\n",
      "Epoch: 48/100... Step: 1751... Loss: 0.9365...\n",
      "Epoch: 48/100... Step: 1752... Loss: 0.9401...\n",
      "Epoch: 48/100... Step: 1753... Loss: 0.9464...\n",
      "Epoch: 48/100... Step: 1754... Loss: 0.9449...\n",
      "Epoch: 48/100... Step: 1755... Loss: 0.9366...\n",
      "Epoch: 48/100... Step: 1756... Loss: 0.9353...\n",
      "Epoch: 48/100... Step: 1757... Loss: 0.9423...\n",
      "Epoch: 48/100... Step: 1758... Loss: 0.9301...\n",
      "Epoch: 48/100... Step: 1759... Loss: 0.9435...\n",
      "Epoch: 48/100... Step: 1760... Loss: 0.9307...\n",
      "Epoch: 48/100... Step: 1761... Loss: 0.9318...\n",
      "Epoch: 48/100... Step: 1762... Loss: 0.9174...\n",
      "Epoch: 48/100... Step: 1763... Loss: 0.9168...\n",
      "Epoch: 48/100... Step: 1764... Loss: 0.9312...\n",
      "Epoch: 48/100... Step: 1765... Loss: 0.9104...\n",
      "Epoch: 48/100... Step: 1766... Loss: 0.9158...\n",
      "Epoch: 48/100... Step: 1767... Loss: 0.9048...\n",
      "Epoch: 48/100... Step: 1768... Loss: 0.9279...\n",
      "Epoch: 48/100... Step: 1769... Loss: 0.9225...\n",
      "Epoch: 48/100... Step: 1770... Loss: 0.9204...\n",
      "Epoch: 48/100... Step: 1771... Loss: 0.9202...\n",
      "Epoch: 48/100... Step: 1772... Loss: 0.9237...\n",
      "Epoch: 48/100... Step: 1773... Loss: 0.9028...\n",
      "Epoch: 48/100... Step: 1774... Loss: 0.9289...\n",
      "Epoch: 48/100... Step: 1775... Loss: 0.9068...\n",
      "Epoch: 48/100... Step: 1776... Loss: 0.9027...\n",
      "Epoch: 49/100... Step: 1777... Loss: 0.9323...\n",
      "Epoch: 49/100... Step: 1778... Loss: 0.9142...\n",
      "Epoch: 49/100... Step: 1779... Loss: 0.9060...\n",
      "Epoch: 49/100... Step: 1780... Loss: 0.9129...\n",
      "Epoch: 49/100... Step: 1781... Loss: 0.9121...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49/100... Step: 1782... Loss: 0.9152...\n",
      "Epoch: 49/100... Step: 1783... Loss: 0.9091...\n",
      "Epoch: 49/100... Step: 1784... Loss: 0.9225...\n",
      "Epoch: 49/100... Step: 1785... Loss: 0.9224...\n",
      "Epoch: 49/100... Step: 1786... Loss: 0.9313...\n",
      "Epoch: 49/100... Step: 1787... Loss: 0.9106...\n",
      "Epoch: 49/100... Step: 1788... Loss: 0.8907...\n",
      "Epoch: 49/100... Step: 1789... Loss: 0.8905...\n",
      "Epoch: 49/100... Step: 1790... Loss: 0.9064...\n",
      "Epoch: 49/100... Step: 1791... Loss: 0.9027...\n",
      "Epoch: 49/100... Step: 1792... Loss: 0.8937...\n",
      "Epoch: 49/100... Step: 1793... Loss: 0.8958...\n",
      "Epoch: 49/100... Step: 1794... Loss: 0.8996...\n",
      "Epoch: 49/100... Step: 1795... Loss: 0.8929...\n",
      "Epoch: 49/100... Step: 1796... Loss: 0.9071...\n",
      "Epoch: 49/100... Step: 1797... Loss: 0.8950...\n",
      "Epoch: 49/100... Step: 1798... Loss: 0.8996...\n",
      "Epoch: 49/100... Step: 1799... Loss: 0.8867...\n",
      "Epoch: 49/100... Step: 1800... Loss: 0.8850...\n",
      "Epoch: 49/100... Step: 1801... Loss: 0.9003...\n",
      "Epoch: 49/100... Step: 1802... Loss: 0.8789...\n",
      "Epoch: 49/100... Step: 1803... Loss: 0.8851...\n",
      "Epoch: 49/100... Step: 1804... Loss: 0.8756...\n",
      "Epoch: 49/100... Step: 1805... Loss: 0.8946...\n",
      "Epoch: 49/100... Step: 1806... Loss: 0.8919...\n",
      "Epoch: 49/100... Step: 1807... Loss: 0.8952...\n",
      "Epoch: 49/100... Step: 1808... Loss: 0.8925...\n",
      "Epoch: 49/100... Step: 1809... Loss: 0.8976...\n",
      "Epoch: 49/100... Step: 1810... Loss: 0.8789...\n",
      "Epoch: 49/100... Step: 1811... Loss: 0.9014...\n",
      "Epoch: 49/100... Step: 1812... Loss: 0.8820...\n",
      "Epoch: 49/100... Step: 1813... Loss: 0.8790...\n",
      "Epoch: 50/100... Step: 1814... Loss: 0.9055...\n",
      "Epoch: 50/100... Step: 1815... Loss: 0.8899...\n",
      "Epoch: 50/100... Step: 1816... Loss: 0.8814...\n",
      "Epoch: 50/100... Step: 1817... Loss: 0.8888...\n",
      "Epoch: 50/100... Step: 1818... Loss: 0.8926...\n",
      "Epoch: 50/100... Step: 1819... Loss: 0.8948...\n",
      "Epoch: 50/100... Step: 1820... Loss: 0.8883...\n",
      "Epoch: 50/100... Step: 1821... Loss: 0.9030...\n",
      "Epoch: 50/100... Step: 1822... Loss: 0.9010...\n",
      "Epoch: 50/100... Step: 1823... Loss: 0.9037...\n",
      "Epoch: 50/100... Step: 1824... Loss: 0.8902...\n",
      "Epoch: 50/100... Step: 1825... Loss: 0.8845...\n",
      "Epoch: 50/100... Step: 1826... Loss: 0.8758...\n",
      "Epoch: 50/100... Step: 1827... Loss: 0.8889...\n",
      "Epoch: 50/100... Step: 1828... Loss: 0.8942...\n",
      "Epoch: 50/100... Step: 1829... Loss: 0.8826...\n",
      "Epoch: 50/100... Step: 1830... Loss: 0.8874...\n",
      "Epoch: 50/100... Step: 1831... Loss: 0.8929...\n",
      "Epoch: 50/100... Step: 1832... Loss: 0.8831...\n",
      "Epoch: 50/100... Step: 1833... Loss: 0.8994...\n",
      "Epoch: 50/100... Step: 1834... Loss: 0.8906...\n",
      "Epoch: 50/100... Step: 1835... Loss: 0.8939...\n",
      "Epoch: 50/100... Step: 1836... Loss: 0.8836...\n",
      "Epoch: 50/100... Step: 1837... Loss: 0.8852...\n",
      "Epoch: 50/100... Step: 1838... Loss: 0.9024...\n",
      "Epoch: 50/100... Step: 1839... Loss: 0.8771...\n",
      "Epoch: 50/100... Step: 1840... Loss: 0.8853...\n",
      "Epoch: 50/100... Step: 1841... Loss: 0.8818...\n",
      "Epoch: 50/100... Step: 1842... Loss: 0.9000...\n",
      "Epoch: 50/100... Step: 1843... Loss: 0.8905...\n",
      "Epoch: 50/100... Step: 1844... Loss: 0.8980...\n",
      "Epoch: 50/100... Step: 1845... Loss: 0.8974...\n",
      "Epoch: 50/100... Step: 1846... Loss: 0.8996...\n",
      "Epoch: 50/100... Step: 1847... Loss: 0.8846...\n",
      "Epoch: 50/100... Step: 1848... Loss: 0.9069...\n",
      "Epoch: 50/100... Step: 1849... Loss: 0.8847...\n",
      "Epoch: 50/100... Step: 1850... Loss: 0.8835...\n",
      "Epoch: 51/100... Step: 1851... Loss: 0.9058...\n",
      "Epoch: 51/100... Step: 1852... Loss: 0.8955...\n",
      "Epoch: 51/100... Step: 1853... Loss: 0.8905...\n",
      "Epoch: 51/100... Step: 1854... Loss: 0.8941...\n",
      "Epoch: 51/100... Step: 1855... Loss: 0.8932...\n",
      "Epoch: 51/100... Step: 1856... Loss: 0.8958...\n",
      "Epoch: 51/100... Step: 1857... Loss: 0.8946...\n",
      "Epoch: 51/100... Step: 1858... Loss: 0.9075...\n",
      "Epoch: 51/100... Step: 1859... Loss: 0.9114...\n",
      "Epoch: 51/100... Step: 1860... Loss: 0.9130...\n",
      "Epoch: 51/100... Step: 1861... Loss: 0.8897...\n",
      "Epoch: 51/100... Step: 1862... Loss: 0.8879...\n",
      "Epoch: 51/100... Step: 1863... Loss: 0.8872...\n",
      "Epoch: 51/100... Step: 1864... Loss: 0.8963...\n",
      "Epoch: 51/100... Step: 1865... Loss: 0.8976...\n",
      "Epoch: 51/100... Step: 1866... Loss: 0.8904...\n",
      "Epoch: 51/100... Step: 1867... Loss: 0.8935...\n",
      "Epoch: 51/100... Step: 1868... Loss: 0.8989...\n",
      "Epoch: 51/100... Step: 1869... Loss: 0.8870...\n",
      "Epoch: 51/100... Step: 1870... Loss: 0.9073...\n",
      "Epoch: 51/100... Step: 1871... Loss: 0.8990...\n",
      "Epoch: 51/100... Step: 1872... Loss: 0.8976...\n",
      "Epoch: 51/100... Step: 1873... Loss: 0.8890...\n",
      "Epoch: 51/100... Step: 1874... Loss: 0.8999...\n",
      "Epoch: 51/100... Step: 1875... Loss: 0.9129...\n",
      "Epoch: 51/100... Step: 1876... Loss: 0.8883...\n",
      "Epoch: 51/100... Step: 1877... Loss: 0.8940...\n",
      "Epoch: 51/100... Step: 1878... Loss: 0.8898...\n",
      "Epoch: 51/100... Step: 1879... Loss: 0.9126...\n",
      "Epoch: 51/100... Step: 1880... Loss: 0.9097...\n",
      "Epoch: 51/100... Step: 1881... Loss: 0.9034...\n",
      "Epoch: 51/100... Step: 1882... Loss: 0.9057...\n",
      "Epoch: 51/100... Step: 1883... Loss: 0.9125...\n",
      "Epoch: 51/100... Step: 1884... Loss: 0.8926...\n",
      "Epoch: 51/100... Step: 1885... Loss: 0.9095...\n",
      "Epoch: 51/100... Step: 1886... Loss: 0.8941...\n",
      "Epoch: 51/100... Step: 1887... Loss: 0.8841...\n",
      "Epoch: 52/100... Step: 1888... Loss: 0.9127...\n",
      "Epoch: 52/100... Step: 1889... Loss: 0.9000...\n",
      "Epoch: 52/100... Step: 1890... Loss: 0.8929...\n",
      "Epoch: 52/100... Step: 1891... Loss: 0.8944...\n",
      "Epoch: 52/100... Step: 1892... Loss: 0.8975...\n",
      "Epoch: 52/100... Step: 1893... Loss: 0.9002...\n",
      "Epoch: 52/100... Step: 1894... Loss: 0.8942...\n",
      "Epoch: 52/100... Step: 1895... Loss: 0.9099...\n",
      "Epoch: 52/100... Step: 1896... Loss: 0.9082...\n",
      "Epoch: 52/100... Step: 1897... Loss: 0.9087...\n",
      "Epoch: 52/100... Step: 1898... Loss: 0.8915...\n",
      "Epoch: 52/100... Step: 1899... Loss: 0.8829...\n",
      "Epoch: 52/100... Step: 1900... Loss: 0.8807...\n",
      "Epoch: 52/100... Step: 1901... Loss: 0.8865...\n",
      "Epoch: 52/100... Step: 1902... Loss: 0.8891...\n",
      "Epoch: 52/100... Step: 1903... Loss: 0.8843...\n",
      "Epoch: 52/100... Step: 1904... Loss: 0.8886...\n",
      "Epoch: 52/100... Step: 1905... Loss: 0.8916...\n",
      "Epoch: 52/100... Step: 1906... Loss: 0.8800...\n",
      "Epoch: 52/100... Step: 1907... Loss: 0.8934...\n",
      "Epoch: 52/100... Step: 1908... Loss: 0.8902...\n",
      "Epoch: 52/100... Step: 1909... Loss: 0.8889...\n",
      "Epoch: 52/100... Step: 1910... Loss: 0.8741...\n",
      "Epoch: 52/100... Step: 1911... Loss: 0.8845...\n",
      "Epoch: 52/100... Step: 1912... Loss: 0.9000...\n",
      "Epoch: 52/100... Step: 1913... Loss: 0.8781...\n",
      "Epoch: 52/100... Step: 1914... Loss: 0.8838...\n",
      "Epoch: 52/100... Step: 1915... Loss: 0.8810...\n",
      "Epoch: 52/100... Step: 1916... Loss: 0.8985...\n",
      "Epoch: 52/100... Step: 1917... Loss: 0.8923...\n",
      "Epoch: 52/100... Step: 1918... Loss: 0.8978...\n",
      "Epoch: 52/100... Step: 1919... Loss: 0.8947...\n",
      "Epoch: 52/100... Step: 1920... Loss: 0.9034...\n",
      "Epoch: 52/100... Step: 1921... Loss: 0.8772...\n",
      "Epoch: 52/100... Step: 1922... Loss: 0.8991...\n",
      "Epoch: 52/100... Step: 1923... Loss: 0.8835...\n",
      "Epoch: 52/100... Step: 1924... Loss: 0.8825...\n",
      "Epoch: 53/100... Step: 1925... Loss: 0.8903...\n",
      "Epoch: 53/100... Step: 1926... Loss: 0.8851...\n",
      "Epoch: 53/100... Step: 1927... Loss: 0.8789...\n",
      "Epoch: 53/100... Step: 1928... Loss: 0.8855...\n",
      "Epoch: 53/100... Step: 1929... Loss: 0.8863...\n",
      "Epoch: 53/100... Step: 1930... Loss: 0.8887...\n",
      "Epoch: 53/100... Step: 1931... Loss: 0.8790...\n",
      "Epoch: 53/100... Step: 1932... Loss: 0.8961...\n",
      "Epoch: 53/100... Step: 1933... Loss: 0.8939...\n",
      "Epoch: 53/100... Step: 1934... Loss: 0.8976...\n",
      "Epoch: 53/100... Step: 1935... Loss: 0.8849...\n",
      "Epoch: 53/100... Step: 1936... Loss: 0.8760...\n",
      "Epoch: 53/100... Step: 1937... Loss: 0.8721...\n",
      "Epoch: 53/100... Step: 1938... Loss: 0.8808...\n",
      "Epoch: 53/100... Step: 1939... Loss: 0.8817...\n",
      "Epoch: 53/100... Step: 1940... Loss: 0.8721...\n",
      "Epoch: 53/100... Step: 1941... Loss: 0.8797...\n",
      "Epoch: 53/100... Step: 1942... Loss: 0.8888...\n",
      "Epoch: 53/100... Step: 1943... Loss: 0.8731...\n",
      "Epoch: 53/100... Step: 1944... Loss: 0.8848...\n",
      "Epoch: 53/100... Step: 1945... Loss: 0.8821...\n",
      "Epoch: 53/100... Step: 1946... Loss: 0.8818...\n",
      "Epoch: 53/100... Step: 1947... Loss: 0.8699...\n",
      "Epoch: 53/100... Step: 1948... Loss: 0.8743...\n",
      "Epoch: 53/100... Step: 1949... Loss: 0.8900...\n",
      "Epoch: 53/100... Step: 1950... Loss: 0.8686...\n",
      "Epoch: 53/100... Step: 1951... Loss: 0.8758...\n",
      "Epoch: 53/100... Step: 1952... Loss: 0.8743...\n",
      "Epoch: 53/100... Step: 1953... Loss: 0.8904...\n",
      "Epoch: 53/100... Step: 1954... Loss: 0.8825...\n",
      "Epoch: 53/100... Step: 1955... Loss: 0.8891...\n",
      "Epoch: 53/100... Step: 1956... Loss: 0.8847...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53/100... Step: 1957... Loss: 0.8925...\n",
      "Epoch: 53/100... Step: 1958... Loss: 0.8771...\n",
      "Epoch: 53/100... Step: 1959... Loss: 0.8932...\n",
      "Epoch: 53/100... Step: 1960... Loss: 0.8732...\n",
      "Epoch: 53/100... Step: 1961... Loss: 0.8727...\n",
      "Epoch: 54/100... Step: 1962... Loss: 0.8964...\n",
      "Epoch: 54/100... Step: 1963... Loss: 0.8800...\n",
      "Epoch: 54/100... Step: 1964... Loss: 0.8785...\n",
      "Epoch: 54/100... Step: 1965... Loss: 0.8859...\n",
      "Epoch: 54/100... Step: 1966... Loss: 0.8806...\n",
      "Epoch: 54/100... Step: 1967... Loss: 0.8867...\n",
      "Epoch: 54/100... Step: 1968... Loss: 0.8813...\n",
      "Epoch: 54/100... Step: 1969... Loss: 0.8899...\n",
      "Epoch: 54/100... Step: 1970... Loss: 0.8937...\n",
      "Epoch: 54/100... Step: 1971... Loss: 0.8966...\n",
      "Epoch: 54/100... Step: 1972... Loss: 0.8708...\n",
      "Epoch: 54/100... Step: 1973... Loss: 0.8727...\n",
      "Epoch: 54/100... Step: 1974... Loss: 0.8713...\n",
      "Epoch: 54/100... Step: 1975... Loss: 0.8792...\n",
      "Epoch: 54/100... Step: 1976... Loss: 0.8777...\n",
      "Epoch: 54/100... Step: 1977... Loss: 0.8762...\n",
      "Epoch: 54/100... Step: 1978... Loss: 0.8780...\n",
      "Epoch: 54/100... Step: 1979... Loss: 0.8802...\n",
      "Epoch: 54/100... Step: 1980... Loss: 0.8698...\n",
      "Epoch: 54/100... Step: 1981... Loss: 0.8849...\n",
      "Epoch: 54/100... Step: 1982... Loss: 0.8787...\n",
      "Epoch: 54/100... Step: 1983... Loss: 0.8807...\n",
      "Epoch: 54/100... Step: 1984... Loss: 0.8719...\n",
      "Epoch: 54/100... Step: 1985... Loss: 0.8697...\n",
      "Epoch: 54/100... Step: 1986... Loss: 0.8863...\n",
      "Epoch: 54/100... Step: 1987... Loss: 0.8724...\n",
      "Epoch: 54/100... Step: 1988... Loss: 0.8771...\n",
      "Epoch: 54/100... Step: 1989... Loss: 0.8724...\n",
      "Epoch: 54/100... Step: 1990... Loss: 0.8844...\n",
      "Epoch: 54/100... Step: 1991... Loss: 0.8799...\n",
      "Epoch: 54/100... Step: 1992... Loss: 0.8840...\n",
      "Epoch: 54/100... Step: 1993... Loss: 0.8848...\n",
      "Epoch: 54/100... Step: 1994... Loss: 0.8861...\n",
      "Epoch: 54/100... Step: 1995... Loss: 0.8664...\n",
      "Epoch: 54/100... Step: 1996... Loss: 0.8991...\n",
      "Epoch: 54/100... Step: 1997... Loss: 0.8773...\n",
      "Epoch: 54/100... Step: 1998... Loss: 0.8631...\n",
      "Epoch: 55/100... Step: 1999... Loss: 0.8840...\n",
      "Epoch: 55/100... Step: 2000... Loss: 0.8761...\n",
      "Epoch: 55/100... Step: 2001... Loss: 0.8748...\n",
      "Epoch: 55/100... Step: 2002... Loss: 0.8956...\n",
      "Epoch: 55/100... Step: 2003... Loss: 0.9014...\n",
      "Epoch: 55/100... Step: 2004... Loss: 0.9046...\n",
      "Epoch: 55/100... Step: 2005... Loss: 0.8966...\n",
      "Epoch: 55/100... Step: 2006... Loss: 0.9079...\n",
      "Epoch: 55/100... Step: 2007... Loss: 0.9090...\n",
      "Epoch: 55/100... Step: 2008... Loss: 0.9151...\n",
      "Epoch: 55/100... Step: 2009... Loss: 0.8953...\n",
      "Epoch: 55/100... Step: 2010... Loss: 0.8797...\n",
      "Epoch: 55/100... Step: 2011... Loss: 0.8733...\n",
      "Epoch: 55/100... Step: 2012... Loss: 0.8876...\n",
      "Epoch: 55/100... Step: 2013... Loss: 0.8912...\n",
      "Epoch: 55/100... Step: 2014... Loss: 0.8814...\n",
      "Epoch: 55/100... Step: 2015... Loss: 0.8843...\n",
      "Epoch: 55/100... Step: 2016... Loss: 0.8860...\n",
      "Epoch: 55/100... Step: 2017... Loss: 0.8732...\n",
      "Epoch: 55/100... Step: 2018... Loss: 0.8871...\n",
      "Epoch: 55/100... Step: 2019... Loss: 0.8821...\n",
      "Epoch: 55/100... Step: 2020... Loss: 0.8848...\n",
      "Epoch: 55/100... Step: 2021... Loss: 0.8682...\n",
      "Epoch: 55/100... Step: 2022... Loss: 0.8722...\n",
      "Epoch: 55/100... Step: 2023... Loss: 0.8841...\n",
      "Epoch: 55/100... Step: 2024... Loss: 0.8658...\n",
      "Epoch: 55/100... Step: 2025... Loss: 0.8723...\n",
      "Epoch: 55/100... Step: 2026... Loss: 0.8635...\n",
      "Epoch: 55/100... Step: 2027... Loss: 0.8859...\n",
      "Epoch: 55/100... Step: 2028... Loss: 0.8853...\n",
      "Epoch: 55/100... Step: 2029... Loss: 0.8836...\n",
      "Epoch: 55/100... Step: 2030... Loss: 0.8821...\n",
      "Epoch: 55/100... Step: 2031... Loss: 0.8880...\n",
      "Epoch: 55/100... Step: 2032... Loss: 0.8650...\n",
      "Epoch: 55/100... Step: 2033... Loss: 0.8861...\n",
      "Epoch: 55/100... Step: 2034... Loss: 0.8740...\n",
      "Epoch: 55/100... Step: 2035... Loss: 0.8723...\n",
      "Epoch: 56/100... Step: 2036... Loss: 0.8854...\n",
      "Epoch: 56/100... Step: 2037... Loss: 0.8749...\n",
      "Epoch: 56/100... Step: 2038... Loss: 0.8698...\n",
      "Epoch: 56/100... Step: 2039... Loss: 0.8817...\n",
      "Epoch: 56/100... Step: 2040... Loss: 0.8792...\n",
      "Epoch: 56/100... Step: 2041... Loss: 0.8784...\n",
      "Epoch: 56/100... Step: 2042... Loss: 0.8678...\n",
      "Epoch: 56/100... Step: 2043... Loss: 0.8802...\n",
      "Epoch: 56/100... Step: 2044... Loss: 0.8874...\n",
      "Epoch: 56/100... Step: 2045... Loss: 0.8859...\n",
      "Epoch: 56/100... Step: 2046... Loss: 0.8675...\n",
      "Epoch: 56/100... Step: 2047... Loss: 0.8583...\n",
      "Epoch: 56/100... Step: 2048... Loss: 0.8570...\n",
      "Epoch: 56/100... Step: 2049... Loss: 0.8670...\n",
      "Epoch: 56/100... Step: 2050... Loss: 0.8644...\n",
      "Epoch: 56/100... Step: 2051... Loss: 0.8561...\n",
      "Epoch: 56/100... Step: 2052... Loss: 0.8566...\n",
      "Epoch: 56/100... Step: 2053... Loss: 0.8657...\n",
      "Epoch: 56/100... Step: 2054... Loss: 0.8552...\n",
      "Epoch: 56/100... Step: 2055... Loss: 0.8621...\n",
      "Epoch: 56/100... Step: 2056... Loss: 0.8595...\n",
      "Epoch: 56/100... Step: 2057... Loss: 0.8642...\n",
      "Epoch: 56/100... Step: 2058... Loss: 0.8539...\n",
      "Epoch: 56/100... Step: 2059... Loss: 0.8527...\n",
      "Epoch: 56/100... Step: 2060... Loss: 0.8665...\n",
      "Epoch: 56/100... Step: 2061... Loss: 0.8469...\n",
      "Epoch: 56/100... Step: 2062... Loss: 0.8515...\n",
      "Epoch: 56/100... Step: 2063... Loss: 0.8499...\n",
      "Epoch: 56/100... Step: 2064... Loss: 0.8640...\n",
      "Epoch: 56/100... Step: 2065... Loss: 0.8587...\n",
      "Epoch: 56/100... Step: 2066... Loss: 0.8674...\n",
      "Epoch: 56/100... Step: 2067... Loss: 0.8665...\n",
      "Epoch: 56/100... Step: 2068... Loss: 0.8682...\n",
      "Epoch: 56/100... Step: 2069... Loss: 0.8539...\n",
      "Epoch: 56/100... Step: 2070... Loss: 0.8788...\n",
      "Epoch: 56/100... Step: 2071... Loss: 0.8593...\n",
      "Epoch: 56/100... Step: 2072... Loss: 0.8597...\n",
      "Epoch: 57/100... Step: 2073... Loss: 0.8721...\n",
      "Epoch: 57/100... Step: 2074... Loss: 0.8615...\n",
      "Epoch: 57/100... Step: 2075... Loss: 0.8603...\n",
      "Epoch: 57/100... Step: 2076... Loss: 0.8792...\n",
      "Epoch: 57/100... Step: 2077... Loss: 0.8708...\n",
      "Epoch: 57/100... Step: 2078... Loss: 0.8732...\n",
      "Epoch: 57/100... Step: 2079... Loss: 0.8694...\n",
      "Epoch: 57/100... Step: 2080... Loss: 0.8739...\n",
      "Epoch: 57/100... Step: 2081... Loss: 0.8738...\n",
      "Epoch: 57/100... Step: 2082... Loss: 0.8785...\n",
      "Epoch: 57/100... Step: 2083... Loss: 0.8699...\n",
      "Epoch: 57/100... Step: 2084... Loss: 0.8545...\n",
      "Epoch: 57/100... Step: 2085... Loss: 0.8507...\n",
      "Epoch: 57/100... Step: 2086... Loss: 0.8592...\n",
      "Epoch: 57/100... Step: 2087... Loss: 0.8579...\n",
      "Epoch: 57/100... Step: 2088... Loss: 0.8499...\n",
      "Epoch: 57/100... Step: 2089... Loss: 0.8557...\n",
      "Epoch: 57/100... Step: 2090... Loss: 0.8599...\n",
      "Epoch: 57/100... Step: 2091... Loss: 0.8472...\n",
      "Epoch: 57/100... Step: 2092... Loss: 0.8617...\n",
      "Epoch: 57/100... Step: 2093... Loss: 0.8552...\n",
      "Epoch: 57/100... Step: 2094... Loss: 0.8584...\n",
      "Epoch: 57/100... Step: 2095... Loss: 0.8519...\n",
      "Epoch: 57/100... Step: 2096... Loss: 0.8568...\n",
      "Epoch: 57/100... Step: 2097... Loss: 0.8657...\n",
      "Epoch: 57/100... Step: 2098... Loss: 0.8488...\n",
      "Epoch: 57/100... Step: 2099... Loss: 0.8524...\n",
      "Epoch: 57/100... Step: 2100... Loss: 0.8464...\n",
      "Epoch: 57/100... Step: 2101... Loss: 0.8666...\n",
      "Epoch: 57/100... Step: 2102... Loss: 0.8594...\n",
      "Epoch: 57/100... Step: 2103... Loss: 0.8622...\n",
      "Epoch: 57/100... Step: 2104... Loss: 0.8629...\n",
      "Epoch: 57/100... Step: 2105... Loss: 0.8684...\n",
      "Epoch: 57/100... Step: 2106... Loss: 0.8528...\n",
      "Epoch: 57/100... Step: 2107... Loss: 0.8816...\n",
      "Epoch: 57/100... Step: 2108... Loss: 0.8633...\n",
      "Epoch: 57/100... Step: 2109... Loss: 0.8547...\n",
      "Epoch: 58/100... Step: 2110... Loss: 0.8766...\n",
      "Epoch: 58/100... Step: 2111... Loss: 0.8656...\n",
      "Epoch: 58/100... Step: 2112... Loss: 0.8630...\n",
      "Epoch: 58/100... Step: 2113... Loss: 0.8678...\n",
      "Epoch: 58/100... Step: 2114... Loss: 0.8695...\n",
      "Epoch: 58/100... Step: 2115... Loss: 0.8822...\n",
      "Epoch: 58/100... Step: 2116... Loss: 0.8750...\n",
      "Epoch: 58/100... Step: 2117... Loss: 0.8841...\n",
      "Epoch: 58/100... Step: 2118... Loss: 0.8837...\n",
      "Epoch: 58/100... Step: 2119... Loss: 0.8806...\n",
      "Epoch: 58/100... Step: 2120... Loss: 0.8648...\n",
      "Epoch: 58/100... Step: 2121... Loss: 0.8550...\n",
      "Epoch: 58/100... Step: 2122... Loss: 0.8523...\n",
      "Epoch: 58/100... Step: 2123... Loss: 0.8658...\n",
      "Epoch: 58/100... Step: 2124... Loss: 0.8592...\n",
      "Epoch: 58/100... Step: 2125... Loss: 0.8511...\n",
      "Epoch: 58/100... Step: 2126... Loss: 0.8567...\n",
      "Epoch: 58/100... Step: 2127... Loss: 0.8622...\n",
      "Epoch: 58/100... Step: 2128... Loss: 0.8487...\n",
      "Epoch: 58/100... Step: 2129... Loss: 0.8620...\n",
      "Epoch: 58/100... Step: 2130... Loss: 0.8561...\n",
      "Epoch: 58/100... Step: 2131... Loss: 0.8556...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58/100... Step: 2132... Loss: 0.8479...\n",
      "Epoch: 58/100... Step: 2133... Loss: 0.8504...\n",
      "Epoch: 58/100... Step: 2134... Loss: 0.8640...\n",
      "Epoch: 58/100... Step: 2135... Loss: 0.8424...\n",
      "Epoch: 58/100... Step: 2136... Loss: 0.8469...\n",
      "Epoch: 58/100... Step: 2137... Loss: 0.8436...\n",
      "Epoch: 58/100... Step: 2138... Loss: 0.8621...\n",
      "Epoch: 58/100... Step: 2139... Loss: 0.8533...\n",
      "Epoch: 58/100... Step: 2140... Loss: 0.8601...\n",
      "Epoch: 58/100... Step: 2141... Loss: 0.8611...\n",
      "Epoch: 58/100... Step: 2142... Loss: 0.8682...\n",
      "Epoch: 58/100... Step: 2143... Loss: 0.8456...\n",
      "Epoch: 58/100... Step: 2144... Loss: 0.8721...\n",
      "Epoch: 58/100... Step: 2145... Loss: 0.8527...\n",
      "Epoch: 58/100... Step: 2146... Loss: 0.8537...\n",
      "Epoch: 59/100... Step: 2147... Loss: 0.8766...\n",
      "Epoch: 59/100... Step: 2148... Loss: 0.8604...\n",
      "Epoch: 59/100... Step: 2149... Loss: 0.8565...\n",
      "Epoch: 59/100... Step: 2150... Loss: 0.8654...\n",
      "Epoch: 59/100... Step: 2151... Loss: 0.8656...\n",
      "Epoch: 59/100... Step: 2152... Loss: 0.8648...\n",
      "Epoch: 59/100... Step: 2153... Loss: 0.8652...\n",
      "Epoch: 59/100... Step: 2154... Loss: 0.8777...\n",
      "Epoch: 59/100... Step: 2155... Loss: 0.8794...\n",
      "Epoch: 59/100... Step: 2156... Loss: 0.8858...\n",
      "Epoch: 59/100... Step: 2157... Loss: 0.8691...\n",
      "Epoch: 59/100... Step: 2158... Loss: 0.8525...\n",
      "Epoch: 59/100... Step: 2159... Loss: 0.8481...\n",
      "Epoch: 59/100... Step: 2160... Loss: 0.8615...\n",
      "Epoch: 59/100... Step: 2161... Loss: 0.8570...\n",
      "Epoch: 59/100... Step: 2162... Loss: 0.8502...\n",
      "Epoch: 59/100... Step: 2163... Loss: 0.8541...\n",
      "Epoch: 59/100... Step: 2164... Loss: 0.8619...\n",
      "Epoch: 59/100... Step: 2165... Loss: 0.8488...\n",
      "Epoch: 59/100... Step: 2166... Loss: 0.8604...\n",
      "Epoch: 59/100... Step: 2167... Loss: 0.8556...\n",
      "Epoch: 59/100... Step: 2168... Loss: 0.8546...\n",
      "Epoch: 59/100... Step: 2169... Loss: 0.8493...\n",
      "Epoch: 59/100... Step: 2170... Loss: 0.8539...\n",
      "Epoch: 59/100... Step: 2171... Loss: 0.8550...\n",
      "Epoch: 59/100... Step: 2172... Loss: 0.8420...\n",
      "Epoch: 59/100... Step: 2173... Loss: 0.8506...\n",
      "Epoch: 59/100... Step: 2174... Loss: 0.8407...\n",
      "Epoch: 59/100... Step: 2175... Loss: 0.8591...\n",
      "Epoch: 59/100... Step: 2176... Loss: 0.8511...\n",
      "Epoch: 59/100... Step: 2177... Loss: 0.8572...\n",
      "Epoch: 59/100... Step: 2178... Loss: 0.8546...\n",
      "Epoch: 59/100... Step: 2179... Loss: 0.8618...\n",
      "Epoch: 59/100... Step: 2180... Loss: 0.8445...\n",
      "Epoch: 59/100... Step: 2181... Loss: 0.8633...\n",
      "Epoch: 59/100... Step: 2182... Loss: 0.8474...\n",
      "Epoch: 59/100... Step: 2183... Loss: 0.8465...\n",
      "Epoch: 60/100... Step: 2184... Loss: 0.8635...\n",
      "Epoch: 60/100... Step: 2185... Loss: 0.8527...\n",
      "Epoch: 60/100... Step: 2186... Loss: 0.8448...\n",
      "Epoch: 60/100... Step: 2187... Loss: 0.8517...\n",
      "Epoch: 60/100... Step: 2188... Loss: 0.8558...\n",
      "Epoch: 60/100... Step: 2189... Loss: 0.8657...\n",
      "Epoch: 60/100... Step: 2190... Loss: 0.8564...\n",
      "Epoch: 60/100... Step: 2191... Loss: 0.8739...\n",
      "Epoch: 60/100... Step: 2192... Loss: 0.8755...\n",
      "Epoch: 60/100... Step: 2193... Loss: 0.8712...\n",
      "Epoch: 60/100... Step: 2194... Loss: 0.8565...\n",
      "Epoch: 60/100... Step: 2195... Loss: 0.8454...\n",
      "Epoch: 60/100... Step: 2196... Loss: 0.8465...\n",
      "Epoch: 60/100... Step: 2197... Loss: 0.8511...\n",
      "Epoch: 60/100... Step: 2198... Loss: 0.8521...\n",
      "Epoch: 60/100... Step: 2199... Loss: 0.8485...\n",
      "Epoch: 60/100... Step: 2200... Loss: 0.8479...\n",
      "Epoch: 60/100... Step: 2201... Loss: 0.8546...\n",
      "Epoch: 60/100... Step: 2202... Loss: 0.8438...\n",
      "Epoch: 60/100... Step: 2203... Loss: 0.8573...\n",
      "Epoch: 60/100... Step: 2204... Loss: 0.8532...\n",
      "Epoch: 60/100... Step: 2205... Loss: 0.8556...\n",
      "Epoch: 60/100... Step: 2206... Loss: 0.8449...\n",
      "Epoch: 60/100... Step: 2207... Loss: 0.8500...\n",
      "Epoch: 60/100... Step: 2208... Loss: 0.8664...\n",
      "Epoch: 60/100... Step: 2209... Loss: 0.8415...\n",
      "Epoch: 60/100... Step: 2210... Loss: 0.8391...\n",
      "Epoch: 60/100... Step: 2211... Loss: 0.8411...\n",
      "Epoch: 60/100... Step: 2212... Loss: 0.8641...\n",
      "Epoch: 60/100... Step: 2213... Loss: 0.8507...\n",
      "Epoch: 60/100... Step: 2214... Loss: 0.8586...\n",
      "Epoch: 60/100... Step: 2215... Loss: 0.8513...\n",
      "Epoch: 60/100... Step: 2216... Loss: 0.8583...\n",
      "Epoch: 60/100... Step: 2217... Loss: 0.8427...\n",
      "Epoch: 60/100... Step: 2218... Loss: 0.8628...\n",
      "Epoch: 60/100... Step: 2219... Loss: 0.8448...\n",
      "Epoch: 60/100... Step: 2220... Loss: 0.8432...\n",
      "Epoch: 61/100... Step: 2221... Loss: 0.8617...\n",
      "Epoch: 61/100... Step: 2222... Loss: 0.8491...\n",
      "Epoch: 61/100... Step: 2223... Loss: 0.8393...\n",
      "Epoch: 61/100... Step: 2224... Loss: 0.8480...\n",
      "Epoch: 61/100... Step: 2225... Loss: 0.8490...\n",
      "Epoch: 61/100... Step: 2226... Loss: 0.8562...\n",
      "Epoch: 61/100... Step: 2227... Loss: 0.8511...\n",
      "Epoch: 61/100... Step: 2228... Loss: 0.8649...\n",
      "Epoch: 61/100... Step: 2229... Loss: 0.8664...\n",
      "Epoch: 61/100... Step: 2230... Loss: 0.8627...\n",
      "Epoch: 61/100... Step: 2231... Loss: 0.8436...\n",
      "Epoch: 61/100... Step: 2232... Loss: 0.8382...\n",
      "Epoch: 61/100... Step: 2233... Loss: 0.8416...\n",
      "Epoch: 61/100... Step: 2234... Loss: 0.8503...\n",
      "Epoch: 61/100... Step: 2235... Loss: 0.8440...\n",
      "Epoch: 61/100... Step: 2236... Loss: 0.8440...\n",
      "Epoch: 61/100... Step: 2237... Loss: 0.8560...\n",
      "Epoch: 61/100... Step: 2238... Loss: 0.8641...\n",
      "Epoch: 61/100... Step: 2239... Loss: 0.8536...\n",
      "Epoch: 61/100... Step: 2240... Loss: 0.8611...\n",
      "Epoch: 61/100... Step: 2241... Loss: 0.8598...\n",
      "Epoch: 61/100... Step: 2242... Loss: 0.8616...\n",
      "Epoch: 61/100... Step: 2243... Loss: 0.8482...\n",
      "Epoch: 61/100... Step: 2244... Loss: 0.8506...\n",
      "Epoch: 61/100... Step: 2245... Loss: 0.8633...\n",
      "Epoch: 61/100... Step: 2246... Loss: 0.8507...\n",
      "Epoch: 61/100... Step: 2247... Loss: 0.8555...\n",
      "Epoch: 61/100... Step: 2248... Loss: 0.8450...\n",
      "Epoch: 61/100... Step: 2249... Loss: 0.8635...\n",
      "Epoch: 61/100... Step: 2250... Loss: 0.8508...\n",
      "Epoch: 61/100... Step: 2251... Loss: 0.8565...\n",
      "Epoch: 61/100... Step: 2252... Loss: 0.8581...\n",
      "Epoch: 61/100... Step: 2253... Loss: 0.8612...\n",
      "Epoch: 61/100... Step: 2254... Loss: 0.8393...\n",
      "Epoch: 61/100... Step: 2255... Loss: 0.8678...\n",
      "Epoch: 61/100... Step: 2256... Loss: 0.8518...\n",
      "Epoch: 61/100... Step: 2257... Loss: 0.8450...\n",
      "Epoch: 62/100... Step: 2258... Loss: 0.8604...\n",
      "Epoch: 62/100... Step: 2259... Loss: 0.8464...\n",
      "Epoch: 62/100... Step: 2260... Loss: 0.8410...\n",
      "Epoch: 62/100... Step: 2261... Loss: 0.8515...\n",
      "Epoch: 62/100... Step: 2262... Loss: 0.8468...\n",
      "Epoch: 62/100... Step: 2263... Loss: 0.8522...\n",
      "Epoch: 62/100... Step: 2264... Loss: 0.8476...\n",
      "Epoch: 62/100... Step: 2265... Loss: 0.8582...\n",
      "Epoch: 62/100... Step: 2266... Loss: 0.8617...\n",
      "Epoch: 62/100... Step: 2267... Loss: 0.8582...\n",
      "Epoch: 62/100... Step: 2268... Loss: 0.8485...\n",
      "Epoch: 62/100... Step: 2269... Loss: 0.8421...\n",
      "Epoch: 62/100... Step: 2270... Loss: 0.8372...\n",
      "Epoch: 62/100... Step: 2271... Loss: 0.8501...\n",
      "Epoch: 62/100... Step: 2272... Loss: 0.8474...\n",
      "Epoch: 62/100... Step: 2273... Loss: 0.8346...\n",
      "Epoch: 62/100... Step: 2274... Loss: 0.8406...\n",
      "Epoch: 62/100... Step: 2275... Loss: 0.8518...\n",
      "Epoch: 62/100... Step: 2276... Loss: 0.8423...\n",
      "Epoch: 62/100... Step: 2277... Loss: 0.8485...\n",
      "Epoch: 62/100... Step: 2278... Loss: 0.8461...\n",
      "Epoch: 62/100... Step: 2279... Loss: 0.8514...\n",
      "Epoch: 62/100... Step: 2280... Loss: 0.8412...\n",
      "Epoch: 62/100... Step: 2281... Loss: 0.8419...\n",
      "Epoch: 62/100... Step: 2282... Loss: 0.8554...\n",
      "Epoch: 62/100... Step: 2283... Loss: 0.8369...\n",
      "Epoch: 62/100... Step: 2284... Loss: 0.8488...\n",
      "Epoch: 62/100... Step: 2285... Loss: 0.8373...\n",
      "Epoch: 62/100... Step: 2286... Loss: 0.8532...\n",
      "Epoch: 62/100... Step: 2287... Loss: 0.8481...\n",
      "Epoch: 62/100... Step: 2288... Loss: 0.8484...\n",
      "Epoch: 62/100... Step: 2289... Loss: 0.8502...\n",
      "Epoch: 62/100... Step: 2290... Loss: 0.8604...\n",
      "Epoch: 62/100... Step: 2291... Loss: 0.8427...\n",
      "Epoch: 62/100... Step: 2292... Loss: 0.8611...\n",
      "Epoch: 62/100... Step: 2293... Loss: 0.8412...\n",
      "Epoch: 62/100... Step: 2294... Loss: 0.8335...\n",
      "Epoch: 63/100... Step: 2295... Loss: 0.8490...\n",
      "Epoch: 63/100... Step: 2296... Loss: 0.8394...\n",
      "Epoch: 63/100... Step: 2297... Loss: 0.8383...\n",
      "Epoch: 63/100... Step: 2298... Loss: 0.8480...\n",
      "Epoch: 63/100... Step: 2299... Loss: 0.8417...\n",
      "Epoch: 63/100... Step: 2300... Loss: 0.8431...\n",
      "Epoch: 63/100... Step: 2301... Loss: 0.8429...\n",
      "Epoch: 63/100... Step: 2302... Loss: 0.8547...\n",
      "Epoch: 63/100... Step: 2303... Loss: 0.8558...\n",
      "Epoch: 63/100... Step: 2304... Loss: 0.8556...\n",
      "Epoch: 63/100... Step: 2305... Loss: 0.8379...\n",
      "Epoch: 63/100... Step: 2306... Loss: 0.8324...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 63/100... Step: 2307... Loss: 0.8257...\n",
      "Epoch: 63/100... Step: 2308... Loss: 0.8365...\n",
      "Epoch: 63/100... Step: 2309... Loss: 0.8398...\n",
      "Epoch: 63/100... Step: 2310... Loss: 0.8324...\n",
      "Epoch: 63/100... Step: 2311... Loss: 0.8366...\n",
      "Epoch: 63/100... Step: 2312... Loss: 0.8468...\n",
      "Epoch: 63/100... Step: 2313... Loss: 0.8332...\n",
      "Epoch: 63/100... Step: 2314... Loss: 0.8430...\n",
      "Epoch: 63/100... Step: 2315... Loss: 0.8369...\n",
      "Epoch: 63/100... Step: 2316... Loss: 0.8421...\n",
      "Epoch: 63/100... Step: 2317... Loss: 0.8299...\n",
      "Epoch: 63/100... Step: 2318... Loss: 0.8335...\n",
      "Epoch: 63/100... Step: 2319... Loss: 0.8473...\n",
      "Epoch: 63/100... Step: 2320... Loss: 0.8361...\n",
      "Epoch: 63/100... Step: 2321... Loss: 0.8360...\n",
      "Epoch: 63/100... Step: 2322... Loss: 0.8291...\n",
      "Epoch: 63/100... Step: 2323... Loss: 0.8509...\n",
      "Epoch: 63/100... Step: 2324... Loss: 0.8455...\n",
      "Epoch: 63/100... Step: 2325... Loss: 0.8457...\n",
      "Epoch: 63/100... Step: 2326... Loss: 0.8441...\n",
      "Epoch: 63/100... Step: 2327... Loss: 0.8585...\n",
      "Epoch: 63/100... Step: 2328... Loss: 0.8421...\n",
      "Epoch: 63/100... Step: 2329... Loss: 0.8588...\n",
      "Epoch: 63/100... Step: 2330... Loss: 0.8466...\n",
      "Epoch: 63/100... Step: 2331... Loss: 0.8457...\n",
      "Epoch: 64/100... Step: 2332... Loss: 0.8548...\n",
      "Epoch: 64/100... Step: 2333... Loss: 0.8361...\n",
      "Epoch: 64/100... Step: 2334... Loss: 0.8297...\n",
      "Epoch: 64/100... Step: 2335... Loss: 0.8440...\n",
      "Epoch: 64/100... Step: 2336... Loss: 0.8437...\n",
      "Epoch: 64/100... Step: 2337... Loss: 0.8481...\n",
      "Epoch: 64/100... Step: 2338... Loss: 0.8487...\n",
      "Epoch: 64/100... Step: 2339... Loss: 0.8628...\n",
      "Epoch: 64/100... Step: 2340... Loss: 0.8521...\n",
      "Epoch: 64/100... Step: 2341... Loss: 0.8521...\n",
      "Epoch: 64/100... Step: 2342... Loss: 0.8418...\n",
      "Epoch: 64/100... Step: 2343... Loss: 0.8343...\n",
      "Epoch: 64/100... Step: 2344... Loss: 0.8328...\n",
      "Epoch: 64/100... Step: 2345... Loss: 0.8385...\n",
      "Epoch: 64/100... Step: 2346... Loss: 0.8346...\n",
      "Epoch: 64/100... Step: 2347... Loss: 0.8319...\n",
      "Epoch: 64/100... Step: 2348... Loss: 0.8381...\n",
      "Epoch: 64/100... Step: 2349... Loss: 0.8396...\n",
      "Epoch: 64/100... Step: 2350... Loss: 0.8277...\n",
      "Epoch: 64/100... Step: 2351... Loss: 0.8458...\n",
      "Epoch: 64/100... Step: 2352... Loss: 0.8382...\n",
      "Epoch: 64/100... Step: 2353... Loss: 0.8342...\n",
      "Epoch: 64/100... Step: 2354... Loss: 0.8337...\n",
      "Epoch: 64/100... Step: 2355... Loss: 0.8421...\n",
      "Epoch: 64/100... Step: 2356... Loss: 0.8384...\n",
      "Epoch: 64/100... Step: 2357... Loss: 0.8275...\n",
      "Epoch: 64/100... Step: 2358... Loss: 0.8415...\n",
      "Epoch: 64/100... Step: 2359... Loss: 0.8320...\n",
      "Epoch: 64/100... Step: 2360... Loss: 0.8428...\n",
      "Epoch: 64/100... Step: 2361... Loss: 0.8402...\n",
      "Epoch: 64/100... Step: 2362... Loss: 0.8448...\n",
      "Epoch: 64/100... Step: 2363... Loss: 0.8453...\n",
      "Epoch: 64/100... Step: 2364... Loss: 0.8509...\n",
      "Epoch: 64/100... Step: 2365... Loss: 0.8336...\n",
      "Epoch: 64/100... Step: 2366... Loss: 0.8598...\n",
      "Epoch: 64/100... Step: 2367... Loss: 0.8430...\n",
      "Epoch: 64/100... Step: 2368... Loss: 0.8368...\n",
      "Epoch: 65/100... Step: 2369... Loss: 0.8511...\n",
      "Epoch: 65/100... Step: 2370... Loss: 0.8425...\n",
      "Epoch: 65/100... Step: 2371... Loss: 0.8305...\n",
      "Epoch: 65/100... Step: 2372... Loss: 0.8314...\n",
      "Epoch: 65/100... Step: 2373... Loss: 0.8355...\n",
      "Epoch: 65/100... Step: 2374... Loss: 0.8491...\n",
      "Epoch: 65/100... Step: 2375... Loss: 0.8403...\n",
      "Epoch: 65/100... Step: 2376... Loss: 0.8574...\n",
      "Epoch: 65/100... Step: 2377... Loss: 0.8566...\n",
      "Epoch: 65/100... Step: 2378... Loss: 0.8540...\n",
      "Epoch: 65/100... Step: 2379... Loss: 0.8395...\n",
      "Epoch: 65/100... Step: 2380... Loss: 0.8301...\n",
      "Epoch: 65/100... Step: 2381... Loss: 0.8235...\n",
      "Epoch: 65/100... Step: 2382... Loss: 0.8386...\n",
      "Epoch: 65/100... Step: 2383... Loss: 0.8405...\n",
      "Epoch: 65/100... Step: 2384... Loss: 0.8273...\n",
      "Epoch: 65/100... Step: 2385... Loss: 0.8320...\n",
      "Epoch: 65/100... Step: 2386... Loss: 0.8383...\n",
      "Epoch: 65/100... Step: 2387... Loss: 0.8314...\n",
      "Epoch: 65/100... Step: 2388... Loss: 0.8366...\n",
      "Epoch: 65/100... Step: 2389... Loss: 0.8360...\n",
      "Epoch: 65/100... Step: 2390... Loss: 0.8390...\n",
      "Epoch: 65/100... Step: 2391... Loss: 0.8196...\n",
      "Epoch: 65/100... Step: 2392... Loss: 0.8283...\n",
      "Epoch: 65/100... Step: 2393... Loss: 0.8469...\n",
      "Epoch: 65/100... Step: 2394... Loss: 0.8216...\n",
      "Epoch: 65/100... Step: 2395... Loss: 0.8233...\n",
      "Epoch: 65/100... Step: 2396... Loss: 0.8266...\n",
      "Epoch: 65/100... Step: 2397... Loss: 0.8432...\n",
      "Epoch: 65/100... Step: 2398... Loss: 0.8320...\n",
      "Epoch: 65/100... Step: 2399... Loss: 0.8375...\n",
      "Epoch: 65/100... Step: 2400... Loss: 0.8409...\n",
      "Epoch: 65/100... Step: 2401... Loss: 0.8441...\n",
      "Epoch: 65/100... Step: 2402... Loss: 0.8304...\n",
      "Epoch: 65/100... Step: 2403... Loss: 0.8495...\n",
      "Epoch: 65/100... Step: 2404... Loss: 0.8322...\n",
      "Epoch: 65/100... Step: 2405... Loss: 0.8338...\n",
      "Epoch: 66/100... Step: 2406... Loss: 0.8480...\n",
      "Epoch: 66/100... Step: 2407... Loss: 0.8314...\n",
      "Epoch: 66/100... Step: 2408... Loss: 0.8247...\n",
      "Epoch: 66/100... Step: 2409... Loss: 0.8345...\n",
      "Epoch: 66/100... Step: 2410... Loss: 0.8304...\n",
      "Epoch: 66/100... Step: 2411... Loss: 0.8350...\n",
      "Epoch: 66/100... Step: 2412... Loss: 0.8314...\n",
      "Epoch: 66/100... Step: 2413... Loss: 0.8453...\n",
      "Epoch: 66/100... Step: 2414... Loss: 0.8458...\n",
      "Epoch: 66/100... Step: 2415... Loss: 0.8437...\n",
      "Epoch: 66/100... Step: 2416... Loss: 0.8241...\n",
      "Epoch: 66/100... Step: 2417... Loss: 0.8169...\n",
      "Epoch: 66/100... Step: 2418... Loss: 0.8173...\n",
      "Epoch: 66/100... Step: 2419... Loss: 0.8257...\n",
      "Epoch: 66/100... Step: 2420... Loss: 0.8174...\n",
      "Epoch: 66/100... Step: 2421... Loss: 0.8147...\n",
      "Epoch: 66/100... Step: 2422... Loss: 0.8259...\n",
      "Epoch: 66/100... Step: 2423... Loss: 0.8216...\n",
      "Epoch: 66/100... Step: 2424... Loss: 0.8123...\n",
      "Epoch: 66/100... Step: 2425... Loss: 0.8307...\n",
      "Epoch: 66/100... Step: 2426... Loss: 0.8229...\n",
      "Epoch: 66/100... Step: 2427... Loss: 0.8251...\n",
      "Epoch: 66/100... Step: 2428... Loss: 0.8165...\n",
      "Epoch: 66/100... Step: 2429... Loss: 0.8200...\n",
      "Epoch: 66/100... Step: 2430... Loss: 0.8312...\n",
      "Epoch: 66/100... Step: 2431... Loss: 0.8155...\n",
      "Epoch: 66/100... Step: 2432... Loss: 0.8164...\n",
      "Epoch: 66/100... Step: 2433... Loss: 0.8057...\n",
      "Epoch: 66/100... Step: 2434... Loss: 0.8327...\n",
      "Epoch: 66/100... Step: 2435... Loss: 0.8240...\n",
      "Epoch: 66/100... Step: 2436... Loss: 0.8202...\n",
      "Epoch: 66/100... Step: 2437... Loss: 0.8217...\n",
      "Epoch: 66/100... Step: 2438... Loss: 0.8347...\n",
      "Epoch: 66/100... Step: 2439... Loss: 0.8155...\n",
      "Epoch: 66/100... Step: 2440... Loss: 0.8369...\n",
      "Epoch: 66/100... Step: 2441... Loss: 0.8212...\n",
      "Epoch: 66/100... Step: 2442... Loss: 0.8211...\n",
      "Epoch: 67/100... Step: 2443... Loss: 0.8366...\n",
      "Epoch: 67/100... Step: 2444... Loss: 0.8247...\n",
      "Epoch: 67/100... Step: 2445... Loss: 0.8176...\n",
      "Epoch: 67/100... Step: 2446... Loss: 0.8280...\n",
      "Epoch: 67/100... Step: 2447... Loss: 0.8263...\n",
      "Epoch: 67/100... Step: 2448... Loss: 0.8221...\n",
      "Epoch: 67/100... Step: 2449... Loss: 0.8229...\n",
      "Epoch: 67/100... Step: 2450... Loss: 0.8351...\n",
      "Epoch: 67/100... Step: 2451... Loss: 0.8402...\n",
      "Epoch: 67/100... Step: 2452... Loss: 0.8432...\n",
      "Epoch: 67/100... Step: 2453... Loss: 0.8203...\n",
      "Epoch: 67/100... Step: 2454... Loss: 0.8125...\n",
      "Epoch: 67/100... Step: 2455... Loss: 0.8084...\n",
      "Epoch: 67/100... Step: 2456... Loss: 0.8194...\n",
      "Epoch: 67/100... Step: 2457... Loss: 0.8225...\n",
      "Epoch: 67/100... Step: 2458... Loss: 0.8104...\n",
      "Epoch: 67/100... Step: 2459... Loss: 0.8158...\n",
      "Epoch: 67/100... Step: 2460... Loss: 0.8229...\n",
      "Epoch: 67/100... Step: 2461... Loss: 0.8095...\n",
      "Epoch: 67/100... Step: 2462... Loss: 0.8255...\n",
      "Epoch: 67/100... Step: 2463... Loss: 0.8272...\n",
      "Epoch: 67/100... Step: 2464... Loss: 0.8224...\n",
      "Epoch: 67/100... Step: 2465... Loss: 0.8052...\n",
      "Epoch: 67/100... Step: 2466... Loss: 0.8161...\n",
      "Epoch: 67/100... Step: 2467... Loss: 0.8228...\n",
      "Epoch: 67/100... Step: 2468... Loss: 0.8066...\n",
      "Epoch: 67/100... Step: 2469... Loss: 0.8203...\n",
      "Epoch: 67/100... Step: 2470... Loss: 0.8111...\n",
      "Epoch: 67/100... Step: 2471... Loss: 0.8262...\n",
      "Epoch: 67/100... Step: 2472... Loss: 0.8188...\n",
      "Epoch: 67/100... Step: 2473... Loss: 0.8244...\n",
      "Epoch: 67/100... Step: 2474... Loss: 0.8217...\n",
      "Epoch: 67/100... Step: 2475... Loss: 0.8263...\n",
      "Epoch: 67/100... Step: 2476... Loss: 0.8098...\n",
      "Epoch: 67/100... Step: 2477... Loss: 0.8333...\n",
      "Epoch: 67/100... Step: 2478... Loss: 0.8154...\n",
      "Epoch: 67/100... Step: 2479... Loss: 0.8181...\n",
      "Epoch: 68/100... Step: 2480... Loss: 0.8381...\n",
      "Epoch: 68/100... Step: 2481... Loss: 0.8201...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68/100... Step: 2482... Loss: 0.8123...\n",
      "Epoch: 68/100... Step: 2483... Loss: 0.8265...\n",
      "Epoch: 68/100... Step: 2484... Loss: 0.8267...\n",
      "Epoch: 68/100... Step: 2485... Loss: 0.8248...\n",
      "Epoch: 68/100... Step: 2486... Loss: 0.8225...\n",
      "Epoch: 68/100... Step: 2487... Loss: 0.8311...\n",
      "Epoch: 68/100... Step: 2488... Loss: 0.8327...\n",
      "Epoch: 68/100... Step: 2489... Loss: 0.8408...\n",
      "Epoch: 68/100... Step: 2490... Loss: 0.8248...\n",
      "Epoch: 68/100... Step: 2491... Loss: 0.8150...\n",
      "Epoch: 68/100... Step: 2492... Loss: 0.8072...\n",
      "Epoch: 68/100... Step: 2493... Loss: 0.8203...\n",
      "Epoch: 68/100... Step: 2494... Loss: 0.8187...\n",
      "Epoch: 68/100... Step: 2495... Loss: 0.8132...\n",
      "Epoch: 68/100... Step: 2496... Loss: 0.8156...\n",
      "Epoch: 68/100... Step: 2497... Loss: 0.8193...\n",
      "Epoch: 68/100... Step: 2498... Loss: 0.8096...\n",
      "Epoch: 68/100... Step: 2499... Loss: 0.8194...\n",
      "Epoch: 68/100... Step: 2500... Loss: 0.8149...\n",
      "Epoch: 68/100... Step: 2501... Loss: 0.8186...\n",
      "Epoch: 68/100... Step: 2502... Loss: 0.8090...\n",
      "Epoch: 68/100... Step: 2503... Loss: 0.8151...\n",
      "Epoch: 68/100... Step: 2504... Loss: 0.8215...\n",
      "Epoch: 68/100... Step: 2505... Loss: 0.8063...\n",
      "Epoch: 68/100... Step: 2506... Loss: 0.8071...\n",
      "Epoch: 68/100... Step: 2507... Loss: 0.8014...\n",
      "Epoch: 68/100... Step: 2508... Loss: 0.8258...\n",
      "Epoch: 68/100... Step: 2509... Loss: 0.8186...\n",
      "Epoch: 68/100... Step: 2510... Loss: 0.8187...\n",
      "Epoch: 68/100... Step: 2511... Loss: 0.8201...\n",
      "Epoch: 68/100... Step: 2512... Loss: 0.8269...\n",
      "Epoch: 68/100... Step: 2513... Loss: 0.8079...\n",
      "Epoch: 68/100... Step: 2514... Loss: 0.8278...\n",
      "Epoch: 68/100... Step: 2515... Loss: 0.8147...\n",
      "Epoch: 68/100... Step: 2516... Loss: 0.8127...\n",
      "Epoch: 69/100... Step: 2517... Loss: 0.8263...\n",
      "Epoch: 69/100... Step: 2518... Loss: 0.8199...\n",
      "Epoch: 69/100... Step: 2519... Loss: 0.8126...\n",
      "Epoch: 69/100... Step: 2520... Loss: 0.8222...\n",
      "Epoch: 69/100... Step: 2521... Loss: 0.8249...\n",
      "Epoch: 69/100... Step: 2522... Loss: 0.8259...\n",
      "Epoch: 69/100... Step: 2523... Loss: 0.8189...\n",
      "Epoch: 69/100... Step: 2524... Loss: 0.8289...\n",
      "Epoch: 69/100... Step: 2525... Loss: 0.8372...\n",
      "Epoch: 69/100... Step: 2526... Loss: 0.8336...\n",
      "Epoch: 69/100... Step: 2527... Loss: 0.8137...\n",
      "Epoch: 69/100... Step: 2528... Loss: 0.8123...\n",
      "Epoch: 69/100... Step: 2529... Loss: 0.8132...\n",
      "Epoch: 69/100... Step: 2530... Loss: 0.8145...\n",
      "Epoch: 69/100... Step: 2531... Loss: 0.8169...\n",
      "Epoch: 69/100... Step: 2532... Loss: 0.8153...\n",
      "Epoch: 69/100... Step: 2533... Loss: 0.8199...\n",
      "Epoch: 69/100... Step: 2534... Loss: 0.8141...\n",
      "Epoch: 69/100... Step: 2535... Loss: 0.8037...\n",
      "Epoch: 69/100... Step: 2536... Loss: 0.8218...\n",
      "Epoch: 69/100... Step: 2537... Loss: 0.8164...\n",
      "Epoch: 69/100... Step: 2538... Loss: 0.8183...\n",
      "Epoch: 69/100... Step: 2539... Loss: 0.8068...\n",
      "Epoch: 69/100... Step: 2540... Loss: 0.8127...\n",
      "Epoch: 69/100... Step: 2541... Loss: 0.8244...\n",
      "Epoch: 69/100... Step: 2542... Loss: 0.8056...\n",
      "Epoch: 69/100... Step: 2543... Loss: 0.8090...\n",
      "Epoch: 69/100... Step: 2544... Loss: 0.8061...\n",
      "Epoch: 69/100... Step: 2545... Loss: 0.8216...\n",
      "Epoch: 69/100... Step: 2546... Loss: 0.8138...\n",
      "Epoch: 69/100... Step: 2547... Loss: 0.8162...\n",
      "Epoch: 69/100... Step: 2548... Loss: 0.8151...\n",
      "Epoch: 69/100... Step: 2549... Loss: 0.8183...\n",
      "Epoch: 69/100... Step: 2550... Loss: 0.8035...\n",
      "Epoch: 69/100... Step: 2551... Loss: 0.8259...\n",
      "Epoch: 69/100... Step: 2552... Loss: 0.8093...\n",
      "Epoch: 69/100... Step: 2553... Loss: 0.8090...\n",
      "Epoch: 70/100... Step: 2554... Loss: 0.8236...\n",
      "Epoch: 70/100... Step: 2555... Loss: 0.8100...\n",
      "Epoch: 70/100... Step: 2556... Loss: 0.8107...\n",
      "Epoch: 70/100... Step: 2557... Loss: 0.8178...\n",
      "Epoch: 70/100... Step: 2558... Loss: 0.8215...\n",
      "Epoch: 70/100... Step: 2559... Loss: 0.8223...\n",
      "Epoch: 70/100... Step: 2560... Loss: 0.8210...\n",
      "Epoch: 70/100... Step: 2561... Loss: 0.8329...\n",
      "Epoch: 70/100... Step: 2562... Loss: 0.8261...\n",
      "Epoch: 70/100... Step: 2563... Loss: 0.8290...\n",
      "Epoch: 70/100... Step: 2564... Loss: 0.8172...\n",
      "Epoch: 70/100... Step: 2565... Loss: 0.8047...\n",
      "Epoch: 70/100... Step: 2566... Loss: 0.8037...\n",
      "Epoch: 70/100... Step: 2567... Loss: 0.8183...\n",
      "Epoch: 70/100... Step: 2568... Loss: 0.8203...\n",
      "Epoch: 70/100... Step: 2569... Loss: 0.8078...\n",
      "Epoch: 70/100... Step: 2570... Loss: 0.8113...\n",
      "Epoch: 70/100... Step: 2571... Loss: 0.8186...\n",
      "Epoch: 70/100... Step: 2572... Loss: 0.8059...\n",
      "Epoch: 70/100... Step: 2573... Loss: 0.8171...\n",
      "Epoch: 70/100... Step: 2574... Loss: 0.8169...\n",
      "Epoch: 70/100... Step: 2575... Loss: 0.8202...\n",
      "Epoch: 70/100... Step: 2576... Loss: 0.8085...\n",
      "Epoch: 70/100... Step: 2577... Loss: 0.8152...\n",
      "Epoch: 70/100... Step: 2578... Loss: 0.8233...\n",
      "Epoch: 70/100... Step: 2579... Loss: 0.8082...\n",
      "Epoch: 70/100... Step: 2580... Loss: 0.8094...\n",
      "Epoch: 70/100... Step: 2581... Loss: 0.8037...\n",
      "Epoch: 70/100... Step: 2582... Loss: 0.8189...\n",
      "Epoch: 70/100... Step: 2583... Loss: 0.8092...\n",
      "Epoch: 70/100... Step: 2584... Loss: 0.8180...\n",
      "Epoch: 70/100... Step: 2585... Loss: 0.8149...\n",
      "Epoch: 70/100... Step: 2586... Loss: 0.8123...\n",
      "Epoch: 70/100... Step: 2587... Loss: 0.8020...\n",
      "Epoch: 70/100... Step: 2588... Loss: 0.8251...\n",
      "Epoch: 70/100... Step: 2589... Loss: 0.7973...\n",
      "Epoch: 70/100... Step: 2590... Loss: 0.8059...\n",
      "Epoch: 71/100... Step: 2591... Loss: 0.8212...\n",
      "Epoch: 71/100... Step: 2592... Loss: 0.8062...\n",
      "Epoch: 71/100... Step: 2593... Loss: 0.8007...\n",
      "Epoch: 71/100... Step: 2594... Loss: 0.8187...\n",
      "Epoch: 71/100... Step: 2595... Loss: 0.8191...\n",
      "Epoch: 71/100... Step: 2596... Loss: 0.8165...\n",
      "Epoch: 71/100... Step: 2597... Loss: 0.8142...\n",
      "Epoch: 71/100... Step: 2598... Loss: 0.8238...\n",
      "Epoch: 71/100... Step: 2599... Loss: 0.8282...\n",
      "Epoch: 71/100... Step: 2600... Loss: 0.8266...\n",
      "Epoch: 71/100... Step: 2601... Loss: 0.8077...\n",
      "Epoch: 71/100... Step: 2602... Loss: 0.8017...\n",
      "Epoch: 71/100... Step: 2603... Loss: 0.8033...\n",
      "Epoch: 71/100... Step: 2604... Loss: 0.8095...\n",
      "Epoch: 71/100... Step: 2605... Loss: 0.8103...\n",
      "Epoch: 71/100... Step: 2606... Loss: 0.8012...\n",
      "Epoch: 71/100... Step: 2607... Loss: 0.8115...\n",
      "Epoch: 71/100... Step: 2608... Loss: 0.8161...\n",
      "Epoch: 71/100... Step: 2609... Loss: 0.8035...\n",
      "Epoch: 71/100... Step: 2610... Loss: 0.8165...\n",
      "Epoch: 71/100... Step: 2611... Loss: 0.8119...\n",
      "Epoch: 71/100... Step: 2612... Loss: 0.8171...\n",
      "Epoch: 71/100... Step: 2613... Loss: 0.8079...\n",
      "Epoch: 71/100... Step: 2614... Loss: 0.8129...\n",
      "Epoch: 71/100... Step: 2615... Loss: 0.8181...\n",
      "Epoch: 71/100... Step: 2616... Loss: 0.8048...\n",
      "Epoch: 71/100... Step: 2617... Loss: 0.8068...\n",
      "Epoch: 71/100... Step: 2618... Loss: 0.8022...\n",
      "Epoch: 71/100... Step: 2619... Loss: 0.8294...\n",
      "Epoch: 71/100... Step: 2620... Loss: 0.8120...\n",
      "Epoch: 71/100... Step: 2621... Loss: 0.8097...\n",
      "Epoch: 71/100... Step: 2622... Loss: 0.8182...\n",
      "Epoch: 71/100... Step: 2623... Loss: 0.8212...\n",
      "Epoch: 71/100... Step: 2624... Loss: 0.8032...\n",
      "Epoch: 71/100... Step: 2625... Loss: 0.8229...\n",
      "Epoch: 71/100... Step: 2626... Loss: 0.8063...\n",
      "Epoch: 71/100... Step: 2627... Loss: 0.8095...\n",
      "Epoch: 72/100... Step: 2628... Loss: 0.8199...\n",
      "Epoch: 72/100... Step: 2629... Loss: 0.8083...\n",
      "Epoch: 72/100... Step: 2630... Loss: 0.7979...\n",
      "Epoch: 72/100... Step: 2631... Loss: 0.8092...\n",
      "Epoch: 72/100... Step: 2632... Loss: 0.8122...\n",
      "Epoch: 72/100... Step: 2633... Loss: 0.8148...\n",
      "Epoch: 72/100... Step: 2634... Loss: 0.8148...\n",
      "Epoch: 72/100... Step: 2635... Loss: 0.8247...\n",
      "Epoch: 72/100... Step: 2636... Loss: 0.8274...\n",
      "Epoch: 72/100... Step: 2637... Loss: 0.8262...\n",
      "Epoch: 72/100... Step: 2638... Loss: 0.8145...\n",
      "Epoch: 72/100... Step: 2639... Loss: 0.8044...\n",
      "Epoch: 72/100... Step: 2640... Loss: 0.7990...\n",
      "Epoch: 72/100... Step: 2641... Loss: 0.8077...\n",
      "Epoch: 72/100... Step: 2642... Loss: 0.8154...\n",
      "Epoch: 72/100... Step: 2643... Loss: 0.8005...\n",
      "Epoch: 72/100... Step: 2644... Loss: 0.8061...\n",
      "Epoch: 72/100... Step: 2645... Loss: 0.8148...\n",
      "Epoch: 72/100... Step: 2646... Loss: 0.8014...\n",
      "Epoch: 72/100... Step: 2647... Loss: 0.8160...\n",
      "Epoch: 72/100... Step: 2648... Loss: 0.8085...\n",
      "Epoch: 72/100... Step: 2649... Loss: 0.8123...\n",
      "Epoch: 72/100... Step: 2650... Loss: 0.8046...\n",
      "Epoch: 72/100... Step: 2651... Loss: 0.8121...\n",
      "Epoch: 72/100... Step: 2652... Loss: 0.8160...\n",
      "Epoch: 72/100... Step: 2653... Loss: 0.8009...\n",
      "Epoch: 72/100... Step: 2654... Loss: 0.8030...\n",
      "Epoch: 72/100... Step: 2655... Loss: 0.8012...\n",
      "Epoch: 72/100... Step: 2656... Loss: 0.8204...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72/100... Step: 2657... Loss: 0.8124...\n",
      "Epoch: 72/100... Step: 2658... Loss: 0.8169...\n",
      "Epoch: 72/100... Step: 2659... Loss: 0.8180...\n",
      "Epoch: 72/100... Step: 2660... Loss: 0.8206...\n",
      "Epoch: 72/100... Step: 2661... Loss: 0.8030...\n",
      "Epoch: 72/100... Step: 2662... Loss: 0.8169...\n",
      "Epoch: 72/100... Step: 2663... Loss: 0.8005...\n",
      "Epoch: 72/100... Step: 2664... Loss: 0.8067...\n",
      "Epoch: 73/100... Step: 2665... Loss: 0.8167...\n",
      "Epoch: 73/100... Step: 2666... Loss: 0.8036...\n",
      "Epoch: 73/100... Step: 2667... Loss: 0.8026...\n",
      "Epoch: 73/100... Step: 2668... Loss: 0.8119...\n",
      "Epoch: 73/100... Step: 2669... Loss: 0.8110...\n",
      "Epoch: 73/100... Step: 2670... Loss: 0.8078...\n",
      "Epoch: 73/100... Step: 2671... Loss: 0.8103...\n",
      "Epoch: 73/100... Step: 2672... Loss: 0.8170...\n",
      "Epoch: 73/100... Step: 2673... Loss: 0.8192...\n",
      "Epoch: 73/100... Step: 2674... Loss: 0.8281...\n",
      "Epoch: 73/100... Step: 2675... Loss: 0.8122...\n",
      "Epoch: 73/100... Step: 2676... Loss: 0.7982...\n",
      "Epoch: 73/100... Step: 2677... Loss: 0.7984...\n",
      "Epoch: 73/100... Step: 2678... Loss: 0.8103...\n",
      "Epoch: 73/100... Step: 2679... Loss: 0.8079...\n",
      "Epoch: 73/100... Step: 2680... Loss: 0.8021...\n",
      "Epoch: 73/100... Step: 2681... Loss: 0.8054...\n",
      "Epoch: 73/100... Step: 2682... Loss: 0.8051...\n",
      "Epoch: 73/100... Step: 2683... Loss: 0.7928...\n",
      "Epoch: 73/100... Step: 2684... Loss: 0.8158...\n",
      "Epoch: 73/100... Step: 2685... Loss: 0.8076...\n",
      "Epoch: 73/100... Step: 2686... Loss: 0.8125...\n",
      "Epoch: 73/100... Step: 2687... Loss: 0.8022...\n",
      "Epoch: 73/100... Step: 2688... Loss: 0.8163...\n",
      "Epoch: 73/100... Step: 2689... Loss: 0.8194...\n",
      "Epoch: 73/100... Step: 2690... Loss: 0.7990...\n",
      "Epoch: 73/100... Step: 2691... Loss: 0.8019...\n",
      "Epoch: 73/100... Step: 2692... Loss: 0.7986...\n",
      "Epoch: 73/100... Step: 2693... Loss: 0.8198...\n",
      "Epoch: 73/100... Step: 2694... Loss: 0.8086...\n",
      "Epoch: 73/100... Step: 2695... Loss: 0.8074...\n",
      "Epoch: 73/100... Step: 2696... Loss: 0.8126...\n",
      "Epoch: 73/100... Step: 2697... Loss: 0.8213...\n",
      "Epoch: 73/100... Step: 2698... Loss: 0.8048...\n",
      "Epoch: 73/100... Step: 2699... Loss: 0.8239...\n",
      "Epoch: 73/100... Step: 2700... Loss: 0.8020...\n",
      "Epoch: 73/100... Step: 2701... Loss: 0.8043...\n",
      "Epoch: 74/100... Step: 2702... Loss: 0.8159...\n",
      "Epoch: 74/100... Step: 2703... Loss: 0.8062...\n",
      "Epoch: 74/100... Step: 2704... Loss: 0.8016...\n",
      "Epoch: 74/100... Step: 2705... Loss: 0.8148...\n",
      "Epoch: 74/100... Step: 2706... Loss: 0.8179...\n",
      "Epoch: 74/100... Step: 2707... Loss: 0.8086...\n",
      "Epoch: 74/100... Step: 2708... Loss: 0.8045...\n",
      "Epoch: 74/100... Step: 2709... Loss: 0.8149...\n",
      "Epoch: 74/100... Step: 2710... Loss: 0.8185...\n",
      "Epoch: 74/100... Step: 2711... Loss: 0.8209...\n",
      "Epoch: 74/100... Step: 2712... Loss: 0.8110...\n",
      "Epoch: 74/100... Step: 2713... Loss: 0.8014...\n",
      "Epoch: 74/100... Step: 2714... Loss: 0.7934...\n",
      "Epoch: 74/100... Step: 2715... Loss: 0.8066...\n",
      "Epoch: 74/100... Step: 2716... Loss: 0.8114...\n",
      "Epoch: 74/100... Step: 2717... Loss: 0.8015...\n",
      "Epoch: 74/100... Step: 2718... Loss: 0.8034...\n",
      "Epoch: 74/100... Step: 2719... Loss: 0.8051...\n",
      "Epoch: 74/100... Step: 2720... Loss: 0.7929...\n",
      "Epoch: 74/100... Step: 2721... Loss: 0.8094...\n",
      "Epoch: 74/100... Step: 2722... Loss: 0.7991...\n",
      "Epoch: 74/100... Step: 2723... Loss: 0.8004...\n",
      "Epoch: 74/100... Step: 2724... Loss: 0.8008...\n",
      "Epoch: 74/100... Step: 2725... Loss: 0.8058...\n",
      "Epoch: 74/100... Step: 2726... Loss: 0.8144...\n",
      "Epoch: 74/100... Step: 2727... Loss: 0.7966...\n",
      "Epoch: 74/100... Step: 2728... Loss: 0.8012...\n",
      "Epoch: 74/100... Step: 2729... Loss: 0.7940...\n",
      "Epoch: 74/100... Step: 2730... Loss: 0.8095...\n",
      "Epoch: 74/100... Step: 2731... Loss: 0.8012...\n",
      "Epoch: 74/100... Step: 2732... Loss: 0.8074...\n",
      "Epoch: 74/100... Step: 2733... Loss: 0.8072...\n",
      "Epoch: 74/100... Step: 2734... Loss: 0.8151...\n",
      "Epoch: 74/100... Step: 2735... Loss: 0.7966...\n",
      "Epoch: 74/100... Step: 2736... Loss: 0.8176...\n",
      "Epoch: 74/100... Step: 2737... Loss: 0.7963...\n",
      "Epoch: 74/100... Step: 2738... Loss: 0.8004...\n",
      "Epoch: 75/100... Step: 2739... Loss: 0.8103...\n",
      "Epoch: 75/100... Step: 2740... Loss: 0.8011...\n",
      "Epoch: 75/100... Step: 2741... Loss: 0.7941...\n",
      "Epoch: 75/100... Step: 2742... Loss: 0.8059...\n",
      "Epoch: 75/100... Step: 2743... Loss: 0.8063...\n",
      "Epoch: 75/100... Step: 2744... Loss: 0.7990...\n",
      "Epoch: 75/100... Step: 2745... Loss: 0.8016...\n",
      "Epoch: 75/100... Step: 2746... Loss: 0.8132...\n",
      "Epoch: 75/100... Step: 2747... Loss: 0.8150...\n",
      "Epoch: 75/100... Step: 2748... Loss: 0.8154...\n",
      "Epoch: 75/100... Step: 2749... Loss: 0.7983...\n",
      "Epoch: 75/100... Step: 2750... Loss: 0.7916...\n",
      "Epoch: 75/100... Step: 2751... Loss: 0.7954...\n",
      "Epoch: 75/100... Step: 2752... Loss: 0.8043...\n",
      "Epoch: 75/100... Step: 2753... Loss: 0.8040...\n",
      "Epoch: 75/100... Step: 2754... Loss: 0.7969...\n",
      "Epoch: 75/100... Step: 2755... Loss: 0.8032...\n",
      "Epoch: 75/100... Step: 2756... Loss: 0.8028...\n",
      "Epoch: 75/100... Step: 2757... Loss: 0.7875...\n",
      "Epoch: 75/100... Step: 2758... Loss: 0.8030...\n",
      "Epoch: 75/100... Step: 2759... Loss: 0.8012...\n",
      "Epoch: 75/100... Step: 2760... Loss: 0.7975...\n",
      "Epoch: 75/100... Step: 2761... Loss: 0.7930...\n",
      "Epoch: 75/100... Step: 2762... Loss: 0.8014...\n",
      "Epoch: 75/100... Step: 2763... Loss: 0.8105...\n",
      "Epoch: 75/100... Step: 2764... Loss: 0.7917...\n",
      "Epoch: 75/100... Step: 2765... Loss: 0.7956...\n",
      "Epoch: 75/100... Step: 2766... Loss: 0.7867...\n",
      "Epoch: 75/100... Step: 2767... Loss: 0.8129...\n",
      "Epoch: 75/100... Step: 2768... Loss: 0.7987...\n",
      "Epoch: 75/100... Step: 2769... Loss: 0.8004...\n",
      "Epoch: 75/100... Step: 2770... Loss: 0.8023...\n",
      "Epoch: 75/100... Step: 2771... Loss: 0.8088...\n",
      "Epoch: 75/100... Step: 2772... Loss: 0.8000...\n",
      "Epoch: 75/100... Step: 2773... Loss: 0.8143...\n",
      "Epoch: 75/100... Step: 2774... Loss: 0.7921...\n",
      "Epoch: 75/100... Step: 2775... Loss: 0.7998...\n",
      "Epoch: 76/100... Step: 2776... Loss: 0.8113...\n",
      "Epoch: 76/100... Step: 2777... Loss: 0.7965...\n",
      "Epoch: 76/100... Step: 2778... Loss: 0.7905...\n",
      "Epoch: 76/100... Step: 2779... Loss: 0.8005...\n",
      "Epoch: 76/100... Step: 2780... Loss: 0.8017...\n",
      "Epoch: 76/100... Step: 2781... Loss: 0.7945...\n",
      "Epoch: 76/100... Step: 2782... Loss: 0.7961...\n",
      "Epoch: 76/100... Step: 2783... Loss: 0.8100...\n",
      "Epoch: 76/100... Step: 2784... Loss: 0.8074...\n",
      "Epoch: 76/100... Step: 2785... Loss: 0.8104...\n",
      "Epoch: 76/100... Step: 2786... Loss: 0.7940...\n",
      "Epoch: 76/100... Step: 2787... Loss: 0.7836...\n",
      "Epoch: 76/100... Step: 2788... Loss: 0.7886...\n",
      "Epoch: 76/100... Step: 2789... Loss: 0.7993...\n",
      "Epoch: 76/100... Step: 2790... Loss: 0.7964...\n",
      "Epoch: 76/100... Step: 2791... Loss: 0.7889...\n",
      "Epoch: 76/100... Step: 2792... Loss: 0.7993...\n",
      "Epoch: 76/100... Step: 2793... Loss: 0.8048...\n",
      "Epoch: 76/100... Step: 2794... Loss: 0.7849...\n",
      "Epoch: 76/100... Step: 2795... Loss: 0.7976...\n",
      "Epoch: 76/100... Step: 2796... Loss: 0.7925...\n",
      "Epoch: 76/100... Step: 2797... Loss: 0.7966...\n",
      "Epoch: 76/100... Step: 2798... Loss: 0.7894...\n",
      "Epoch: 76/100... Step: 2799... Loss: 0.7932...\n",
      "Epoch: 76/100... Step: 2800... Loss: 0.8046...\n",
      "Epoch: 76/100... Step: 2801... Loss: 0.7899...\n",
      "Epoch: 76/100... Step: 2802... Loss: 0.7956...\n",
      "Epoch: 76/100... Step: 2803... Loss: 0.7903...\n",
      "Epoch: 76/100... Step: 2804... Loss: 0.8071...\n",
      "Epoch: 76/100... Step: 2805... Loss: 0.7964...\n",
      "Epoch: 76/100... Step: 2806... Loss: 0.7960...\n",
      "Epoch: 76/100... Step: 2807... Loss: 0.7977...\n",
      "Epoch: 76/100... Step: 2808... Loss: 0.8017...\n",
      "Epoch: 76/100... Step: 2809... Loss: 0.7921...\n",
      "Epoch: 76/100... Step: 2810... Loss: 0.8131...\n",
      "Epoch: 76/100... Step: 2811... Loss: 0.7965...\n",
      "Epoch: 76/100... Step: 2812... Loss: 0.7918...\n",
      "Epoch: 77/100... Step: 2813... Loss: 0.8063...\n",
      "Epoch: 77/100... Step: 2814... Loss: 0.7957...\n",
      "Epoch: 77/100... Step: 2815... Loss: 0.7924...\n",
      "Epoch: 77/100... Step: 2816... Loss: 0.7992...\n",
      "Epoch: 77/100... Step: 2817... Loss: 0.8011...\n",
      "Epoch: 77/100... Step: 2818... Loss: 0.7902...\n",
      "Epoch: 77/100... Step: 2819... Loss: 0.7924...\n",
      "Epoch: 77/100... Step: 2820... Loss: 0.8057...\n",
      "Epoch: 77/100... Step: 2821... Loss: 0.8068...\n",
      "Epoch: 77/100... Step: 2822... Loss: 0.8035...\n",
      "Epoch: 77/100... Step: 2823... Loss: 0.7923...\n",
      "Epoch: 77/100... Step: 2824... Loss: 0.7829...\n",
      "Epoch: 77/100... Step: 2825... Loss: 0.7815...\n",
      "Epoch: 77/100... Step: 2826... Loss: 0.7926...\n",
      "Epoch: 77/100... Step: 2827... Loss: 0.7937...\n",
      "Epoch: 77/100... Step: 2828... Loss: 0.7835...\n",
      "Epoch: 77/100... Step: 2829... Loss: 0.7898...\n",
      "Epoch: 77/100... Step: 2830... Loss: 0.7995...\n",
      "Epoch: 77/100... Step: 2831... Loss: 0.7830...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77/100... Step: 2832... Loss: 0.7961...\n",
      "Epoch: 77/100... Step: 2833... Loss: 0.7891...\n",
      "Epoch: 77/100... Step: 2834... Loss: 0.7956...\n",
      "Epoch: 77/100... Step: 2835... Loss: 0.7871...\n",
      "Epoch: 77/100... Step: 2836... Loss: 0.7862...\n",
      "Epoch: 77/100... Step: 2837... Loss: 0.7900...\n",
      "Epoch: 77/100... Step: 2838... Loss: 0.7848...\n",
      "Epoch: 77/100... Step: 2839... Loss: 0.7935...\n",
      "Epoch: 77/100... Step: 2840... Loss: 0.7831...\n",
      "Epoch: 77/100... Step: 2841... Loss: 0.7980...\n",
      "Epoch: 77/100... Step: 2842... Loss: 0.7925...\n",
      "Epoch: 77/100... Step: 2843... Loss: 0.7929...\n",
      "Epoch: 77/100... Step: 2844... Loss: 0.7944...\n",
      "Epoch: 77/100... Step: 2845... Loss: 0.7974...\n",
      "Epoch: 77/100... Step: 2846... Loss: 0.7818...\n",
      "Epoch: 77/100... Step: 2847... Loss: 0.8012...\n",
      "Epoch: 77/100... Step: 2848... Loss: 0.7815...\n",
      "Epoch: 77/100... Step: 2849... Loss: 0.7826...\n",
      "Epoch: 78/100... Step: 2850... Loss: 0.7984...\n",
      "Epoch: 78/100... Step: 2851... Loss: 0.7888...\n",
      "Epoch: 78/100... Step: 2852... Loss: 0.7857...\n",
      "Epoch: 78/100... Step: 2853... Loss: 0.7949...\n",
      "Epoch: 78/100... Step: 2854... Loss: 0.7994...\n",
      "Epoch: 78/100... Step: 2855... Loss: 0.7909...\n",
      "Epoch: 78/100... Step: 2856... Loss: 0.7912...\n",
      "Epoch: 78/100... Step: 2857... Loss: 0.8001...\n",
      "Epoch: 78/100... Step: 2858... Loss: 0.7999...\n",
      "Epoch: 78/100... Step: 2859... Loss: 0.8005...\n",
      "Epoch: 78/100... Step: 2860... Loss: 0.7848...\n",
      "Epoch: 78/100... Step: 2861... Loss: 0.7767...\n",
      "Epoch: 78/100... Step: 2862... Loss: 0.7710...\n",
      "Epoch: 78/100... Step: 2863... Loss: 0.7858...\n",
      "Epoch: 78/100... Step: 2864... Loss: 0.7874...\n",
      "Epoch: 78/100... Step: 2865... Loss: 0.7820...\n",
      "Epoch: 78/100... Step: 2866... Loss: 0.7846...\n",
      "Epoch: 78/100... Step: 2867... Loss: 0.7912...\n",
      "Epoch: 78/100... Step: 2868... Loss: 0.7766...\n",
      "Epoch: 78/100... Step: 2869... Loss: 0.7878...\n",
      "Epoch: 78/100... Step: 2870... Loss: 0.7785...\n",
      "Epoch: 78/100... Step: 2871... Loss: 0.7810...\n",
      "Epoch: 78/100... Step: 2872... Loss: 0.7761...\n",
      "Epoch: 78/100... Step: 2873... Loss: 0.7868...\n",
      "Epoch: 78/100... Step: 2874... Loss: 0.7961...\n",
      "Epoch: 78/100... Step: 2875... Loss: 0.7759...\n",
      "Epoch: 78/100... Step: 2876... Loss: 0.7844...\n",
      "Epoch: 78/100... Step: 2877... Loss: 0.7769...\n",
      "Epoch: 78/100... Step: 2878... Loss: 0.7902...\n",
      "Epoch: 78/100... Step: 2879... Loss: 0.7780...\n",
      "Epoch: 78/100... Step: 2880... Loss: 0.7823...\n",
      "Epoch: 78/100... Step: 2881... Loss: 0.7870...\n",
      "Epoch: 78/100... Step: 2882... Loss: 0.7896...\n",
      "Epoch: 78/100... Step: 2883... Loss: 0.7747...\n",
      "Epoch: 78/100... Step: 2884... Loss: 0.8010...\n",
      "Epoch: 78/100... Step: 2885... Loss: 0.7819...\n",
      "Epoch: 78/100... Step: 2886... Loss: 0.7809...\n",
      "Epoch: 79/100... Step: 2887... Loss: 0.7893...\n",
      "Epoch: 79/100... Step: 2888... Loss: 0.7780...\n",
      "Epoch: 79/100... Step: 2889... Loss: 0.7747...\n",
      "Epoch: 79/100... Step: 2890... Loss: 0.7888...\n",
      "Epoch: 79/100... Step: 2891... Loss: 0.7963...\n",
      "Epoch: 79/100... Step: 2892... Loss: 0.7928...\n",
      "Epoch: 79/100... Step: 2893... Loss: 0.7899...\n"
     ]
    }
   ],
   "source": [
    "batch_size = 96\n",
    "seq_length = 500\n",
    "n_epochs = 100 # start small if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, weight_decay=0, clip=5, print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T07:46:36.908578Z",
     "start_time": "2019-02-02T07:46:33.904504Z"
    }
   },
   "outputs": [],
   "source": [
    "sleep(3)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the best model\n",
    "\n",
    "To set your hyperparameters to get the best performance, you'll want to watch the training and validation losses. If your training loss is much lower than the validation loss, you're overfitting. Increase regularization (more dropout) or use a smaller network. If the training and validation losses are close, you're underfitting so you can increase the size of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here are the hyperparameters for the network.\n",
    "\n",
    "In defining the model:\n",
    "* `n_hidden` - The number of units in the hidden layers.\n",
    "* `n_layers` - Number of hidden LSTM layers to use.\n",
    "\n",
    "We assume that dropout probability and learning rate will be kept at the default, in this example.\n",
    "\n",
    "And in training:\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `seq_length` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lr` - Learning rate for training\n",
    "\n",
    "Here's some good advice from Andrej Karpathy on training the network. I'm going to copy it in here for your benefit, but also link to [where it originally came from](https://github.com/karpathy/char-rnn#tips-and-tricks).\n",
    "\n",
    "> ## Tips and Tricks\n",
    "\n",
    ">### Monitoring Validation Loss vs. Training Loss\n",
    ">If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "\n",
    "> - If your training loss is much lower than validation loss then this means the network might be **overfitting**. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\n",
    "> - If your training/validation loss are about equal then your model is **underfitting**. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
    "\n",
    "> ### Approximate number of parameters\n",
    "\n",
    "> The two most important parameters that control the model are `n_hidden` and `n_layers`. I would advise that you always use `n_layers` of either 2/3. The `n_hidden` can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "> - The number of parameters in your model. This is printed when you start training.\n",
    "> - The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    ">These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "> - I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make `n_hidden` larger.\n",
    "> - I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that helps the validation loss.\n",
    "\n",
    "> ### Best models strategy\n",
    "\n",
    ">The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
    "\n",
    ">It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n",
    "\n",
    ">By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "After training, we'll save the model so we can load it again later if we need too. Here I'm saving the parameters needed to create the same architecture, the hidden layer hyperparameters and the text characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name, for saving multiple files\n",
    "model_name = 'BNPhLSTM.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Making Predictions\n",
    "\n",
    "Now that the model is trained, we'll want to sample from it and make predictions about next characters! To sample, we pass in a character and have the network predict the next character. Then we take that character, pass it back in, and get another predicted character. Just keep doing this and you'll generate a bunch of text!\n",
    "\n",
    "### A note on the `predict`  function\n",
    "\n",
    "The output of our RNN is from a fully-connected layer and it outputs a **distribution of next-character scores**.\n",
    "\n",
    "> To actually get the next character, we apply a softmax function, which gives us a *probability* distribution that we can then sample to predict the next character.\n",
    "\n",
    "### Top K sampling\n",
    "\n",
    "Our predictions come from a categorical probability distribution over all the possible characters. We can make the sample text and make it more reasonable to handle (with less variables) by only considering some $K$ most probable characters. This will prevent the network from giving us completely absurd characters while allowing it to introduce some noise and randomness into the sampled text. Read more about [topk, here](https://pytorch.org/docs/stable/torch.html#torch.topk).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T23:22:20.795173Z",
     "start_time": "2019-02-02T23:22:20.786141Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        if isinstance(h, (list, tuple)):\n",
    "            h = tuple([each.data for each in h])\n",
    "        else:\n",
    "            h = h.data\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out.view(-1, out.shape[-1]), dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priming and generating text \n",
    "\n",
    "Typically you'll want to prime the network so you can build up a hidden state. Otherwise the network will start out generating characters at random. In general the first bunch of characters will be a little rough since it hasn't built up a long history of characters to predict from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T23:22:21.687829Z",
     "start_time": "2019-02-02T23:22:21.682074Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T23:22:36.935908Z",
     "start_time": "2019-02-02T23:22:22.345091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna she made haste\n",
      "to ask the sake of the land, and was a fool one has to do\n",
      "this house. Anna was not the fight to herself--\"and so it always is a fallen world with his\n",
      "own illness, or directly, that you were the\n",
      "snipe-telming--at the willopy passion that had been\n",
      "already busy with the little correct for the\n",
      "previous step it called to the crumble. It was a matter of\n",
      "district more for him anything; but he\n",
      "could not have a thing what was worse in such a fool,\n",
      "and so stumfing over her fashion. At leapin in the\n",
      "corner which she had fancied she was staying at the one's\n",
      "words that her mother andwought has been done in the struggle.\n",
      "They went up. \"How does I met your husband with you to ask you to say, and then you have a nambardrance,\"\n",
      "he said with a silking subordinate, at his embroad,\n",
      "said again with she shaking about something, alleady she stopped her husband. \"It cannot be\n",
      "often doubts. The wartzon sends to their name that\n",
      "we would be in the wide as too that a certain songender\n",
      "world. And I have to go\n",
      "to Anna Arkadyevna,\" whispinned to talk to him.\n",
      "\n",
      "\"Oh, what did you say worse that wavored? You are\n",
      "all strong at the compartment. Take if at all this has to be managing anything of the last\n",
      "time. The confession in the pasterns circles, or you do not tell\n",
      "me to her, and I have driven away, and tea write to you.\n",
      "Then?\" said Kitty, that she felt that it was not a post of the\n",
      "waltz; but it was impossible; and thesefore, with the money's exquisite\n",
      "detection. The money had money, and was all the same as they had sent a\n",
      "day, wanting to her husband, and what were his fingers.\n",
      "\n",
      "The marshal of the proves the princess and Varenka destrued him a feeling of remarks to\n",
      "her for electrictings, the little fingers were settled in her artuments, and then\n",
      "she saw that he was already been an antilly and men. Beside her,\n",
      "too, that it\n",
      "would be\n",
      "met to take another, expense. The old landowner had gone out, taking lying\n",
      "and stubborn. \"Whether we've takes me,\" she\n",
      "was smiling.\n",
      "\n",
      "\"Here's one of the child\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 2000, prime='Anna ', top_k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
    "with open('rnn_x_epoch.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample using a loaded model\n",
    "print(sample(loaded, 2000, top_k=5, prime=\"And Levin said\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
